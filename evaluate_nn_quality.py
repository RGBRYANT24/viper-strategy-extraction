"""
ç¥ç»ç½‘ç»œè´¨é‡å…¨é¢è¯„ä¼°å·¥å…·
åˆ¤æ–­TicTacToeç­–ç•¥æ˜¯å¦è¾¾åˆ°æœ€ä¼˜
"""

import argparse
import numpy as np
from stable_baselines3 import DQN
import gymnasium as gym
from itertools import permutations
import torch

# å¯¼å…¥ç¯å¢ƒæ³¨å†Œ
import gym_env


class TicTacToeNNEvaluator:
    """TicTacToeç¥ç»ç½‘ç»œè´¨é‡è¯„ä¼°å™¨"""

    def __init__(self, model_path):
        self.model = DQN.load(model_path)
        self.win_combinations = [
            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # è¡Œ
            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # åˆ—
            [0, 4, 8], [2, 4, 6]              # å¯¹è§’çº¿
        ]

    def evaluate_critical_positions(self):
        """è¯„ä¼°å…³é”®å±€é¢è¯†åˆ«èƒ½åŠ›ï¼ˆæœ€ä¼˜ç­–ç•¥å¿…é¡»æ­£ç¡®çš„å±€é¢ï¼‰"""
        print("=" * 70)
        print("è¯„ä¼° 1: å…³é”®å±€é¢è¯†åˆ«èƒ½åŠ›")
        print("=" * 70)

        test_cases = [
            # (æ£‹ç›˜çŠ¶æ€, æ­£ç¡®åŠ¨ä½œåˆ—è¡¨, æè¿°)
            # 1. ç«‹å³è·èƒœæœºä¼š
            ([1, 1, 0, 0, 0, 0, 0, 0, 0], [2], "ç«‹å³è·èƒœ-æ¨ª"),
            ([1, 0, 0, 1, 0, 0, 0, 0, 0], [7], "ç«‹å³è·èƒœ-ç«–"),
            ([1, 0, 0, 0, 1, 0, 0, 0, 0], [8], "ç«‹å³è·èƒœ-å¯¹è§’çº¿"),
            ([0, 0, 1, 0, 1, 0, 1, 0, 0], [3], "ç«‹å³è·èƒœ-åå¯¹è§’çº¿"),

            # 2. å¿…é¡»é˜²å®ˆ
            ([-1, -1, 0, 0, 0, 0, 0, 0, 0], [2], "é˜²å®ˆè·èƒœ-æ¨ª"),
            ([-1, 0, 0, -1, 0, 0, 0, 0, 0], [7], "é˜²å®ˆè·èƒœ-ç«–"),
            ([-1, 0, 0, 0, -1, 0, 0, 0, 0], [8], "é˜²å®ˆè·èƒœ-å¯¹è§’çº¿"),

            # 3. åˆ›é€ åŒå¨èƒï¼ˆå‰æ”»ï¼‰
            ([1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 2, 6, 8], "å¼€å±€-ä¸­å¿ƒæˆ–è§’è½"),
            ([1, 0, 0, 0, 0, 0, 0, 0, 1], [4], "åŒè§’-å ä¸­å¿ƒ"),
            ([1, 0, 0, 0, 1, 0, 0, 0, 0], [2, 6], "ä¸­å¿ƒ+è§’è½-åˆ›é€ å‰æ”»"),

            # 4. ç ´åå¯¹æ‰‹å‰æ”»
            ([-1, 0, 0, 0, 0, 0, 0, 0, -1], [4], "å¯¹æ‰‹åŒè§’-å¿…é¡»å ä¸­å¿ƒ"),
            ([-1, 0, 0, 0, -1, 0, 0, 0, 0], [1, 3, 5, 7], "å¯¹æ‰‹ä¸­å¿ƒ+è§’è½-å è¾¹"),

            # 5. ç©ºæ£‹ç›˜
            ([0, 0, 0, 0, 0, 0, 0, 0, 0], [4], "ç©ºæ£‹ç›˜-ä¸­å¿ƒæœ€ä¼˜"),
        ]

        total = 0
        correct = 0

        for board, correct_actions, description in test_cases:
            obs = np.array(board, dtype=np.float32)
            action, _ = self.model.predict(obs, deterministic=True)

            is_correct = action in correct_actions
            total += 1
            if is_correct:
                correct += 1

            symbol = "âœ“" if is_correct else "âœ—"
            print(f"\n{symbol} {description}")
            print(f"  æ£‹ç›˜:\n{obs.reshape(3, 3)}")
            print(f"  é¢„æµ‹: {action}, æ­£ç¡®: {correct_actions}")

        accuracy = correct / total * 100
        print(f"\næ€»ä½“å‡†ç¡®ç‡: {correct}/{total} = {accuracy:.1f}%")

        if accuracy == 100:
            print("ğŸ† å®Œç¾ï¼æ‰€æœ‰å…³é”®å±€é¢éƒ½è¯†åˆ«æ­£ç¡®")
        elif accuracy >= 90:
            print("âœ“ ä¼˜ç§€ï¼å¤§éƒ¨åˆ†å…³é”®å±€é¢è¯†åˆ«æ­£ç¡®")
        elif accuracy >= 70:
            print("â–³ è‰¯å¥½ï¼Œä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("âš  éœ€è¦æ”¹è¿›ï¼Œå…³é”®å±€é¢è¯†åˆ«ç‡è¾ƒä½")

        return accuracy

    def evaluate_symmetry_consistency(self):
        """è¯„ä¼°å¯¹ç§°ä¸€è‡´æ€§ï¼ˆæœ€ä¼˜ç­–ç•¥å¿…é¡»å¯¹ç§°ç­‰ä»·ï¼‰"""
        print("\n" + "=" * 70)
        print("è¯„ä¼° 2: å¯¹ç§°æ€§ä¸€è‡´æ€§")
        print("=" * 70)

        # æµ‹è¯•ç©ºæ£‹ç›˜çš„8ç§å¯¹ç§°å˜æ¢
        print("\n[æµ‹è¯•] ç©ºæ£‹ç›˜çš„å¯¹ç§°æ€§")
        empty_board = np.zeros(9, dtype=np.float32)
        action_empty, _ = self.model.predict(empty_board, deterministic=True)

        # ä¸­å¿ƒ(4)æ˜¯å”¯ä¸€çš„ï¼Œè§’è½(0,2,6,8)ç­‰ä»·ï¼Œè¾¹(1,3,5,7)ç­‰ä»·
        if action_empty == 4:
            print(f"  âœ“ é€‰æ‹©ä¸­å¿ƒ({action_empty}) - æœ€ä¼˜")
        elif action_empty in [0, 2, 6, 8]:
            print(f"  â–³ é€‰æ‹©è§’è½({action_empty}) - æ¬¡ä¼˜ä½†å¯æ¥å—")
        else:
            print(f"  âœ— é€‰æ‹©è¾¹({action_empty}) - ä¸ä½³")

        # æµ‹è¯•å¯¹ç§°å±€é¢
        print("\n[æµ‹è¯•] å¯¹ç§°å±€é¢ä¸€è‡´æ€§")
        test_board = np.array([1, 0, -1, 0, 0, 0, 0, 0, 0], dtype=np.float32)
        print(f"  åŸå§‹æ£‹ç›˜:\n{test_board.reshape(3, 3)}")

        # æ—‹è½¬90åº¦çš„ç­‰ä»·ä½ç½®æ˜ å°„
        rotate_90 = [6, 3, 0, 7, 4, 1, 8, 5, 2]
        rotate_180 = [8, 7, 6, 5, 4, 3, 2, 1, 0]
        rotate_270 = [2, 5, 8, 1, 4, 7, 0, 3, 6]

        transformations = [
            (test_board, "åŸå§‹"),
            (test_board[rotate_90], "æ—‹è½¬90Â°"),
            (test_board[rotate_180], "æ—‹è½¬180Â°"),
            (test_board[rotate_270], "æ—‹è½¬270Â°"),
        ]

        actions = []
        for board, name in transformations:
            action, _ = self.model.predict(board, deterministic=True)
            actions.append(action)
            print(f"  {name}: é€‰æ‹©ä½ç½® {action}")

        # æ£€æŸ¥æ˜¯å¦å¯¹ç§°ç­‰ä»·ï¼ˆç®€åŒ–æ£€æŸ¥ï¼šè‡³å°‘é€‰æ‹©ç±»å‹ç›¸åŒï¼‰
        # ä¸­å¿ƒ=4, è§’è½={0,2,6,8}, è¾¹={1,3,5,7}
        def get_type(pos):
            if pos == 4:
                return "center"
            elif pos in [0, 2, 6, 8]:
                return "corner"
            else:
                return "edge"

        types = [get_type(a) for a in actions]
        if len(set(types)) == 1:
            print(f"  âœ“ å¯¹ç§°ä¸€è‡´æ€§è‰¯å¥½ï¼ˆéƒ½é€‰æ‹©{types[0]}ï¼‰")
            return True
        else:
            print(f"  âš  å¯¹ç§°ä¸ä¸€è‡´: {types}")
            return False

    def evaluate_against_perfect_play(self):
        """è¯„ä¼°å¯¹æˆ˜å®Œç¾å¯¹æ‰‹ï¼ˆMinMaxï¼‰çš„è¡¨ç°"""
        print("\n" + "=" * 70)
        print("è¯„ä¼° 3: å¯¹æˆ˜å®Œç¾å¯¹æ‰‹ (MinMax)")
        print("=" * 70)

        # å…ˆæ‰‹æµ‹è¯•
        print("\n[å…ˆæ‰‹] 50å±€ vs MinMax")
        env_x = gym.make('TicTacToe-v0', opponent_type='minmax')
        wins_x, losses_x, draws_x = 0, 0, 0

        for _ in range(50):
            obs, _ = env_x.reset()
            done = False
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = env_x.step(action)
                done = terminated or truncated
                if done:
                    if reward > 0:
                        wins_x += 1
                    elif reward < 0:
                        losses_x += 1
                    else:
                        draws_x += 1

        env_x.close()

        print(f"  èƒœ: {wins_x} ({wins_x/50*100:.1f}%)")
        print(f"  å¹³: {draws_x} ({draws_x/50*100:.1f}%)")
        print(f"  è´Ÿ: {losses_x} ({losses_x/50*100:.1f}%)")

        # åæ‰‹æµ‹è¯•ï¼ˆéœ€è¦ä¿®æ”¹ç¯å¢ƒè®©MinMaxå…ˆæ‰‹ï¼‰
        print("\n[åæ‰‹] é€šè¿‡ç¿»è½¬æ¨¡æ‹Ÿåæ‰‹å¯¹æˆ˜")
        # ä½¿ç”¨delta-selfplayç¯å¢ƒæ¨¡æ‹Ÿåæ‰‹
        from gym_env.tictactoe_delta_selfplay import TicTacToeDeltaSelfPlayEnv
        from gym_env.policies import MinMaxPlayerPolicy
        from gymnasium import spaces

        obs_space = spaces.Box(low=-1, high=1, shape=(9,), dtype=np.float32)
        act_space = spaces.Discrete(9)
        minmax_pool = [MinMaxPlayerPolicy(obs_space, act_space)]

        env_o = TicTacToeDeltaSelfPlayEnv(
            baseline_pool=minmax_pool,
            learned_pool=None,
            play_as_o_prob=1.0  # å¼ºåˆ¶åæ‰‹
        )

        wins_o, losses_o, draws_o = 0, 0, 0
        for _ in range(50):
            obs, _ = env_o.reset()
            done = False
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = env_o.step(action)
                done = terminated or truncated
                if done:
                    if reward > 0:
                        wins_o += 1
                    elif reward < 0:
                        losses_o += 1
                    else:
                        draws_o += 1

        env_o.close()

        print(f"  èƒœ: {wins_o} ({wins_o/50*100:.1f}%)")
        print(f"  å¹³: {draws_o} ({draws_o/50*100:.1f}%)")
        print(f"  è´Ÿ: {losses_o} ({losses_o/50*100:.1f}%)")

        # æ€»ä½“è¯„ä¼°
        total_draws = draws_x + draws_o
        total_losses = losses_x + losses_o
        total_games = 100

        print(f"\næ€»ä½“ (100å±€):")
        print(f"  å¹³å±€ç‡: {total_draws/total_games*100:.1f}%")
        print(f"  è¾“æ‰ç‡: {total_losses/total_games*100:.1f}%")

        # åˆ¤æ–­æ˜¯å¦æœ€ä¼˜
        is_optimal = total_draws >= 95 and total_losses == 0

        if is_optimal:
            print(f"  ğŸ† è¾¾åˆ°æœ€ä¼˜ï¼æ— è¾“å±€ä¸”å¹³å±€ç‡â‰¥95%")
        elif total_draws >= 90:
            print(f"  âœ“ æ¥è¿‘æœ€ä¼˜ï¼å¹³å±€ç‡â‰¥90%")
        elif total_draws >= 80:
            print(f"  â–³ è‰¯å¥½ï¼Œä½†è¿˜æœ‰æå‡ç©ºé—´")
        else:
            print(f"  âš  éœ€è¦æ”¹è¿›")

        return is_optimal, total_draws / total_games

    def evaluate_q_value_quality(self):
        """è¯„ä¼°Qå€¼è´¨é‡"""
        print("\n" + "=" * 70)
        print("è¯„ä¼° 4: Qå€¼è´¨é‡åˆ†æ")
        print("=" * 70)

        test_cases = [
            # (æ£‹ç›˜, æè¿°, åº”è¯¥é«˜Qå€¼çš„åŠ¨ä½œ, åº”è¯¥ä½Qå€¼çš„åŠ¨ä½œ)
            (
                np.array([1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32),
                "ç«‹å³è·èƒœæœºä¼š",
                [2],  # è·èƒœåŠ¨ä½œ
                [3, 4, 5, 6, 7, 8]  # å…¶ä»–åŠ¨ä½œ
            ),
            (
                np.array([-1, -1, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32),
                "å¿…é¡»é˜²å®ˆ",
                [2],  # é˜²å®ˆåŠ¨ä½œ
                [3, 4, 5, 6, 7, 8]  # ä¸é˜²å®ˆä¼šè¾“
            ),
            (
                np.zeros(9, dtype=np.float32),
                "ç©ºæ£‹ç›˜",
                [4],  # ä¸­å¿ƒ
                [1, 3, 5, 7]  # è¾¹ï¼ˆæœ€å·®ï¼‰
            ),
        ]

        for obs, description, high_q_actions, low_q_actions in test_cases:
            print(f"\n[{description}]")
            print(f"  æ£‹ç›˜:\n{obs.reshape(3, 3)}")

            # è·å–Qå€¼
            obs_tensor = self.model.policy.obs_to_tensor(obs)[0]
            with torch.no_grad():
                q_values = self.model.policy.q_net(obs_tensor).cpu().numpy()[0]

            legal_actions = np.where(obs == 0)[0]
            q_legal = {i: q_values[i] for i in legal_actions}

            print(f"  åˆæ³•åŠ¨ä½œQå€¼: {{{', '.join(f'{k}:{v:.3f}' for k, v in q_legal.items())}}}")

            # æ£€æŸ¥æœ€é«˜Qå€¼æ˜¯å¦åœ¨æ­£ç¡®åŠ¨ä½œä¸­
            best_action = max(q_legal, key=q_legal.get)
            best_q = q_legal[best_action]

            if best_action in high_q_actions:
                print(f"  âœ“ æœ€é«˜Qå€¼åœ¨æ­£ç¡®åŠ¨ä½œ {best_action} (Q={best_q:.3f})")
            else:
                print(f"  âœ— æœ€é«˜Qå€¼åœ¨é”™è¯¯åŠ¨ä½œ {best_action} (Q={best_q:.3f})")
                print(f"     æ­£ç¡®åŠ¨ä½œåº”è¯¥æ˜¯: {high_q_actions}")

            # æ£€æŸ¥Qå€¼å·®å¼‚
            if high_q_actions[0] in q_legal and low_q_actions:
                low_q_in_legal = [a for a in low_q_actions if a in q_legal]
                if low_q_in_legal:
                    high_q = q_legal[high_q_actions[0]]
                    low_q = np.mean([q_legal[a] for a in low_q_in_legal])
                    diff = high_q - low_q

                    if diff > 0.5:
                        print(f"  âœ“ Qå€¼åŒºåˆ†æ˜æ˜¾ (Î”={diff:.3f})")
                    elif diff > 0.1:
                        print(f"  â–³ Qå€¼æœ‰åŒºåˆ† (Î”={diff:.3f})")
                    else:
                        print(f"  âš  Qå€¼åŒºåˆ†ä¸è¶³ (Î”={diff:.3f})")

    def evaluate_exploration_vs_exploitation(self):
        """è¯„ä¼°æ¢ç´¢vsåˆ©ç”¨çš„å¹³è¡¡"""
        print("\n" + "=" * 70)
        print("è¯„ä¼° 5: ç­–ç•¥ç¨³å®šæ€§ï¼ˆç¡®å®šæ€§vséšæœºæ€§ï¼‰")
        print("=" * 70)

        obs = np.zeros(9, dtype=np.float32)

        # å¤šæ¬¡é¢„æµ‹ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
        print("\n[æµ‹è¯•] ç©ºæ£‹ç›˜é¢„æµ‹10æ¬¡ï¼ˆç¡®å®šæ€§ï¼‰")
        actions_det = [self.model.predict(obs, deterministic=True)[0] for _ in range(10)]
        unique_det = set(actions_det)

        print(f"  é¢„æµ‹ç»“æœ: {actions_det}")
        print(f"  å”¯ä¸€å€¼: {unique_det}")

        if len(unique_det) == 1:
            print(f"  âœ“ å®Œå…¨ç¡®å®šæ€§ - ç­–ç•¥ç¨³å®š")
        else:
            print(f"  âš  å­˜åœ¨éšæœºæ€§ - å¯èƒ½æ¢ç´¢ç‡æœªå½’é›¶")

        # æ£€æŸ¥å½“å‰æ¢ç´¢ç‡
        print(f"\n[å‚æ•°] å½“å‰æ¢ç´¢ç‡: {self.model.exploration_rate:.6f}")
        if self.model.exploration_rate <= 0.05:
            print(f"  âœ“ æ¢ç´¢ç‡å·²æ”¶æ•›åˆ°æœ€ä½å€¼")
        else:
            print(f"  âš  æ¢ç´¢ç‡ä»è¾ƒé«˜ï¼Œå¯èƒ½å½±å“ç¡®å®šæ€§")

    def evaluate_comprehensive(self):
        """ç»¼åˆè¯„ä¼°"""
        print("\n" + "ğŸ¯" * 35)
        print("TicTacToe ç¥ç»ç½‘ç»œè´¨é‡ç»¼åˆè¯„ä¼°")
        print("ğŸ¯" * 35 + "\n")

        print(f"æ¨¡å‹è·¯å¾„: {self.model.num_timesteps} æ­¥è®­ç»ƒ")
        print(f"ç½‘ç»œç»“æ„: {self.model.policy.net_arch}")
        print()

        # æ‰§è¡Œæ‰€æœ‰è¯„ä¼°
        critical_accuracy = self.evaluate_critical_positions()
        symmetry_ok = self.evaluate_symmetry_consistency()
        is_optimal, draw_rate = self.evaluate_against_perfect_play()
        self.evaluate_q_value_quality()
        self.evaluate_exploration_vs_exploitation()

        # æœ€ç»ˆåˆ¤æ–­
        print("\n" + "=" * 70)
        print("æœ€ç»ˆåˆ¤æ–­: ç­–ç•¥æ˜¯å¦æœ€ä¼˜ï¼Ÿ")
        print("=" * 70)

        criteria = {
            "å…³é”®å±€é¢è¯†åˆ«": critical_accuracy >= 90,
            "å¯¹ç§°æ€§ä¸€è‡´": symmetry_ok,
            "vs MinMaxå¹³å±€ç‡": draw_rate >= 0.95,
            "æ— è¾“å±€": is_optimal,
        }

        print("\nè¯„ä¼°æ ‡å‡†:")
        for criterion, passed in criteria.items():
            symbol = "âœ“" if passed else "âœ—"
            print(f"  {symbol} {criterion}")

        all_passed = all(criteria.values())

        print("\n" + "=" * 70)
        if all_passed:
            print("ğŸ† æ­å–œï¼ä½ çš„ç¥ç»ç½‘ç»œå·²è¾¾åˆ°TicTacToeçš„æœ€ä¼˜ç­–ç•¥ï¼")
            print()
            print("è¯æ®:")
            print(f"  1. å…³é”®å±€é¢è¯†åˆ«å‡†ç¡®ç‡: {critical_accuracy:.1f}%")
            print(f"  2. vs MinMaxå¹³å±€ç‡: {draw_rate*100:.1f}%")
            print(f"  3. å¯¹ç§°æ€§ä¸€è‡´: {'æ˜¯' if symmetry_ok else 'å¦'}")
            print()
            print("è¿™æ„å‘³ç€:")
            print("  â€¢ å…ˆæ‰‹: æ°¸ä¸ä¼šè¾“ï¼Œé™¤éè‡ªå·±çŠ¯é”™")
            print("  â€¢ åæ‰‹: æ°¸ä¸ä¼šè¾“ç»™å®Œç¾å¯¹æ‰‹")
            print("  â€¢ æ‰€æœ‰å…³é”®æˆ˜æœ¯ç‚¹ï¼ˆè·èƒœ/é˜²å®ˆ/å‰æ”»ï¼‰éƒ½èƒ½è¯†åˆ«")
            print()
            print("ä¸‹ä¸€æ­¥å»ºè®®:")
            print("  1. ä½¿ç”¨VIPERæå–å†³ç­–æ ‘")
            print("  2. å¯è§†åŒ–ç­–ç•¥è§„åˆ™")
            print("  3. å½¢å¼åŒ–éªŒè¯")

        elif draw_rate >= 0.9:
            print("âœ“ ä½ çš„ç¥ç»ç½‘ç»œæ¥è¿‘æœ€ä¼˜ç­–ç•¥ï¼")
            print()
            print(f"  â€¢ vs MinMaxå¹³å±€ç‡: {draw_rate*100:.1f}% (ç›®æ ‡: â‰¥95%)")
            print(f"  â€¢ å…³é”®å±€é¢å‡†ç¡®ç‡: {critical_accuracy:.1f}% (ç›®æ ‡: â‰¥90%)")
            print()
            print("æ”¹è¿›å»ºè®®:")
            if critical_accuracy < 90:
                print("  â€¢ ç»§ç»­è®­ç»ƒï¼Œæé«˜å…³é”®å±€é¢è¯†åˆ«")
            if draw_rate < 0.95:
                print("  â€¢ å¢åŠ MinMaxåŸºå‡†å¯¹æ‰‹çš„è®­ç»ƒæ¯”ä¾‹")

        else:
            print("â–³ ç­–ç•¥è‰¯å¥½ï¼Œä½†è·ç¦»æœ€ä¼˜è¿˜æœ‰è·ç¦»")
            print()
            print("éœ€è¦æ”¹è¿›:")
            for criterion, passed in criteria.items():
                if not passed:
                    print(f"  â€¢ {criterion}")

        print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°TicTacToeç¥ç»ç½‘ç»œè´¨é‡')
    parser.add_argument('--model', type=str,
                        default='log/oracle_TicTacToe_delta_selfplay.zip',
                        help='æ¨¡å‹è·¯å¾„')
    args = parser.parse_args()

    evaluator = TicTacToeNNEvaluator(args.model)
    evaluator.evaluate_comprehensive()


if __name__ == "__main__":
    main()
