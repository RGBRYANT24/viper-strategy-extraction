# TicTacToe Action Masking è®­ç»ƒæ–¹æ¡ˆ - å®Œæ•´æŒ‡å—

> ç›®æ ‡ï¼šä½¿ç”¨ Action Mask é˜²æ­¢éæ³•ç§»åŠ¨ï¼Œè®­ç»ƒTicTacToeæœ€ä¼˜ç­–ç•¥ï¼Œå¹¶ç”¨VIPERæå–å†³ç­–æ ‘

## ç›®å½•
- [é—®é¢˜èƒŒæ™¯](#é—®é¢˜èƒŒæ™¯)
- [æ–¹æ¡ˆé€‰æ‹©](#æ–¹æ¡ˆé€‰æ‹©)
- [å½“å‰è¿›åº¦](#å½“å‰è¿›åº¦)
- [ç»§ç»­å·¥ä½œæŒ‡å—](#ç»§ç»­å·¥ä½œæŒ‡å—)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [FAQ](#faq)

---

## é—®é¢˜èƒŒæ™¯

### åŸå§‹é—®é¢˜
- ä½¿ç”¨ `-10` æƒ©ç½šéæ³•åŠ¨ä½œ â†’ Qå€¼æ±¡æŸ“
- éœ€è¦æ”¹ç”¨ Action Mask åœ¨ç¥ç»ç½‘ç»œå±‚é¢å±è”½éæ³•åŠ¨ä½œ

### å°è¯•è¿‡çš„æ–¹æ¡ˆ
1. **MaskedDQNPolicy**ï¼ˆå·²å®ç°ä½†æœ‰é—®é¢˜ï¼‰
   - æ–‡ä»¶ï¼š[gym_env/masked_dqn_policy.py](gym_env/masked_dqn_policy.py)
   - é—®é¢˜ï¼šDQN çš„ Îµ-greedy exploration ç»•è¿‡äº† mask
   - ç»“æœï¼šè®­ç»ƒåˆæœŸä»æœ‰ 59.4% çš„éæ³•ç§»åŠ¨

### æ ¸å¿ƒé—®é¢˜è¯Šæ–­
```python
# DQN çš„ exploration æœºåˆ¶
if random() < exploration_rate:  # åˆæœŸ exploration_rate=1.0
    action = env.action_space.sample()  # âŒ ç›´æ¥éšæœºï¼Œç»•è¿‡ policy!
else:
    action = policy.predict(obs)         # âœ… ç»è¿‡ mask
```

**ç—‡çŠ¶**ï¼šè®­ç»ƒæ—¥å¿—æ˜¾ç¤º `ep_rew_mean = -6.18`ï¼ˆå¤§é‡ -10 æƒ©ç½šï¼‰

---

## æ–¹æ¡ˆé€‰æ‹©

### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å·¥ä½œé‡ | VIPERå…¼å®¹ |
|------|------|------|--------|----------|
| **A. MaskablePPO** â­ | åŸç”Ÿæ”¯æŒmask<br>æ— Îµ-greedyé—®é¢˜<br>å·²éªŒè¯å…¼å®¹ | éœ€è¦sb3-contrib | â­â­ | âœ… |
| B. è‡ªå®šä¹‰MaskedDQN | åªéœ€sb3<br>è®­ç»ƒå¿« | éœ€é‡å†™collect_rollouts<br>å¤æ‚æ˜“é”™ | â­â­â­â­ | âœ… |

### æ¨èæ–¹æ¡ˆï¼šMaskablePPO

**ç†ç”±**ï¼š
1. âœ… å·²éªŒè¯ï¼š`MaskablePPO.predict()` æ¥å£ä¸ VIPER å®Œå…¨å…¼å®¹
2. âœ… åŸç”Ÿæ”¯æŒï¼šä¸“é—¨ä¸º action masking è®¾è®¡
3. âœ… å·²æœ‰ä»£ç ï¼š`train/train_delta_selfplay_ppo.py` å¯ç›´æ¥ä½¿ç”¨
4. âœ… æ— ç»•è¿‡é—®é¢˜ï¼šPPO ç”¨ç­–ç•¥æ¢¯åº¦ï¼Œä¸ç”¨éšæœº exploration

**éªŒè¯ç»“æœ**ï¼ˆå·²åœ¨æœåŠ¡å™¨ç¡®è®¤ï¼‰ï¼š
```python
# MaskablePPO.predict ç­¾å
def predict(
    observation,
    state=None,
    episode_start=None,
    deterministic=False,
    action_masks=None  # â­ å¯é€‰å‚æ•°ï¼ŒVIPERè°ƒç”¨æ—¶ä¸éœ€è¦
) -> Tuple[np.ndarray, Optional[Tuple]]

# æµ‹è¯•é€šè¿‡
action, state = model.predict(obs, deterministic=True)  # âœ“ ä¸éœ€è¦é¢å¤–å‚æ•°
```

---

## å½“å‰è¿›åº¦

### âœ… å·²å®Œæˆ

1. **MaskedDQNPolicy å®ç°**
   - æ–‡ä»¶ï¼š[gym_env/masked_dqn_policy.py](gym_env/masked_dqn_policy.py)
   - çŠ¶æ€ï¼šå®Œæˆä½†æœ‰ exploration é—®é¢˜

2. **è®­ç»ƒè„šæœ¬ä¿®æ”¹**
   - æ–‡ä»¶ï¼š[train/train_delta_selfplay.py](train/train_delta_selfplay.py)
   - çŠ¶æ€ï¼šä½¿ç”¨ MaskedDQNPolicyï¼Œä½†è®­ç»ƒæ—¶ä»æœ‰éæ³•ç§»åŠ¨

3. **è¯„ä¼°å·¥å…·å¢å¼º**
   - æ–‡ä»¶ï¼š[evaluate_nn_quality.py](evaluate_nn_quality.py)
   - çŠ¶æ€ï¼šæ·»åŠ äº† Action Masking æµ‹è¯•åŠŸèƒ½

4. **PPO è®­ç»ƒè„šæœ¬**
   - æ–‡ä»¶ï¼š[train/train_delta_selfplay_ppo.py](train/train_delta_selfplay_ppo.py)
   - çŠ¶æ€ï¼šå·²å­˜åœ¨ä¸”å¯ç”¨

5. **ä¾èµ–æ›´æ–°**
   - æ–‡ä»¶ï¼š[requirements.txt](requirements.txt)
   - çŠ¶æ€ï¼šå·²æ·»åŠ  `sb3-contrib==2.4.0`

6. **å…¼å®¹æ€§éªŒè¯**
   - çŠ¶æ€ï¼šâœ… å·²ç¡®è®¤ MaskablePPO ä¸ VIPER å®Œå…¨å…¼å®¹

### ğŸ”² å¾…å®Œæˆ

1. ä¿®æ”¹ [train/oracle.py](train/oracle.py) æ·»åŠ  MaskablePPO é…ç½®
2. åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_maskable_ppo_viper.py`
3. è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹å¹¶éªŒè¯
4. VIPER å†³ç­–æ ‘æå–

---

## ç»§ç»­å·¥ä½œæŒ‡å—

### ç»™ Claude Code çš„ Prompt

```
æˆ‘åœ¨ä½¿ç”¨ VIPER æ¡†æ¶è®­ç»ƒ TicTacToeï¼Œéœ€è¦ç”¨ Action Mask é˜²æ­¢éæ³•ç§»åŠ¨ã€‚

èƒŒæ™¯ï¼š
- å°è¯•äº† MaskedDQNPolicyï¼Œä½† DQN çš„ Îµ-greedy ä¼šç»•è¿‡ mask
- å·²éªŒè¯ MaskablePPO ä¸ VIPER å®Œå…¨å…¼å®¹
- å·²æœ‰ train/train_delta_selfplay_ppo.py è®­ç»ƒè„šæœ¬
- å·²æ·»åŠ  sb3-contrib==2.4.0 ä¾èµ–

ç°åœ¨éœ€è¦ï¼š
1. ä¿®æ”¹ train/oracle.pyï¼Œæ·»åŠ  TicTacToe-MaskablePPO é…ç½®ï¼ˆç”¨äºVIPERé›†æˆï¼‰
2. åˆ›å»ºæµ‹è¯•è„šæœ¬éªŒè¯ MaskablePPO + VIPER æµç¨‹
3. æ›´æ–°æ–‡æ¡£è¯´æ˜ä½¿ç”¨æ–¹æ³•

å‚è€ƒï¼šMASKED_TRAINING_GUIDE.md
```

### ä»»åŠ¡æ¸…å•

#### ä»»åŠ¡1ï¼šä¿®æ”¹ oracle.py

**æ–‡ä»¶**ï¼š[train/oracle.py](train/oracle.py)

**ä½ç½®1**ï¼šåœ¨ç¬¬1-6è¡Œå¯¼å…¥ MaskablePPO

```python
from stable_baselines3 import DQN, PPO
try:
    from sb3_contrib import MaskablePPO
    HAS_MASKABLE_PPO = True
except ImportError:
    HAS_MASKABLE_PPO = False
```

**ä½ç½®2**ï¼šåœ¨ `ENV_TO_MODEL` å­—å…¸ä¸­æ·»åŠ é…ç½®ï¼ˆç¬¬75è¡Œä¹‹åï¼‰

```python
'TicTacToe-MaskablePPO-v0': {
    'model': MaskablePPO,
    'kwargs': {
        'policy': 'MlpPolicy',
        'learning_rate': 1e-3,
        'n_steps': 128,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.01,
        'policy_kwargs': {
            'net_arch': [128, 128]
        }
    }
}
```

#### ä»»åŠ¡2ï¼šåˆ›å»ºæµ‹è¯•è„šæœ¬

**æ–°æ–‡ä»¶**ï¼š`test_maskable_ppo_viper.py`ï¼ˆè§[æŠ€æœ¯ç»†èŠ‚](#æµ‹è¯•è„šæœ¬å®Œæ•´ä»£ç )ï¼‰

#### ä»»åŠ¡3ï¼šè¿è¡Œè®­ç»ƒå’ŒéªŒè¯

```bash
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ2åˆ†é’Ÿï¼‰
python3 train/train_delta_selfplay_ppo.py \
    --total-timesteps 20000 \
    --n-env 4 \
    --output log/test_ppo.zip

# 2. æ£€æŸ¥æ—¥å¿—ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
# é¢„æœŸï¼šep_rew_mean åº”è¯¥åœ¨ [-1, 1] èŒƒå›´ï¼ˆæ—  -10ï¼‰
# é¢„æœŸï¼šéæ³•ç§»åŠ¨ = 0

# 3. å®Œæ•´è®­ç»ƒï¼ˆ20åˆ†é’Ÿï¼‰
python3 train/train_delta_selfplay_ppo.py \
    --total-timesteps 200000 \
    --n-env 8 \
    --use-minmax \
    --output log/oracle_TicTacToe_ppo.zip

# 4. è¯„ä¼°è´¨é‡
python3 evaluate_nn_quality.py --model log/oracle_TicTacToe_ppo.zip

# 5. æµ‹è¯• VIPER å…¼å®¹æ€§
python3 test_maskable_ppo_viper.py

# 6. æå–å†³ç­–æ ‘
python3 main.py \
    --train-viper \
    --env-name TicTacToe-MaskablePPO-v0 \
    --oracle log/oracle_TicTacToe_ppo.zip
```

---

## æŠ€æœ¯ç»†èŠ‚

### MaskablePPO å·¥ä½œåŸç†

```python
# 1. ç¯å¢ƒåŒ…è£…
env = TicTacToeDeltaSelfPlayEnv(...)
env = ActionMasker(env, mask_fn)  # æ·»åŠ  mask wrapper

def mask_fn(env):
    """è¿”å›åˆæ³•åŠ¨ä½œçš„ mask"""
    board = env.board
    return (board == 0).astype(np.int8)  # 0=ç©ºä½=åˆæ³•

# 2. è®­ç»ƒ
model = MaskablePPO('MlpPolicy', env)
model.learn(total_timesteps=200000)  # mask è‡ªåŠ¨ç”Ÿæ•ˆ

# 3. é¢„æµ‹ï¼ˆVIPER è°ƒç”¨æ–¹å¼ï¼‰
action, state = model.predict(obs, deterministic=True)
# ActionMasker è‡ªåŠ¨æä¾› maskï¼Œpolicy è‡ªåŠ¨åº”ç”¨
```

### ä¸ºä»€ä¹ˆ PPO ä¸ä¼šç»•è¿‡ maskï¼Ÿ

```python
# PPO çš„åŠ¨ä½œé€‰æ‹©ï¼ˆç®€åŒ–ï¼‰
logits = policy_network(obs)           # å¾—åˆ°æ¯ä¸ªåŠ¨ä½œçš„åˆ†æ•°
masked_logits = logits.masked_fill(    # å°†éæ³•åŠ¨ä½œçš„åˆ†æ•°è®¾ä¸º -inf
    ~action_mask, float('-inf')
)
probs = softmax(masked_logits)         # éæ³•åŠ¨ä½œçš„æ¦‚ç‡ = 0
action = sample(probs)                  # æ°¸è¿œä¸ä¼šé‡‡æ ·åˆ°éæ³•åŠ¨ä½œ

# å¯¹æ¯” DQN çš„ Îµ-greedy
if random() < epsilon:
    action = random_choice(all_actions)  # âŒ å¯èƒ½é€‰åˆ°éæ³•åŠ¨ä½œ
```

### MaskablePPO vs MaskedDQN å¯¹æ¯”è¡¨

| ç‰¹æ€§ | MaskablePPO | MaskedDQN (è‡ªå®šä¹‰) |
|------|------------|-------------------|
| éæ³•ç§»åŠ¨é—®é¢˜ | âœ… è§£å†³ | âŒ éœ€é‡å†™ collect_rollouts |
| VIPER å…¼å®¹æ€§ | âœ… éªŒè¯é€šè¿‡ | âœ… ç†è®ºå…¼å®¹ |
| å®ç°å¤æ‚åº¦ | â­ ä½ | â­â­â­â­ é«˜ |
| ä¾èµ– | sb3-contrib | stable-baselines3 |
| è®­ç»ƒé€Ÿåº¦ | ä¸­ç­‰ | å¿« |
| è°ƒè¯•éš¾åº¦ | ä½ | é«˜ |

### æµ‹è¯•è„šæœ¬å®Œæ•´ä»£ç 

```python
"""
test_maskable_ppo_viper.py
æµ‹è¯• MaskablePPO ä¸ VIPER çš„å…¼å®¹æ€§
"""

import gymnasium as gym
import numpy as np
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import gym_env

def mask_fn(env):
    """è¿”å› action mask"""
    if hasattr(env, 'board'):
        board = env.board
    else:
        board = env.env.board
    return (board == 0).astype(np.int8)

print("=" * 70)
print("æµ‹è¯• MaskablePPO ä¸ VIPER å…¼å®¹æ€§")
print("=" * 70)

# 1. åˆ›å»ºç¯å¢ƒ
print("\n1. åˆ›å»ºç¯å¢ƒ...")
from gym_env.tictactoe_delta_selfplay import TicTacToeDeltaSelfPlayEnv
from gym_env.policies.baseline_policies import RandomPlayerPolicy

obs_space = gym.spaces.Box(low=-1, high=1, shape=(9,), dtype=np.float32)
act_space = gym.spaces.Discrete(9)
baseline_pool = [RandomPlayerPolicy(obs_space, act_space)]

env = TicTacToeDeltaSelfPlayEnv(
    baseline_pool=baseline_pool,
    learned_pool=None
)
env = ActionMasker(env, mask_fn)
print("âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")

# 2. åˆ›å»ºæ¨¡å‹
print("\n2. åˆ›å»º MaskablePPO æ¨¡å‹...")
model = MaskablePPO(
    policy='MlpPolicy',
    env=env,
    learning_rate=1e-3,
    n_steps=128,
    batch_size=64,
    verbose=1
)
print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")

# 3. çŸ­æš‚è®­ç»ƒ
print("\n3. è®­ç»ƒ 5000 æ­¥...")
model.learn(total_timesteps=5000)
print("âœ“ è®­ç»ƒå®Œæˆ")

# 4. æµ‹è¯• predict() æ¥å£ï¼ˆæ¨¡æ‹Ÿ VIPER è°ƒç”¨ï¼‰
print("\n4. æµ‹è¯• VIPER å…¼å®¹çš„ predict() æ¥å£...")
obs, _ = env.reset()

# æ¨¡æ‹Ÿ VIPER è°ƒç”¨ï¼ˆä¸æä¾› action_masksï¼‰
action, state = model.predict(obs, deterministic=True)
print(f"  âœ“ predict(obs, deterministic=True) æˆåŠŸ")
print(f"    è¿”å›: action={action}, state={state}")

# 5. æ£€æŸ¥éæ³•ç§»åŠ¨
print("\n5. æµ‹è¯• 100 å±€ï¼Œæ£€æŸ¥éæ³•ç§»åŠ¨...")
illegal_count = 0
total_reward = 0

for _ in range(100):
    obs, _ = env.reset()
    done = False
    episode_reward = 0

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        episode_reward += reward
        done = terminated or truncated

        if done and 'illegal_move' in info and info['illegal_move']:
            illegal_count += 1

    total_reward += episode_reward

avg_reward = total_reward / 100

print(f"\nç»“æœ:")
print(f"  éæ³•ç§»åŠ¨æ¬¡æ•°: {illegal_count}/100")
print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")

if illegal_count == 0:
    print("\nâœ… æµ‹è¯•é€šè¿‡ï¼")
    print("  - æ— éæ³•ç§»åŠ¨")
    print("  - MaskablePPO æ­£å¸¸å·¥ä½œ")
    print("  - ä¸ VIPER æ¥å£å…¼å®¹")
else:
    print(f"\nâš  è­¦å‘Šï¼šæœ‰ {illegal_count} æ¬¡éæ³•ç§»åŠ¨")

env.close()
```

### ç¯å¢ƒä¸ VIPER çš„é›†æˆç‚¹

```python
# train/viper.py:157
action, _states = active_policy.predict(obs, deterministic=True)

# MaskablePPO å®Œå…¨å…¼å®¹è¿™ä¸ªæ¥å£
# action_masks å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä¸æä¾›æ—¶ï¼š
# - å¦‚æœç¯å¢ƒæœ‰ ActionMasker wrapper â†’ è‡ªåŠ¨è·å– mask
# - å¦‚æœæ²¡æœ‰ wrapper â†’ ä¸ä½¿ç”¨ maskï¼ˆä½†ä¸ä¼šæŠ¥é”™ï¼‰
```

---

## é¢„æœŸç»“æœ

### è®­ç»ƒæ—¥å¿—ï¼ˆæ­£å¸¸æƒ…å†µï¼‰

```
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 3.2      |
|    ep_rew_mean      | -0.3     |  â­ åº”è¯¥æ¥è¿‘ 0ï¼Œä¸æ˜¯ -6.18
| time/               |          |
|    episodes         | 600      |
----------------------------------

æµ‹è¯•ç»“æœ (50å±€ vs MinMax):
  èƒœ: 2 (4.0%)
  è´Ÿ: 3 (6.0%)
  å¹³: 45 (90.0%)
  éæ³•ç§»åŠ¨: 0  â­ å…³é”®æŒ‡æ ‡
```

### è¯„ä¼°è¾“å‡ºï¼ˆæœ€ä¼˜ç­–ç•¥ï¼‰

```
âœ“ Action Masking åŠŸèƒ½æ­£å¸¸å·¥ä½œ
âœ“ å…³é”®å±€é¢è¯†åˆ«: 100.0%
âœ“ vs MinMaxå¹³å±€ç‡: 98.0%
âœ“ æ— è¾“å±€

ğŸ† æ­å–œï¼ä½ çš„ç¥ç»ç½‘ç»œå·²è¾¾åˆ°TicTacToeçš„æœ€ä¼˜ç­–ç•¥ï¼
```

---

## FAQ

### Q1: ä¸ºä»€ä¹ˆä¸ç»§ç»­ç”¨ DQNï¼Ÿ
**A**: DQN çš„ Îµ-greedy exploration ä¼šç»•è¿‡ maskã€‚è¦è§£å†³éœ€è¦é‡å†™ `collect_rollouts()`ï¼Œå·¥ä½œé‡å¤§ä¸”å®¹æ˜“å‡ºé”™ã€‚

### Q2: MaskablePPO è®­ç»ƒä¼šæ…¢å¤šå°‘ï¼Ÿ
**A**: æ¯” DQN æ…¢çº¦ 20-30%ï¼Œä½†æ¯”è‡ªå·±è°ƒè¯• MaskedDQN å¿«å¾—å¤šã€‚TicTacToe è§„æ¨¡å°ï¼Œé€Ÿåº¦å·®å¼‚ä¸æ˜æ˜¾ã€‚

### Q3: å¦‚ä½•ç¡®è®¤ mask ç”Ÿæ•ˆäº†ï¼Ÿ
**A**: çœ‹ä¸¤ä¸ªæŒ‡æ ‡ï¼š
1. è®­ç»ƒæ—¥å¿—ï¼š`ep_rew_mean` åº”è¯¥åœ¨ [-1, 1]ï¼Œä¸åº”è¯¥æœ‰ -10
2. æµ‹è¯•ç»“æœï¼š`éæ³•ç§»åŠ¨: 0`

### Q4: ActionMasker ä¼šå½±å“ VIPER å—ï¼Ÿ
**A**: ä¸ä¼šã€‚VIPER è°ƒç”¨ `predict(obs, deterministic=True)` æ—¶ï¼ŒActionMasker åªæ˜¯åœ¨å†…éƒ¨æä¾› maskï¼Œä¸æ”¹å˜å¤–éƒ¨æ¥å£ã€‚

### Q5: ä¸ºä»€ä¹ˆ sb3-contrib è¦ç”¨ 2.4.0ï¼Ÿ
**A**: ä¸ä½ ç¯å¢ƒä¸­çš„ stable-baselines3==1.5.0 ç‰ˆæœ¬åŒ¹é…ã€‚å¤ªé«˜ç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹ã€‚

### Q6: å¦‚æœ MaskablePPO ä¹Ÿå¤±è´¥æ€ä¹ˆåŠï¼Ÿ
**A**: å›é€€åˆ°æ–¹æ¡ˆBï¼ˆè‡ªå®šä¹‰ MaskedDQNï¼‰ã€‚æ–‡ä»¶å·²åˆ›å»ºï¼š[gym_env/masked_dqn.py](gym_env/masked_dqn.py)ï¼ˆè™½ç„¶è¢«æ‹’ç»ä½†å¯ä»¥æ¢å¤ï¼‰ã€‚

### Q7: ç¯å¢ƒçš„ -10 æƒ©ç½šè¿˜ä¿ç•™å—ï¼Ÿ
**A**: æ˜¯çš„ï¼Œä¿ç•™åœ¨ [tictactoe_delta_selfplay.py:144](gym_env/tictactoe_delta_selfplay.py#L144)ã€‚ä½œä¸ºå®‰å…¨ç½‘ï¼Œå¦‚æœ mask å¤±æ•ˆä¼šæš´éœ²é—®é¢˜ã€‚

---

## å…³é”®æ–‡ä»¶æ¸…å•

### éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶
- [ ] [train/oracle.py](train/oracle.py) - æ·»åŠ  MaskablePPO é…ç½®
- [ ] æ–°å»º `test_maskable_ppo_viper.py` - æµ‹è¯•è„šæœ¬

### å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ–‡ä»¶
- âœ… [train/train_delta_selfplay_ppo.py](train/train_delta_selfplay_ppo.py)
- âœ… [requirements.txt](requirements.txt)
- âœ… [evaluate_nn_quality.py](evaluate_nn_quality.py)

### å‚è€ƒæ–‡ä»¶ï¼ˆå·²å®ç°ä½†æœ‰é—®é¢˜ï¼‰
- [gym_env/masked_dqn_policy.py](gym_env/masked_dqn_policy.py) - MaskedDQNPolicyï¼ˆæœ‰ exploration é—®é¢˜ï¼‰
- [train/train_delta_selfplay.py](train/train_delta_selfplay.py) - DQN è®­ç»ƒè„šæœ¬ï¼ˆéæ³•ç§»åŠ¨ç‡é«˜ï¼‰

---

## è¿›åº¦è¿½è¸ª

- [x] åˆ†æé—®é¢˜æ ¹å› ï¼ˆDQN Îµ-greedy ç»•è¿‡ maskï¼‰
- [x] éªŒè¯ MaskablePPO ä¸ VIPER å…¼å®¹æ€§
- [x] æ›´æ–°ä¾èµ–æ–‡ä»¶
- [ ] ä¿®æ”¹ oracle.py
- [ ] åˆ›å»ºæµ‹è¯•è„šæœ¬
- [ ] è¿è¡Œå®Œæ•´è®­ç»ƒ
- [ ] éªŒè¯æ— éæ³•ç§»åŠ¨
- [ ] VIPER å†³ç­–æ ‘æå–
- [ ] æ–‡æ¡£å®Œå–„

---

**å½“å‰çŠ¶æ€**: æ–¹æ¡ˆå·²ç¡®å®šï¼Œç­‰å¾…å®æ–½
**æ¨èæ–¹æ¡ˆ**: MaskablePPO
**é¢„è®¡å®Œæˆæ—¶é—´**: 1-2 å°æ—¶ï¼ˆå«è®­ç»ƒï¼‰
**æœ€åæ›´æ–°**: 2025-01-XX
