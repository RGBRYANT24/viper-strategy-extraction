# MaskablePPO ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install sb3-contrib
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
# åŸºç¡€è®­ç»ƒï¼ˆæ¨èé…ç½®ï¼‰
python train/train_delta_selfplay_ppo.py \
    --total-timesteps 250000 \
    --max-pool-size 5 \
    --use-minmax \
    --output log/oracle_TicTacToe_ppo_masked.zip

# å¿«é€Ÿæµ‹è¯•ï¼ˆ5ä¸‡æ­¥ï¼‰
python train/train_delta_selfplay_ppo.py \
    --total-timesteps 50000 \
    --max-pool-size 3 \
    --use-minmax \
    --output log/oracle_TicTacToe_ppo_test.zip

# å®Œæ•´è®­ç»ƒï¼ˆ40ä¸‡æ­¥ï¼Œå¤§ç½‘ç»œï¼‰
python train/train_delta_selfplay_ppo.py \
    --total-timesteps 400000 \
    --max-pool-size 5 \
    --use-minmax \
    --net-arch "256,256" \
    --output log/oracle_TicTacToe_ppo_big.zip
```

### 3. è¯„ä¼°æ¨¡å‹

```bash
# æ³¨æ„ï¼ševaluate_nn_quality.py éœ€è¦ç¨å¾®ä¿®æ”¹æ‰èƒ½æ”¯æŒ PPO
# ç›®å‰å®ƒæ˜¯ä¸º DQN è®¾è®¡çš„

# å¿«é€Ÿæµ‹è¯•
python -c "
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import gymnasium as gym
import gym_env

def mask_fn(env):
    return (env.board == 0).astype(int)

model = MaskablePPO.load('log/oracle_TicTacToe_ppo_masked.zip')
env = gym.make('TicTacToe-v0', opponent_type='minmax')
env = ActionMasker(env, mask_fn)

wins, draws, losses = 0, 0, 0
for _ in range(50):
    obs, _ = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            if reward > 0: wins += 1
            elif reward < 0: losses += 1
            else: draws += 1

print(f'èƒœ:{wins} å¹³:{draws} è´Ÿ:{losses}')
"
```

---

## ğŸ” æŸ¥çœ‹æºç ï¼ˆç†è§£å†…éƒ¨æœºåˆ¶ï¼‰

### æŸ¥çœ‹ Masking çš„å®ç°ä½ç½®

```bash
# 1. ActionMasker åŒ…è£…å™¨ï¼ˆå¦‚ä½•æ·»åŠ  mask åˆ° infoï¼‰
python -c "from sb3_contrib.common.wrappers import ActionMasker; import inspect; print(inspect.getsource(ActionMasker))"

# 2. MaskableActorCriticPolicy.forwardï¼ˆæ¨ç†æ—¶å¦‚ä½• maskï¼‰
python -c "from sb3_contrib.ppo_mask.policies import MaskableActorCriticPolicy; import inspect; print(inspect.getsource(MaskableActorCriticPolicy.forward))"

# 3. MaskableActorCriticPolicy.evaluate_actionsï¼ˆè®­ç»ƒæ—¶å¦‚ä½• maskï¼‰
python -c "from sb3_contrib.ppo_mask.policies import MaskableActorCriticPolicy; import inspect; print(inspect.getsource(MaskableActorCriticPolicy.evaluate_actions))"

# 4. MaskablePPO.trainï¼ˆå®Œæ•´è®­ç»ƒå¾ªç¯ï¼‰
python -c "from sb3_contrib import MaskablePPO; import inspect; print(inspect.getsource(MaskablePPO.train))"

# 5. MaskablePPO.predictï¼ˆæ¨ç†ï¼‰
python -c "from sb3_contrib import MaskablePPO; import inspect; print(inspect.getsource(MaskablePPO.predict))"
```

### æŸ¥çœ‹æºç æ–‡ä»¶ä½ç½®

```bash
# æ‰¾åˆ° sb3-contrib å®‰è£…ä½ç½®
python -c "from sb3_contrib import MaskablePPO; import inspect; print(inspect.getfile(MaskablePPO))"

# æŸ¥çœ‹å®Œæ•´ç›®å½•ç»“æ„
python -c "from sb3_contrib import MaskablePPO; import os; print(os.path.dirname(inspect.getfile(MaskablePPO)))" | xargs ls -la
```

---

## ğŸ“Š æ ¸å¿ƒæœºåˆ¶è§£é‡Š

### ActionMasker: ç¯å¢ƒåŒ…è£…å™¨

```python
# ä½ çš„ä»£ç ä¸­
def mask_fn(env):
    """è¿”å›åˆæ³•åŠ¨ä½œçš„ mask"""
    board = env.board
    return (board == 0).astype(np.int8)  # 1=åˆæ³•, 0=éæ³•

env = ActionMasker(env, mask_fn)

# å†…éƒ¨æœºåˆ¶ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰
# æ¯æ¬¡ reset() å’Œ step() åï¼š
info['action_masks'] = mask_fn(env)
# ä¾‹å¦‚: [1, 1, 1, 1, 0, 1, 1, 1, 1]
#              ä½ç½®4å·²å ç”¨ â†‘
```

### Predict: æ¨ç†æ—¶çš„ Masking

```python
# ä½ è°ƒç”¨
action, _ = model.predict(obs, deterministic=True)

# å†…éƒ¨æœºåˆ¶ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰
# 1. è·å– logits (æ‰€æœ‰åŠ¨ä½œçš„æœªå½’ä¸€åŒ–åˆ†æ•°)
logits = policy_network(obs)  # [2.1, 1.5, 0.3, 1.8, 0.9, ...]

# 2. åº”ç”¨ mask (â­ å…³é”®æ­¥éª¤)
# ä»ç¯å¢ƒ info è·å– action_masks
masks = env.get_action_masks()  # [1, 1, 1, 1, 0, 1, 1, 1, 1]

# éæ³•åŠ¨ä½œçš„ logit è®¾ä¸º -inf
masked_logits = torch.where(
    masks.bool(),        # mask == 1 (åˆæ³•)
    logits,              # ä¿ç•™åŸå€¼
    torch.tensor(-1e8)   # mask == 0 (éæ³•) â†’ -inf
)
# ç»“æœ: [2.1, 1.5, 0.3, 1.8, -1e8, ...]
#                            â†‘ ä½ç½®4éæ³•

# 3. é€‰æ‹©åŠ¨ä½œ
if deterministic:
    action = torch.argmax(masked_logits)  # æ°¸è¿œä¸ä¼šé€‰åˆ°ä½ç½®4
else:
    probs = softmax(masked_logits)  # ä½ç½®4çš„æ¦‚ç‡ = 0
    action = sample(probs)
```

### Train: è®­ç»ƒæ—¶çš„ Masking

```python
# è®­ç»ƒå¾ªç¯ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰
for epoch in range(n_epochs):
    for batch in rollout_buffer:

        # 1. é‡æ–°è¯„ä¼°åŠ¨ä½œï¼ˆå¸¦ maskï¼‰
        values, log_probs, entropy = policy.evaluate_actions(
            obs=batch.obs,
            actions=batch.actions,
            action_masks=batch.action_masks  # â­ ä½¿ç”¨ä¿å­˜çš„ mask
        )

        # evaluate_actions å†…éƒ¨ï¼ˆâ­ å…³é”®ï¼‰:
        logits = policy_network(obs)
        # åº”ç”¨ mask
        masked_logits = torch.where(
            action_masks.bool(),
            logits,
            torch.tensor(-1e8)
        )
        # è®¡ç®— log_prob (åªå¯¹åˆæ³•åŠ¨ä½œ)
        distribution = Categorical(logits=masked_logits)
        log_probs = distribution.log_prob(actions)

        # 2. è®¡ç®—æŸå¤±
        # â­ å…³é”®ï¼šlog_probs å·²ç»è€ƒè™‘äº† mask
        # éæ³•åŠ¨ä½œçš„ log_prob = -inf (æ¦‚ç‡=0)
        ratio = exp(log_probs - old_log_probs)
        policy_loss = -mean(min(ratio * advantage, ...))

        # 3. åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
```

---

## ğŸ†š ä¸ DQN çš„å¯¹æ¯”

| æ­¥éª¤ | DQN (æ—  mask) | MaskablePPO (æœ‰ mask) |
|------|---------------|----------------------|
| **ç¯å¢ƒ reset** | obs, info = env.reset() | obs, info = env.reset()<br>**+ info['action_masks']** |
| **æ¨ç†** | q = q_net(obs)<br>action = argmax(q)<br>âŒ å¯èƒ½é€‰éæ³•åŠ¨ä½œ | logits = policy_net(obs)<br>**logits[mask==0] = -inf**<br>action = argmax(logits)<br>âœ… 100%åˆæ³• |
| **ç»éªŒä¿å­˜** | buffer.add(obs, a, r, obs') | buffer.add(obs, a, r, obs', **masks**) |
| **è®­ç»ƒç›®æ ‡** | Q_target = r + Î³ max Q(obs', a')<br>âŒ max åŒ…æ‹¬éæ³•åŠ¨ä½œ | V_target = r + Î³ V(obs')<br>âœ… V åªè¯„ä¼°åˆæ³•åŠ¨ä½œ |
| **æŸå¤±è®¡ç®—** | loss = (Q - Q_target)Â²<br>âŒ å¯èƒ½ç”¨éæ³•åŠ¨ä½œçš„Q | loss = PPO_clip(log_prob, ...)<br>**âœ… log_prob åªæ¥è‡ªåˆæ³•åŠ¨ä½œ** |
| **éæ³•åŠ¨ä½œæƒ©ç½š** | reward = -10<br>âŒ æ±¡æŸ“Qå€¼ | ä¸éœ€è¦æƒ©ç½š<br>âœ… æ°¸è¿œä¸ä¼šé€‰éæ³•åŠ¨ä½œ |

---

## ğŸ¯ å…³é”®ä¼˜åŠ¿

### âœ… è®­ç»ƒæ—¶ Masking

```python
# 1. Policy Gradient åªæ¥è‡ªåˆæ³•åŠ¨ä½œ
# éæ³•åŠ¨ä½œçš„æ¦‚ç‡ = 0
# æ¢¯åº¦ä¸ä¼šæ¥è‡ªéæ³•åŠ¨ä½œ

# 2. Value Function æ­£ç¡®è¯„ä¼°
# V(s) åæ˜ "åªé€‰åˆæ³•åŠ¨ä½œ"çš„æœŸæœ›å›æŠ¥
# ä¸ä¼šè¢«éæ³•åŠ¨ä½œæ±¡æŸ“

# 3. ç†µè®¡ç®—æ­£ç¡®
# H = -Î£ p(a) log p(a)  åªå¯¹åˆæ³•åŠ¨ä½œæ±‚å’Œ
# é¼“åŠ±åœ¨åˆæ³•åŠ¨ä½œé—´æ¢ç´¢
```

### âœ… æ¨ç†æ—¶ Masking

```python
# 1. 100% ä¿è¯åˆæ³•
# éæ³•åŠ¨ä½œ logit = -inf
# softmax åæ¦‚ç‡ = 0
# æ°¸è¿œä¸ä¼šè¢«é€‰ä¸­

# 2. æ— éœ€é¢å¤–æ£€æŸ¥
# ä¸éœ€è¦ if-else åˆ¤æ–­
# ä»£ç ç®€æ´
```

---

## ğŸ“ å¸¸ç”¨å‘½ä»¤æ€»ç»“

```bash
# ==================== è®­ç»ƒ ====================
# æ¨èé…ç½®
python train/train_delta_selfplay_ppo.py \
    --total-timesteps 250000 \
    --max-pool-size 5 \
    --use-minmax \
    --output log/oracle_TicTacToe_ppo.zip

# ==================== æŸ¥çœ‹æºç  ====================
# ActionMaskerï¼ˆç¯å¢ƒåŒ…è£…ï¼‰
python -c "from sb3_contrib.common.wrappers import ActionMasker; import inspect; print(inspect.getsource(ActionMasker))" | less

# æ¨ç†æ—¶ masking
python -c "from sb3_contrib.ppo_mask.policies import MaskableActorCriticPolicy; import inspect; print(inspect.getsource(MaskableActorCriticPolicy.forward))" | less

# è®­ç»ƒæ—¶ masking
python -c "from sb3_contrib.ppo_mask.policies import MaskableActorCriticPolicy; import inspect; print(inspect.getsource(MaskableActorCriticPolicy.evaluate_actions))" | less

# ==================== å¿«é€Ÿæµ‹è¯• ====================
# æµ‹è¯• vs MinMax
python -c "
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import gymnasium as gym
import gym_env

def mask_fn(env):
    return (env.board == 0).astype(int)

model = MaskablePPO.load('log/oracle_TicTacToe_ppo.zip')
env = gym.make('TicTacToe-v0', opponent_type='minmax')
env = ActionMasker(env, mask_fn)

wins, draws, losses = 0, 0, 0
for _ in range(50):
    obs, _ = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            if reward > 0: wins += 1
            elif reward < 0: losses += 1
            else: draws += 1

print(f'èƒœ:{wins} å¹³:{draws} è´Ÿ:{losses}')
"
```

---

## ğŸ’¡ æ•…éšœæ’æŸ¥

### é—®é¢˜1: å¯¼å…¥é”™è¯¯

```bash
# é”™è¯¯: ModuleNotFoundError: No module named 'sb3_contrib'
# è§£å†³:
pip install sb3-contrib
```

### é—®é¢˜2: ActionMasker æ‰¾ä¸åˆ°

```bash
# é”™è¯¯: cannot import name 'ActionMasker'
# æ£€æŸ¥ç‰ˆæœ¬:
pip show sb3-contrib
# éœ€è¦ sb3-contrib >= 1.6.0

# å‡çº§:
pip install --upgrade sb3-contrib
```

### é—®é¢˜3: action_masks æœªä¼ é€’

```bash
# é”™è¯¯: è®­ç»ƒæ—¶éæ³•åŠ¨ä½œä»ç„¶å‡ºç°
# æ£€æŸ¥:
# 1. ç¯å¢ƒæ˜¯å¦ç”¨ ActionMasker åŒ…è£…ï¼Ÿ
env = ActionMasker(env, mask_fn)  # å¿…é¡»

# 2. mask_fn æ˜¯å¦æ­£ç¡®è¿”å›ï¼Ÿ
def mask_fn(env):
    return (env.board == 0).astype(np.int8)  # å¿…é¡»æ˜¯ int8 æˆ– bool
```

---

## ğŸ”— å‚è€ƒèµ„æº

- **sb3-contrib æ–‡æ¡£**: https://sb3-contrib.readthedocs.io/
- **MaskablePPO ç¤ºä¾‹**: https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html
- **æºç **: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib

---

## æ€»ç»“

MaskablePPO é€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°å®Œæ•´çš„ action maskingï¼š

1. **ActionMasker** åŒ…è£…å™¨ï¼šè‡ªåŠ¨æ·»åŠ  `action_masks` åˆ° `info`
2. **Policy.forward()**: æ¨ç†æ—¶ `logits[mask==0] = -inf`
3. **Policy.evaluate_actions()**: è®­ç»ƒæ—¶ `logits[mask==0] = -inf`
4. **PPO æŸå¤±**: åªæ¥è‡ªåˆæ³•åŠ¨ä½œçš„ `log_prob`

æ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼Œå¼€ç®±å³ç”¨ï¼ğŸ‰
