# 模型测试指南

完整的测试流程，评估 PPO 模型的策略质量。

## 测试脚本概览

| 脚本 | 用途 | 测试内容 |
|------|------|----------|
| `evaluate_ppo_strategy.py` | 综合策略评估 | 对抗测试、战术知识、开局质量、对称性、确定性、动作价值 |
| `test_first_second_player.py` | 先后手对比 | 先手/后手对战 Random/MinMax，自己vs自己 |
| `test_model.py` | 快速对战测试 | 简单的对战测试 |

## 完整测试流程

### 步骤 1: 综合策略评估

**运行:**
```bash
python train/evaluate_ppo_strategy.py \
    --model log/oracle_TicTacToe_ppo_balanced.zip \
    --num-games 100
```

**评估维度:**
1. ✅ **对抗测试** (100分)
   - vs Random: 期望 >95% 胜率
   - vs MinMax: 期望 >80% 平局率

2. ✅ **战术知识** (100分)
   - 立即获胜 (4个测试)
   - 阻止对手 (3个测试)
   - 双威胁、中心控制 (2个测试)
   - 期望准确率 >80%

3. ✅ **开局质量** (100分)
   - 首步选择分布
   - 期望: 中心 > 角落 > 边

4. ✅ **策略对称性** (100分)
   - 旋转对称测试
   - 可选，不影响实战

5. ✅ **确定性** (100分)
   - 同一局面多次预测一致性
   - 应该 100%

6. ✅ **动作价值分析**
   - 不同局面的概率分布
   - 定性分析

**总分:** 0-100，综合评级

**期望结果:**
- 总分 >75: 良好
- 总分 >90: 优秀

### 步骤 2: 先后手对比测试

**运行:**
```bash
python train/test_first_second_player.py \
    --model log/oracle_TicTacToe_ppo_balanced.zip \
    --num-games 100
```

**测试内容:**

| 测试 | 角色 | 对手 | 期望 |
|------|------|------|------|
| 1 | 先手(X) | Random | >95% 胜率 |
| 2 | 先手(X) | MinMax | >80% 平局率 |
| 3 | 后手(O) | Random | >80% 胜率 |
| 4 | 后手(O) | MinMax | >60% 平局率 |
| 5 | X vs O | 自己 | >70% 平局率 或 胜率差<10% |

**关键指标:**
- **先后手胜率差**: <20%
- **自己vs自己平局率**: >70%

**如果不均衡:**
- 检查训练时 `--play-as-o-prob` 是否为 0.5
- 后手视角翻转可能有问题

### 步骤 3: 快速对战测试 (可选)

**运行:**
```bash
python train/test_model.py \
    --model log/oracle_TicTacToe_ppo_balanced.zip \
    --opponent minmax \
    --num-games 100 \
    --render 5  # 渲染前5局
```

**用途:**
- 快速验证模型基本能力
- 可视化前几局游戏

## 测试结果解读

### 综合策略评估结果

**优秀模型 (90+分):**
```
1. 对抗测试: 100/100
   vs Random: 98.0% 胜率
   vs MinMax: 100.0% 平局率

2. 战术知识: 89/100
   正确率: 8/9

3. 开局质量: 100/100
   中心: 60%, 角落: 30%, 边: 10%

4. 策略对称性: 80/100
   (可选，不影响评分)

5. 确定性: 100/100

总体评分: 92.2/100
总体评级: 优秀 ✓✓
```

**当前模型 (78分):**
```
1. 对抗测试: 100/100  ✓
   vs Random: 96.0% 胜率
   vs MinMax: 100.0% 平局率

2. 战术知识: 78/100  ⚠️
   正确率: 7/9
   问题: 防守第一行(✗), 中心控制(✗)

3. 开局质量: 60/100  ⚠️
   中心: 23%, 角落: 54%, 边: 23%
   问题: 偏好角落而非中心

4. 策略对称性: 0/100  (不重要)

5. 确定性: 100/100  ✓

总体评分: 77.8/100
总体评级: 良好 ✓
```

**改进方向:**
1. ✅ 提升战术知识 78% → 90%
   - 增加探索 `--ent-coef 0.05`
   - 增加 Random 权重 `--random-weight 2.0`

2. ✅ 改善开局质量 60 → 80
   - 更多训练时间
   - 可能需要奖励塑形

3. ❌ 对称性 (非必需)

### 先后手对比结果

**理想结果:**
```
vs RANDOM:
  先手(X): 96% 胜率
  后手(O): 85% 胜率
  先手优势: +11% ✓ (先手天然优势)

vs MINMAX:
  先手(X): 95% 平局率
  后手(O): 70% 平局率
  先手优势: +25% ✓ (后手对MinMax更难)

自己 vs 自己:
  X胜率: 20%
  O胜率: 15%
  平局率: 65% ✓ (说明策略稳定)
  先后手差距: 5% ✓
```

**不均衡的结果:**
```
vs RANDOM:
  先手(X): 98% 胜率
  后手(O): 50% 胜率  ⚠️ 太低！
  先手优势: +48%

自己 vs 自己:
  X胜率: 45%
  O胜率: 10%  ⚠️ 严重不均衡！
  平局率: 45%
  先后手差距: 35%
```

**诊断:**
- 先后手差距 >30%: 训练数据严重不均衡
- 后手能力差: `--play-as-o-prob` 可能太小 (<0.3)
- 自己vs自己一边倒: 后手视角翻转可能有bug

**解决方案:**
```bash
# 重新训练，确保先后手均衡
python train/train_delta_selfplay_ppo.py \
    --play-as-o-prob 0.5 \
    --total-timesteps 300000 \
    --use-minmax \
    --ent-coef 0.05 \
    --random-weight 2.0
```

## 测试检查清单

### 训练后必做测试

- [ ] 运行综合策略评估
  - [ ] 对抗测试 >80分
  - [ ] 战术知识 >70分
  - [ ] 总分 >75分

- [ ] 运行先后手对比测试
  - [ ] 先后手胜率差 <20%
  - [ ] 自己vs自己平局率 >50%

### 可选测试

- [ ] 对称性测试 (如果追求完美)
- [ ] 可视化对战 (验证合理性)

## 常见问题排查

### Q1: 战术知识低 (<70%)

**原因:**
- 探索不足
- 过拟合 MinMax (只学会防守)
- Random 对手采样太少

**解决:**
```bash
--ent-coef 0.10        # 增加探索
--random-weight 5.0     # 大幅增加 Random 权重
--total-timesteps 500000  # 延长训练
```

### Q2: 先后手不均衡 (>20%)

**原因:**
- `--play-as-o-prob` 不是 0.5
- 后手视角翻转有 bug

**解决:**
```bash
--play-as-o-prob 0.5  # 确保先后手各50%
```

检查代码中的视角翻转逻辑。

### Q3: vs MinMax 胜率/平局率低

**原因:**
- 模型还不够强
- 训练时没用 MinMax

**解决:**
```bash
--use-minmax           # 启用 MinMax 对手
--total-timesteps 300000  # 足够训练时间
```

### Q4: 开局质量差 (偏好边而非中心)

**原因:**
- 训练时间不足
- 奖励信号问题

**解决:**
- 延长训练时间
- 考虑添加开局位置奖励

### Q5: 对称性为 0

**原因:**
- 网络架构不对称
- 没有数据增强

**是否修复:**
- 如果实战表现好，不用管
- 如果追求完美，参考 SYMMETRY_IMPROVEMENT.md

## 测试报告模板

```markdown
# 模型测试报告

**模型:** log/oracle_TicTacToe_ppo_balanced.zip
**训练参数:**
- Total timesteps: 300,000
- ent_coef: 0.05
- random_weight: 2.0
- play_as_o_prob: 0.5

## 综合策略评估

| 维度 | 得分 | 评级 | 备注 |
|------|------|------|------|
| 对抗测试 | 100/100 | ✓✓ | Random 98%, MinMax 100% |
| 战术知识 | 78/100 | ✓ | 7/9 正确 |
| 开局质量 | 60/100 | - | 偏好角落 |
| 确定性 | 100/100 | ✓✓ | 完全一致 |
| **总分** | **77.8/100** | **良好✓** | |

## 先后手对比

| 测试 | 先手 | 后手 | 差距 | 评级 |
|------|------|------|------|------|
| vs Random | 96% | 85% | 11% | ✓ |
| vs MinMax | 100%平 | 70%平 | - | ✓ |
| 自己vs自己 | 20%胜 | 15%胜, 65%平 | 5% | ✓✓ |

## 结论

模型表现良好，实战能力优秀：
- ✅ 对抗 Random: 优秀
- ✅ 对抗 MinMax: 优秀
- ✅ 先后手均衡
- ⚠️ 战术知识还有提升空间
- ⚠️ 开局质量需要改进

## 改进建议

1. 增加探索系数到 0.10
2. 增加 Random 权重到 3.0
3. 延长训练到 500k steps
```

## 总结

测试优先级：
1. **必做**: 综合策略评估
2. **必做**: 先后手对比测试
3. 可选: 对称性、可视化

关键指标：
- 总分 >75
- 战术知识 >70%
- 先后手差距 <20%

你的当前模型：
- 总分 77.8 ✓ 良好
- 战术知识 78% ✓ 良好
- 需要测试先后手均衡性
