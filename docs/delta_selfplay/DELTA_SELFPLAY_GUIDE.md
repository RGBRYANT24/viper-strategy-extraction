# Delta-Uniform Self-Play å®ç°æŒ‡å—

## æ¦‚è¿°

æœ¬å®ç°é’ˆå¯¹ TicTacToe æä¾›äº† **Î´-Uniform Self-Play** è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºè§£å†³æ™®é€šè‡ªæˆ‘å¯¹å¼ˆé™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ã€‚

### æ ¸å¿ƒæ€æƒ³

1. **å¯¹æ‰‹æ± æœºåˆ¶**: ç»´æŠ¤å›ºå®šå¤§å°çš„å†å²ç­–ç•¥æ±  (Kä¸ªå¿«ç…§)
2. **åŸºå‡†ç­–ç•¥**: åŠ å…¥ MinMax å’Œ Random ç­–ç•¥æé«˜é²æ£’æ€§
3. **å‡åŒ€é‡‡æ ·**: æ¯æ¬¡ reset() ä»æ± ä¸­å‡åŒ€é‡‡æ ·å¯¹æ‰‹
4. **å…ˆåæ‰‹è®­ç»ƒ**: é€šè¿‡æ£‹ç›˜ç¿»è½¬ä¿æŒç½‘ç»œè¾“å…¥ä¸€è‡´æ€§

### ä¸æ™®é€š Self-Play çš„åŒºåˆ«

| ç‰¹æ€§ | æ™®é€š Self-Play | Delta-Uniform Self-Play |
|------|----------------|------------------------|
| å¯¹æ‰‹æ¥æº | ä»…å½“å‰ç­–ç•¥ | å†å²æ±  + åŸºå‡†æ±  |
| å¯¹æ‰‹å¤šæ ·æ€§ | ä½ (å›ºå®š) | é«˜ (K+2 ç§) |
| é™·å…¥å±€éƒ¨æœ€ä¼˜ | æ˜“å‘ç”Ÿ | ä¸æ˜“å‘ç”Ÿ |
| å…ˆåæ‰‹è®­ç»ƒ | é€šå¸¸åªè®­ç»ƒå…ˆæ‰‹ | å…ˆåæ‰‹å„ 50% |

---

## æ–‡ä»¶ç»“æ„

```
viper-verifiable-rl-impl/
â”œâ”€â”€ gym_env/
â”‚   â”œâ”€â”€ policies/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ baseline_policies.py          # åŸºå‡†ç­–ç•¥ (MinMax, Random)
â”‚   â”œâ”€â”€ tictactoe_delta_selfplay.py       # Delta-Uniform Self-Play ç¯å¢ƒ
â”‚   â””â”€â”€ __init__.py                        # ç¯å¢ƒæ³¨å†Œ
â”œâ”€â”€ train/
â”‚   â””â”€â”€ train_delta_selfplay.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_delta_selfplay.py                # æµ‹è¯•å¥—ä»¶
â””â”€â”€ DELTA_SELFPLAY_GUIDE.md               # æœ¬æ–‡æ¡£
```

---

## å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•å®‰è£…

è¿è¡Œæµ‹è¯•å¥—ä»¶éªŒè¯æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œ:

```bash
python test_delta_selfplay.py
```

æœŸæœ›è¾“å‡º:
```
âœ“ åŸºå‡†ç­–ç•¥æµ‹è¯•é€šè¿‡
âœ“ Delta-Uniform Self-Play ç¯å¢ƒæµ‹è¯•é€šè¿‡
âœ“ ç¯å¢ƒæ³¨å†Œæµ‹è¯•é€šè¿‡
âœ“ è®­ç»ƒè„šæœ¬å¯¼å…¥æµ‹è¯•é€šè¿‡
ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚
```

### 2. å¼€å§‹è®­ç»ƒ

#### åŸºç¡€è®­ç»ƒ (ä»… Random åŸºå‡†)

```bash
python train/train_delta_selfplay.py \
    --total-timesteps 200000 \
    --max-pool-size 20 \
    --n-env 8
```

#### é«˜çº§è®­ç»ƒ (åŒ…å« MinMax åŸºå‡†)

```bash
python train/train_delta_selfplay.py \
    --total-timesteps 200000 \
    --max-pool-size 20 \
    --n-env 8 \
    --use-minmax
```

### 3. å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--total-timesteps` | 200000 | æ€»è®­ç»ƒæ­¥æ•° |
| `--n-env` | 8 | å¹¶è¡Œç¯å¢ƒæ•° |
| `--update-interval` | 10000 | ç­–ç•¥æ± æ›´æ–°é—´éš” |
| `--max-pool-size` | 20 | å†å²ç­–ç•¥æ± å®¹é‡ (K) |
| `--play-as-o-prob` | 0.5 | åæ‰‹è®­ç»ƒæ¦‚ç‡ |
| `--use-minmax` | False | æ˜¯å¦åŒ…å« MinMax åŸºå‡† |
| `--output` | log/... | æ¨¡å‹ä¿å­˜è·¯å¾„ |

---

## å®ç°ç»†èŠ‚

### 1. åŸºå‡†ç­–ç•¥ (baseline_policies.py)

å®ç°äº†ä¸¤ä¸ªåŸºå‡†ç­–ç•¥,åŒ…è£…ä¸º `stable-baselines3.BasePolicy` æ¥å£:

#### RandomPlayerPolicy
- ä»æ‰€æœ‰åˆæ³•åŠ¨ä½œä¸­éšæœºé€‰æ‹©
- ç”¨äºæä¾›æ¢ç´¢æ€§å¯¹æ‰‹

#### MinMaxPlayerPolicy
- ä½¿ç”¨ Alpha-Beta å‰ªæçš„ Minimax ç®—æ³•
- æä¾›æœ€ä¼˜ç­–ç•¥ä½œä¸ºå¼ºå¯¹æ‰‹
- æ·±åº¦é™åˆ¶: 9 (å®Œæ•´æœç´¢)

### 2. Delta-Uniform Self-Play ç¯å¢ƒ

#### å…³é”®ç‰¹æ€§

**å¯¹æ‰‹æ± ç®¡ç†**:
```python
baseline_pool = [RandomPolicy, MinMaxPolicy]  # å›ºå®šåŸºå‡†
learned_pool = deque(maxlen=K)  # å†å²å¿«ç…§ (FIFO)
```

**é‡‡æ ·æœºåˆ¶**:
```python
def _sample_opponent():
    all_opponents = baseline_pool + list(learned_pool)
    return random.choice(all_opponents)  # å‡åŒ€é‡‡æ ·
```

**å…ˆåæ‰‹è®­ç»ƒ**:
```python
# æ¯å±€éšæœºå†³å®šå…ˆåæ‰‹
play_as_o = (random.random() < play_as_o_prob)

# è§‚å¯Ÿæ€»æ˜¯ä»è‡ªå·±è§†è§’ (è‡ªå·±=1, å¯¹æ‰‹=-1)
if play_as_o:
    obs = -board  # ç¿»è½¬è§†è§’
else:
    obs = board
```

### 3. è®­ç»ƒæµç¨‹

```
åˆå§‹åŒ–:
â”œâ”€â”€ åˆ›å»ºåŸºå‡†æ± : [Random, MinMax (å¯é€‰)]
â”œâ”€â”€ åˆ›å»ºå­¦ä¹ æ± : deque(maxlen=K)
â””â”€â”€ åˆ›å»º DQN æ¨¡å‹

è®­ç»ƒå¾ªç¯ (æ¯ 10000 æ­¥):
â”œâ”€â”€ è®­ç»ƒ: model.learn(10000)
â”‚   â””â”€â”€ ç¯å¢ƒ reset() æ—¶è‡ªåŠ¨é‡‡æ ·å¯¹æ‰‹
â”œâ”€â”€ æ›´æ–°æ± : learned_pool.append(copy.deepcopy(model.policy))
â””â”€â”€ ç»Ÿè®¡: æ‰“å°æ± å¤§å°

è¯„ä¼°:
â””â”€â”€ å¯¹æˆ˜ MinMax 50å±€ (å…ˆæ‰‹)
```

### 4. è§†è§’ä¸€è‡´æ€§

ç¥ç»ç½‘ç»œå¿…é¡»çœ‹åˆ°ä¸€è‡´çš„è¾“å…¥è¡¨ç¤º:

| å®é™…æ£‹ç›˜ | æˆ‘æ–¹æ ‡è®° | å¯¹æ–¹æ ‡è®° | ç½‘ç»œè¾“å…¥ |
|----------|----------|----------|----------|
| å…ˆæ‰‹ (X) | 1 | -1 | board |
| åæ‰‹ (O) | -1 | 1 | -board |

é€šè¿‡ç¿»è½¬,ç½‘ç»œæ€»æ˜¯çœ‹åˆ° "è‡ªå·±=1, å¯¹æ‰‹=-1"ã€‚

---

## è®­ç»ƒå»ºè®®

### 1. æ± å¤§å°é€‰æ‹© (K)

| K å€¼ | å¯¹æ‰‹å¤šæ ·æ€§ | å†…å­˜å ç”¨ | æ¨èåœºæ™¯ |
|------|-----------|----------|----------|
| 10 | ä½ | ä½ | å¿«é€Ÿå®éªŒ |
| 20 | ä¸­ | ä¸­ | **æ¨è** |
| 50 | é«˜ | é«˜ | å¤æ‚æ¸¸æˆ |

### 2. åŸºå‡†ç­–ç•¥é€‰æ‹©

**ä»… Random**:
- ä¼˜ç‚¹: è®­ç»ƒå¿«,å†…å­˜å°‘
- ç¼ºç‚¹: å¯èƒ½å­¦ä¸åˆ°æœ€ä¼˜ç­–ç•¥
- é€‚ç”¨: åˆæ­¥å®éªŒ

**Random + MinMax**:
- ä¼˜ç‚¹: èƒ½å­¦åˆ°æ¥è¿‘æœ€ä¼˜ç­–ç•¥
- ç¼ºç‚¹: MinMax è®¡ç®—æ…¢
- é€‚ç”¨: **ç”Ÿäº§ç¯å¢ƒ**

### 3. å…ˆåæ‰‹æ¯”ä¾‹

| play_as_o_prob | è¯´æ˜ |
|----------------|------|
| 0.0 | ä»…è®­ç»ƒå…ˆæ‰‹ (ä¸æ¨è) |
| 0.5 | **å‡è¡¡è®­ç»ƒ (æ¨è)** |
| 1.0 | ä»…è®­ç»ƒåæ‰‹ (ä¸æ¨è) |

### 4. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡**:
```python
learning_rate=1e-3  # é»˜è®¤å€¼
# å¦‚æœæ”¶æ•›æ…¢: 1e-2
# å¦‚æœä¸ç¨³å®š: 1e-4
```

**æ¢ç´¢ç­–ç•¥**:
```python
exploration_fraction=0.5  # å‰ 50% æ—¶é—´æ¢ç´¢
exploration_final_eps=0.05  # æœ€ç»ˆ 5% æ¢ç´¢ç‡
```

---

## è¯„ä¼°æŒ‡æ ‡

è®­ç»ƒåä¼šè‡ªåŠ¨è¯„ä¼° 50 å±€ vs MinMax (å…ˆæ‰‹):

### ä¼˜ç§€ç­–ç•¥ (ç›®æ ‡)
```
èƒœ: 0-2 (0-4%)
è´Ÿ: 0-2 (0-4%)
å¹³: 46-50 (92-100%)  â† å…³é”®æŒ‡æ ‡
éæ³•ç§»åŠ¨: 0
```

### è‰¯å¥½ç­–ç•¥
```
å¹³: 30-40 (60-80%)
éæ³•ç§»åŠ¨: 0
```

### éœ€è¦æ”¹è¿›
```
å¹³: < 30 (< 60%)
æˆ– éæ³•ç§»åŠ¨ > 0
```

---

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒåä»ç„¶é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Ÿ

**å¯èƒ½åŸå› **:
1. æ± å¤§å° K å¤ªå° â†’ å¢åŠ åˆ° 30-50
2. æœªä½¿ç”¨ MinMax åŸºå‡† â†’ æ·»åŠ  `--use-minmax`
3. è®­ç»ƒæ­¥æ•°ä¸è¶³ â†’ å¢åŠ åˆ° 300k-500k

### Q2: å¯¹æ‰‹æ± å¯¼è‡´å†…å­˜ä¸è¶³ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å° `--max-pool-size`
2. å¢å¤§ `--update-interval` (å‡å°‘å¿«ç…§é¢‘ç‡)
3. ä½¿ç”¨æ›´å°çš„ç½‘ç»œæ¶æ„

### Q3: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**ä¼˜åŒ–å»ºè®®**:
1. ä¸ä½¿ç”¨ MinMax åŸºå‡† (å»æ‰ `--use-minmax`)
2. å‡å°å¹¶è¡Œç¯å¢ƒæ•° `--n-env`
3. å‡å°æ± å¤§å° `--max-pool-size`

### Q4: å¦‚ä½•å¯¹æˆ˜è‡ªå·±è®­ç»ƒçš„æ¨¡å‹ï¼Ÿ

```python
from stable_baselines3 import DQN
import gymnasium as gym

# åŠ è½½æ¨¡å‹
model = DQN.load("log/oracle_TicTacToe_delta_selfplay.zip")

# åˆ›å»ºç¯å¢ƒ
env = gym.make('TicTacToe-v0', opponent_type='random')

# å¯¹æˆ˜
obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    env.render()
    done = terminated or truncated

print(f"Result: {info}")
```

---

## æ‰©å±•åˆ°å…¶ä»–æ¸¸æˆ

### ä¿®æ”¹æ£€æŸ¥æ¸…å•

1. **ç¯å¢ƒé€‚é…**:
   - [ ] ä¿®æ”¹è§‚å¯Ÿç©ºé—´ç»´åº¦
   - [ ] ä¿®æ”¹åŠ¨ä½œç©ºé—´å¤§å°
   - [ ] å®ç°è§†è§’ç¿»è½¬é€»è¾‘

2. **åŸºå‡†ç­–ç•¥**:
   - [ ] å®ç°éšæœºç­–ç•¥
   - [ ] å®ç°è§„åˆ™ç­–ç•¥ (å¦‚ MinMax)

3. **è®­ç»ƒå‚æ•°**:
   - [ ] è°ƒæ•´ç½‘ç»œæ¶æ„
   - [ ] è°ƒæ•´å­¦ä¹ ç‡
   - [ ] è°ƒæ•´æ± å¤§å°

---

## å‚è€ƒæ–‡çŒ®

Delta-Uniform Self-Play åŸºäºä»¥ä¸‹ç ”ç©¶:

1. **PSRO (Policy-Space Response Oracles)**
   - Lanctot et al., 2017
   - ç»´æŠ¤ç­–ç•¥ç§ç¾¤,è®¡ç®—è¿‘ä¼¼çº³ä»€å‡è¡¡

2. **Self-Play æ”¹è¿›æ–¹æ³•**
   - Silver et al., AlphaGo/AlphaZero
   - è‡ªæˆ‘å¯¹å¼ˆ + MCTS

3. **Curriculum Learning in Multi-Agent RL**
   - Uniform sampling over historical opponents
   - é¿å…é—å¿˜æ—©æœŸç­–ç•¥

---

## è´¡çŒ®è€…

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®,è¯·æäº¤ Issue æˆ– PRã€‚

ç¥è®­ç»ƒé¡ºåˆ©! ğŸ‰
