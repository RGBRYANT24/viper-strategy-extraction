# VIPER MaskablePPO å®ç°æŠ€æœ¯åˆ†ææ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ `viper_maskable_ppo.py` çš„å®ç°æ€è·¯ï¼Œä¸ VIPER åŸè®ºæ–‡çš„å¯¹æ¯”ï¼Œä»¥åŠæŠ€æœ¯ç»†èŠ‚ã€‚

---

## ğŸ“š ç›®å½•

1. [VIPER åŸè®ºæ–‡ç®—æ³•å›é¡¾](#1-viper-åŸè®ºæ–‡ç®—æ³•å›é¡¾)
2. [æœ¬å®ç°ä¸åŸè®ºæ–‡çš„å¯¹æ¯”](#2-æœ¬å®ç°ä¸åŸè®ºæ–‡çš„å¯¹æ¯”)
3. [è®­ç»ƒå¯¹æ‰‹é…ç½®è¯´æ˜](#3-è®­ç»ƒå¯¹æ‰‹é…ç½®è¯´æ˜)
4. [æ ¸å¿ƒä»£ç é€æ®µè§£æ](#4-æ ¸å¿ƒä»£ç é€æ®µè§£æ)
5. [å…³é”®æŠ€æœ¯ç»†èŠ‚](#5-å…³é”®æŠ€æœ¯ç»†èŠ‚)
6. [ä¸é¡¹ç›®ä¸­ DQN-VIPER çš„å¯¹æ¯”](#6-ä¸é¡¹ç›®ä¸­-dqn-viper-çš„å¯¹æ¯”)
7. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#7-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## 1. VIPER åŸè®ºæ–‡ç®—æ³•å›é¡¾

### 1.1 è®ºæ–‡åŸºæœ¬ä¿¡æ¯

**è®ºæ–‡**: Verifiable Reinforcement Learning via Policy Extraction
**ä½œè€…**: Osbert Bastani et al. (2018)
**é“¾æ¥**: https://arxiv.org/abs/1805.08328

### 1.2 æ ¸å¿ƒç®—æ³•ï¼ˆä¼ªä»£ç ï¼‰

```python
Algorithm: VIPER (Verifiable Policy Extraction via Reinforcement learning)

Input:
  - Ï€_oracle: Pre-trained DQN/PPO oracle policy
  - M: MDP environment
  - N: Number of iterations
  - n: Number of samples per iteration

Output:
  - Ï€_tree: Interpretable decision tree policy

Procedure:
  D â† âˆ…  # Training dataset
  Ï€_current â† Ï€_oracle  # Current policy (initially oracle)

  for i = 1 to N do:
    # Step 1: Sample trajectories using mixture of oracle and current tree
    Î² â† 1 if i == 1 else 0  # DAgger aggregation parameter
    Ï„ â† Sample_Trajectories(Ï€_current, Ï€_oracle, Î², n)

    # Step 2: Compute criticality weights for each state
    for (s, a, r) in Ï„ do:
      l(s) â† Criticality(Ï€_oracle, s)  # Q_max - Q_min or log_prob_max - log_prob_min
      D â† D âˆª {(s, a, l(s))}

    # Step 3: Train decision tree with weighted samples
    Ï€_tree_i â† Train_DecisionTree(D, sample_weight=l(s))

    # Step 4: Evaluate and track best tree
    R_i â† Evaluate(Ï€_tree_i, M)
    Ï€_current â† Ï€_tree_i

  return Best(Ï€_tree_1, ..., Ï€_tree_N)
```

### 1.3 æ ¸å¿ƒæ€æƒ³

1. **Oracle Teacher**: ä½¿ç”¨é«˜æ€§èƒ½ç¥ç»ç½‘ç»œï¼ˆDQN/PPOï¼‰ä½œä¸ºæ•™å¸ˆ
2. **Imitation Learning**: å†³ç­–æ ‘æ¨¡ä»¿ Oracle çš„è¡Œä¸º
3. **Criticality Weighting**: é‡è¦çŠ¶æ€ï¼ˆå†³ç­–å½±å“å¤§ï¼‰ç»™äºˆæ›´é«˜æƒé‡
4. **Iterative Refinement**: å¤šè½®è¿­ä»£ï¼Œé€‰æ‹©æœ€ä½³æ ‘
5. **DAgger-style Aggregation**: æ··åˆä½¿ç”¨ Oracle å’Œå½“å‰ç­–ç•¥é‡‡æ ·

---

## 2. æœ¬å®ç°ä¸åŸè®ºæ–‡çš„å¯¹æ¯”

### 2.1 æ ¸å¿ƒä¸€è‡´æ€§ âœ…

| ç»´åº¦ | åŸè®ºæ–‡ | æœ¬å®ç° | ä¸€è‡´æ€§ |
|------|--------|--------|--------|
| **ç®—æ³•æ¡†æ¶** | DAgger + Weighted Imitation | DAgger + Weighted Imitation | âœ… å®Œå…¨ä¸€è‡´ |
| **Criticality Loss** | Q_max - Q_min (DQN)<br>log_prob_max - log_prob_min (PPO) | åŒå·¦ | âœ… å®Œå…¨ä¸€è‡´ |
| **Beta è°ƒåº¦** | Î²=1 (iter 0), Î²=0 (iter 1+) | åŒå·¦ | âœ… å®Œå…¨ä¸€è‡´ |
| **æ ‘æ¨¡å‹** | DecisionTreeClassifier | åŒå·¦ | âœ… å®Œå…¨ä¸€è‡´ |
| **æ ·æœ¬æƒé‡** | sample_weight=criticality | åŒå·¦ | âœ… å®Œå…¨ä¸€è‡´ |
| **è¿­ä»£é€‰æ‹©** | é€‰æ‹© N æ£µæ ‘ä¸­æœ€ä½³ | åŒå·¦ | âœ… å®Œå…¨ä¸€è‡´ |

### 2.2 å®ç°å¢å¼º ğŸ”§

| å¢å¼ºç‚¹ | åŸè®ºæ–‡ | æœ¬å®ç° | è¯´æ˜ |
|--------|--------|--------|------|
| **Oracle ç±»å‹** | DQN, ä¼ ç»Ÿ PPO | **MaskablePPO** | âœ… æ”¯æŒ action masking |
| **éæ³•åŠ¨ä½œ** | æœªæ˜ç¡®å¤„ç† | **Mask å¤„ç†** | âœ… 100% é¿å…éæ³•åŠ¨ä½œ |
| **è¾“å‡ºæ–¹å¼** | å•ä¸ªåŠ¨ä½œ | **æ¦‚ç‡åˆ†å¸ƒ + Masking** | âœ… æ›´ç¬¦åˆ TicTacToe |
| **ç¯å¢ƒå…¼å®¹æ€§** | Gym | **Gymnasium + ActionMasker** | âœ… é€‚é…æ–°ç‰ˆ API |

### 2.3 å…³é”®å·®å¼‚åˆ†æ

#### å·®å¼‚ 1: Criticality Loss è®¡ç®—ä¸­è€ƒè™‘ Action Masking

**åŸè®ºæ–‡**:
```python
# DQN
Q_values = oracle.q_net(obs)
criticality = Q_values.max() - Q_values.min()  # æ‰€æœ‰åŠ¨ä½œ

# PPO
log_probs = [oracle.policy.log_prob(obs, a) for a in all_actions]
criticality = log_probs.max() - log_probs.min()  # æ‰€æœ‰åŠ¨ä½œ
```

**æœ¬å®ç°**:
```python
# MaskablePPO (ä»…è€ƒè™‘åˆæ³•åŠ¨ä½œ)
log_probs = oracle.policy.get_distribution(obs).logits
legal_actions = where(obs == 0)  # è·å–åˆæ³•åŠ¨ä½œ
legal_log_probs = log_probs[legal_actions]
criticality = legal_log_probs.max() - legal_log_probs.min()  # â­ ä»…åˆæ³•åŠ¨ä½œ
```

**ä¸ºä»€ä¹ˆé‡è¦**:
- TicTacToe éæ³•åŠ¨ä½œçš„ Q å€¼/log_prob æ— æ„ä¹‰
- ä»…è€ƒè™‘åˆæ³•åŠ¨ä½œæ›´å‡†ç¡®åæ˜ çŠ¶æ€é‡è¦æ€§

#### å·®å¼‚ 2: å†³ç­–æ ‘è¾“å‡ºæ–¹å¼

**åŸè®ºæ–‡**:
```python
# ç›´æ¥è¾“å‡ºå•ä¸ªåŠ¨ä½œ
action = tree.predict(obs)
```

**æœ¬å®ç°**:
```python
# è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶ååº”ç”¨ masking
probs = tree.predict_proba(obs)  # [p0, p1, ..., p8]
legal_actions = where(obs == 0)
masked_probs = probs.copy()
masked_probs[illegal_actions] = -inf
action = argmax(masked_probs)  # â­ ä¿è¯åˆæ³•
```

**ä¸ºä»€ä¹ˆé‡è¦**:
- ä¿è¯æ¨ç†æ—¶ 100% é¿å…éæ³•åŠ¨ä½œ
- ä¸éœ€è¦åœ¨è®­ç»ƒæ—¶ç‰¹æ®Šå¤„ç†éæ³•åŠ¨ä½œæ ‡ç­¾

### 2.4 ç®—æ³•æ ¸å¿ƒæ€æƒ³ä¸€è‡´æ€§ âœ…

| æ ¸å¿ƒæ€æƒ³ | æ˜¯å¦ä¿æŒ |
|----------|---------|
| ä½¿ç”¨é«˜æ€§èƒ½ Oracle ä½œä¸ºæ•™å¸ˆ | âœ… æ˜¯ (MaskablePPO) |
| åŸºäº Criticality åŠ æƒæ ·æœ¬ | âœ… æ˜¯ (è€ƒè™‘ masking) |
| DAgger-style æ•°æ®èšåˆ | âœ… æ˜¯ (Î²=1 then 0) |
| è¿­ä»£è®­ç»ƒå¤šæ£µæ ‘å¹¶é€‰æœ€ä½³ | âœ… æ˜¯ |
| æå–å¯è§£é‡Šå†³ç­–æ ‘ | âœ… æ˜¯ (å•æ£µåˆ†ç±»æ ‘) |

**ç»“è®º**: æœ¬å®ç°åœ¨ä¿æŒ VIPER æ ¸å¿ƒæ€æƒ³çš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹ TicTacToe å’Œ MaskablePPO åšäº†å¿…è¦çš„é€‚é…ï¼Œ**æ²¡æœ‰åç¦»åŸè®ºæ–‡çš„æ ¸å¿ƒç®—æ³•**ã€‚

---

## 3. è®­ç»ƒå¯¹æ‰‹é…ç½®è¯´æ˜

### 3.1 å½“å‰å®ç°çš„å¯¹æ‰‹è®¾ç½®

**ä»£ç ä½ç½®**: `load_maskable_ppo_oracle()` å‡½æ•°

```python
def load_maskable_ppo_oracle(oracle_path, env_name, opponent_type='minmax'):
    """
    Args:
        opponent_type: å¯¹æ‰‹ç±»å‹ ('random', 'minmax')
    """
    env = gym.make(env_name, opponent_type=opponent_type)
    # ...
```

**é»˜è®¤é…ç½®**: `opponent_type='minmax'`

**é—®é¢˜**: ç¡®å®ï¼ŒVIPER è®­ç»ƒæ—¶**åªä½¿ç”¨å•ä¸€å¯¹æ‰‹**ï¼ˆé»˜è®¤ MinMaxï¼‰ã€‚

### 3.2 ä¸ºä»€ä¹ˆåªç”¨ä¸€ä¸ªå¯¹æ‰‹ï¼Ÿ

#### åŸå›  1: Oracle è®­ç»ƒ vs VIPER è®­ç»ƒçš„åŒºåˆ«

| é˜¶æ®µ | è®­ç»ƒæ–¹å¼ | å¯¹æ‰‹é…ç½® | ç›®çš„ |
|------|----------|----------|------|
| **Oracle è®­ç»ƒ** (PPO) | Self-play + å¤šå¯¹æ‰‹æ±  | Random + MinMax + å†å²ç­–ç•¥ | å­¦ä¹ é²æ£’ç­–ç•¥ |
| **VIPER è®­ç»ƒ** (Tree) | æ¨¡ä»¿ Oracle | **å•ä¸€å›ºå®šå¯¹æ‰‹** | æå–å·²å­¦çŸ¥è¯† |

**å…³é”®ç‚¹**: VIPER ä¸æ˜¯ä»é›¶å­¦ä¹ ï¼Œè€Œæ˜¯**æå– Oracle å·²ç»å­¦åˆ°çš„çŸ¥è¯†**ã€‚Oracle å·²ç»åœ¨å¤šæ ·åŒ–å¯¹æ‰‹ä¸Šè®­ç»ƒè¿‡ï¼Œæ‰€ä»¥ VIPER åªéœ€è¦åœ¨ä¸€ä¸ªå›ºå®šå¯¹æ‰‹ä¸Šæå–è§„åˆ™ã€‚

#### åŸå›  2: VIPER è®ºæ–‡çš„è®¾è®¡å“²å­¦

VIPER çš„æ ¸å¿ƒæ˜¯**æ¨¡ä»¿å­¦ä¹  (Imitation Learning)**ï¼Œè€Œä¸æ˜¯å¼ºåŒ–å­¦ä¹ ï¼š
- Oracle å·²ç»çŸ¥é“å¦‚ä½•åº”å¯¹å„ç§å¯¹æ‰‹
- VIPER åªéœ€è¦å­¦ä¹ "Oracle ä¼šæ€ä¹ˆåš"
- å¯¹æ‰‹åªæ˜¯æä¾›çŠ¶æ€åˆ†å¸ƒçš„é‡‡æ ·ç¯å¢ƒ

#### åŸå›  3: è®¡ç®—æ•ˆç‡

ä½¿ç”¨å•ä¸€å¯¹æ‰‹ï¼š
- âœ… æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦
- âœ… æ›´ç¨³å®šçš„çŠ¶æ€åˆ†å¸ƒ
- âœ… æ›´å®¹æ˜“è¯„ä¼°æ”¶æ•›æ€§

### 3.3 åº”è¯¥æ”¹æˆå¤šå¯¹æ‰‹å—ï¼Ÿ

**ç†è®ºä¸Šå¯ä»¥ï¼Œä½†ä¸æ˜¯å¿…éœ€**ã€‚

#### æ–¹æ¡ˆ A: ä¿æŒå•å¯¹æ‰‹ï¼ˆæ¨èï¼‰â­

```python
# ä½¿ç”¨ MinMaxï¼ˆæœ€å¼ºå¯¹æ‰‹ï¼‰
python train/viper_maskable_ppo.py \
    --oracle-path log/oracle_TicTacToe_ppo_aggressive.zip \
    --opponent-type minmax \
    --total-timesteps 50000
```

**ä¼˜åŠ¿**:
- âœ… ç®€å•ç¨³å®š
- âœ… é‡‡æ ·é«˜è´¨é‡çŠ¶æ€
- âœ… ç¬¦åˆ VIPER åŸè®ºæ–‡è®¾è®¡

#### æ–¹æ¡ˆ B: å¤šå¯¹æ‰‹é‡‡æ ·ï¼ˆå¯é€‰å¢å¼ºï¼‰

å¦‚æœæƒ³è¦æ›´é²æ£’çš„æ ‘ï¼Œå¯ä»¥ä¿®æ”¹ä»£ç æ”¯æŒå¤šå¯¹æ‰‹ï¼š

```python
# ä¼ªä»£ç ï¼ˆéœ€è¦ä¿®æ”¹å®ç°ï¼‰
opponents = ['random', 'minmax']
for iteration in range(n_iter):
    opponent = random.choice(opponents)  # æ¯è½®éšæœºå¯¹æ‰‹
    env = gym.make(env_name, opponent_type=opponent)
    trajectory = sample_trajectory(...)
```

**ä½†è¿™ä¸æ˜¯ VIPER åŸè®ºæ–‡çš„åšæ³•**ï¼Œè€Œæ˜¯é¢å¤–å¢å¼ºã€‚

### 3.4 æ¨èé…ç½®

| åœºæ™¯ | æ¨èå¯¹æ‰‹ | åŸå›  |
|------|----------|------|
| **æ ‡å‡†è®­ç»ƒ** | `minmax` | æœ€å¼ºå¯¹æ‰‹ï¼Œæå–æœ€ä¼˜ç­–ç•¥ |
| **å¿«é€Ÿæµ‹è¯•** | `random` | æ›´ç®€å•ï¼Œé‡‡æ ·æ›´å¿« |
| **é«˜é²æ£’æ€§** | ä¿®æ”¹ä»£ç æ”¯æŒå¤šå¯¹æ‰‹ | å®éªŒæ€§ï¼Œéå¿…éœ€ |

**ç»“è®º**: å½“å‰å®ç°ä½¿ç”¨å•ä¸€å¯¹æ‰‹æ˜¯**ç¬¦åˆ VIPER è®ºæ–‡è®¾è®¡**çš„ï¼Œä¸æ˜¯ bug æˆ–é—æ¼ã€‚

---

## 4. æ ¸å¿ƒä»£ç é€æ®µè§£æ

### 4.1 ç®—æ³•æµç¨‹æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VIPER è®­ç»ƒæµç¨‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. åŠ è½½ Oracle (MaskablePPO)
   â†“
2. åˆå§‹åŒ–ç©ºæ•°æ®é›† D = []
   â†“
3. FOR iteration = 1 to N:
   â”‚
   â”œâ”€â†’ 3.1 ç¡®å®šé‡‡æ ·ç­–ç•¥
   â”‚        Î² = 1 (iter 0) or 0 (iter 1+)
   â”‚
   â”œâ”€â†’ 3.2 é‡‡æ ·è½¨è¿¹
   â”‚        - ä½¿ç”¨ Î²-weighted æ··åˆç­–ç•¥
   â”‚        - è·å– (state, oracle_action, criticality)
   â”‚
   â”œâ”€â†’ 3.3 èšåˆæ•°æ®
   â”‚        D = D âˆª new_trajectory
   â”‚
   â”œâ”€â†’ 3.4 è®­ç»ƒå†³ç­–æ ‘
   â”‚        tree = DecisionTreeClassifier()
   â”‚        tree.fit(X, y, sample_weight=criticality)
   â”‚
   â”œâ”€â†’ 3.5 è¯„ä¼°æ ‘æ€§èƒ½
   â”‚        reward = evaluate(tree)
   â”‚
   â””â”€â†’ 3.6 æ›´æ–°å½“å‰ç­–ç•¥
            policy = tree
   â”‚
4. è¿”å›æœ€ä½³æ ‘ (max reward)
```

### 4.2 å…³é”®å‡½æ•°è¯¦è§£

#### å‡½æ•° 1: `mask_fn(env)` - è·å–åŠ¨ä½œæ©ç 

```python
def mask_fn(env):
    """
    è¿”å› action mask for TicTacToe

    å¤„ç†å¤šå±‚åŒ…è£…å™¨: ActionMasker -> Monitor -> TicTacToeEnv
    """
    current_env = env
    max_depth = 10
    depth = 0

    while depth < max_depth:
        if hasattr(current_env, 'board'):
            # æ‰¾åˆ° TicTacToe ç¯å¢ƒ
            board = current_env.board
            mask = (board == 0).astype(np.int8)
            return mask
        elif hasattr(current_env, 'env'):
            # ç»§ç»­è§£åŒ…
            current_env = current_env.env
            depth += 1
        else:
            break

    raise AttributeError("Cannot find 'board' attribute")
```

**ä½œç”¨**:
- ä»å¤šå±‚åŒ…è£…çš„ç¯å¢ƒä¸­æå–æ£‹ç›˜çŠ¶æ€
- è¿”å›åˆæ³•åŠ¨ä½œæ©ç  (1=åˆæ³•, 0=éæ³•)

**ä¸ºä»€ä¹ˆéœ€è¦é€’å½’è§£åŒ…**:
```
ActionMasker(
  Monitor(
    TicTacToeEnv()  â† çœŸæ­£çš„ç¯å¢ƒï¼ŒåŒ…å« board å±æ€§
  )
)
```

#### å‡½æ•° 2: `ProbabilityMaskedTreeWrapper` - å†³ç­–æ ‘åŒ…è£…å™¨

```python
class ProbabilityMaskedTreeWrapper:
    def __init__(self, tree_model):
        self.tree = tree_model  # sklearn DecisionTreeClassifier
        self.n_actions = 9

    def predict(self, observation, ...):
        """
        æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼šæ¦‚ç‡åˆ†å¸ƒ + Masking
        """
        # 1. è·å–æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
        action_probs = self.tree.predict_proba(observation)  # shape: (batch, 9)

        # 2. å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨ masking
        for i in range(observation.shape[0]):
            obs = observation[i]
            probs = action_probs[i]

            # 3. è·å–åˆæ³•åŠ¨ä½œï¼ˆæ£‹ç›˜ä¸Šçš„ç©ºä½ï¼‰
            legal_actions = np.where(obs == 0)[0]

            # 4. åˆ›å»ºæ©ç æ¦‚ç‡ï¼ˆéæ³•åŠ¨ä½œ = -infï¼‰
            masked_probs = np.full(9, -np.inf)
            masked_probs[legal_actions] = probs[legal_actions]

            # 5. é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åˆæ³•åŠ¨ä½œ
            action = np.argmax(masked_probs)
            actions.append(action)

        return actions
```

**å…³é”®ä¼˜åŠ¿**:
- âœ… ä¿è¯ 100% åˆæ³•åŠ¨ä½œ
- âœ… ä¸éœ€è¦è®­ç»ƒæ—¶å¤„ç†éæ³•åŠ¨ä½œæ ‡ç­¾
- âœ… ä¿æŒå†³ç­–æ ‘çš„å¯è§£é‡Šæ€§

**ä¸åŸè®ºæ–‡å¯¹æ¯”**:
- åŸè®ºæ–‡: ç›´æ¥ `tree.predict(obs)`
- æœ¬å®ç°: `tree.predict_proba(obs)` + masking
- **åŸå› **: TicTacToe éœ€è¦å¼ºåˆ¶é¿å…éæ³•åŠ¨ä½œ

#### å‡½æ•° 3: `get_criticality_loss_maskable_ppo()` - è®¡ç®—é‡è¦æ€§æƒé‡

```python
def get_criticality_loss_maskable_ppo(oracle, observations):
    """
    è®¡ç®—çŠ¶æ€çš„ criticalityï¼ˆé‡è¦æ€§ï¼‰

    å…¬å¼: criticality(s) = max_aâˆˆLegal Q(s,a) - min_aâˆˆLegal Q(s,a)

    å…¶ä¸­ Q(s,a) â‰ˆ log Ï€(a|s) (max entropy formulation)
    """
    with torch.no_grad():
        # 1. è·å– log probabilities (è¿‘ä¼¼ Q å€¼)
        obs_tensor = torch.as_tensor(observations).to(oracle.device)
        distribution = oracle.policy.get_distribution(obs_tensor)
        log_probs = distribution.distribution.logits.cpu().numpy()  # (batch, 9)

        # 2. å¯¹æ¯ä¸ªçŠ¶æ€è®¡ç®— masked criticality
        losses = []
        for i in range(observations.shape[0]):
            obs = observations[i]
            log_prob = log_probs[i]

            # 3. åªè€ƒè™‘åˆæ³•åŠ¨ä½œ
            legal_actions = np.where(obs == 0)[0]

            if len(legal_actions) == 0:
                losses.append(0.0)  # æ— åˆæ³•åŠ¨ä½œï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
            else:
                # 4. è®¡ç®—åˆæ³•åŠ¨ä½œçš„ Q å€¼èŒƒå›´
                legal_log_probs = log_prob[legal_actions]
                criticality = legal_log_probs.max() - legal_log_probs.min()
                losses.append(criticality)

        return np.array(losses)
```

**æ ¸å¿ƒæ€æƒ³**:
1. **Q â‰ˆ log Ï€**: PPO çš„ç­–ç•¥æ¦‚ç‡è¿‘ä¼¼ Q å€¼ï¼ˆmax entropy RLï¼‰
2. **Criticality = Range**: æœ€ä½³åŠ¨ä½œå’Œæœ€å·®åŠ¨ä½œçš„å·®è·
3. **é«˜ criticality** â†’ å†³ç­–å¾ˆé‡è¦ï¼ˆæƒé‡é«˜ï¼‰
4. **ä½ criticality** â†’ éšä¾¿é€‰éƒ½è¡Œï¼ˆæƒé‡ä½ï¼‰

**ç¤ºä¾‹**:
```python
# çŠ¶æ€ 1: å³å°†è·èƒœ
legal_actions = [2, 4]  # ä¸¤ä¸ªç©ºä½
log_probs = {2: -0.1, 4: -3.0}  # ä½ç½®2å‡ ä¹ç¡®å®šèµ¢
criticality = -0.1 - (-3.0) = 2.9  # é«˜æƒé‡ â­

# çŠ¶æ€ 2: å¼€å±€ç¬¬ä¸€æ­¥
legal_actions = [0,1,2,3,4,5,6,7,8]
log_probs = [-1.2, -1.3, -1.1, ...]  # å·®ä¸å¤š
criticality = -1.1 - (-1.3) = 0.2  # ä½æƒé‡
```

#### å‡½æ•° 4: `sample_trajectory_maskable_ppo()` - é‡‡æ ·è½¨è¿¹

```python
def sample_trajectory_maskable_ppo(oracle, env, policy, beta, n_steps, verbose=0):
    """
    ä½¿ç”¨ beta-weighted æ··åˆç­–ç•¥é‡‡æ ·è½¨è¿¹

    Args:
        oracle: MaskablePPO æ•™å¸ˆæ¨¡å‹
        env: ç¯å¢ƒï¼ˆå¸¦ ActionMaskerï¼‰
        policy: å½“å‰å†³ç­–æ ‘ï¼ˆæˆ– Noneï¼‰
        beta: Oracle é‡‡æ ·æ¦‚ç‡ (1=å…¨oracle, 0=å…¨tree)
        n_steps: é‡‡æ ·æ­¥æ•°

    Returns:
        trajectory: [(obs, oracle_action, criticality_weight), ...]
    """
    trajectory = []
    obs, _ = env.reset()

    while len(trajectory) < n_steps:
        # 1. é€‰æ‹©ç­–ç•¥ï¼šä»¥ beta æ¦‚ç‡ä½¿ç”¨ oracle
        use_oracle = (policy is None) or (np.random.random() < beta)

        # 2. è·å– action mask
        action_mask = mask_fn(env)

        # 3. æ‰§è¡ŒåŠ¨ä½œ
        if use_oracle:
            action, _ = oracle.predict(obs, action_masks=action_mask)
        else:
            action, _ = policy.predict(obs)  # tree å†…ç½® masking

        # 4. è·å– oracle çš„æ ‡ç­¾ï¼ˆç”¨äºè®­ç»ƒï¼‰
        oracle_action, _ = oracle.predict(obs, action_masks=action_mask)

        # 5. ç¯å¢ƒäº¤äº’
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # 6. è®¡ç®— criticality æƒé‡
        obs_batch = obs.reshape(1, -1)
        state_loss = get_criticality_loss_maskable_ppo(oracle, obs_batch)[0]

        # 7. æ·»åŠ æ ·æœ¬ (çŠ¶æ€, oracleåŠ¨ä½œ, æƒé‡)
        trajectory.append((obs.copy(), oracle_action, state_loss))

        obs = next_obs
        if done:
            obs, _ = env.reset()

    return trajectory
```

**Beta è°ƒåº¦è¯´æ˜**:
```python
# Iteration 0: beta = 1.0
#   â†’ 100% ä½¿ç”¨ oracle é‡‡æ ·
#   â†’ ç›®çš„: æ”¶é›†é«˜è´¨é‡åˆå§‹æ•°æ®

# Iteration 1+: beta = 0.0
#   â†’ 100% ä½¿ç”¨å½“å‰ tree é‡‡æ ·
#   â†’ ç›®çš„: DAgger-style æ•°æ®èšåˆ
#   â†’ ä¿®æ­£ tree çš„é”™è¯¯åˆ†å¸ƒ
```

**ä¸ºä»€ä¹ˆéœ€è¦ oracle_action ä½œä¸ºæ ‡ç­¾**:
- å³ä½¿ç”¨ tree é‡‡æ ·ï¼Œä¹Ÿè¦å­¦ä¹  oracle çš„å†³ç­–
- è¿™æ · tree é€æ¸ä¿®æ­£è‡ªå·±çš„é”™è¯¯

#### å‡½æ•° 5: `train_viper_maskable_ppo()` - ä¸»è®­ç»ƒå¾ªç¯

```python
def train_viper_maskable_ppo(args):
    """
    VIPER ä¸»è®­ç»ƒæµç¨‹
    """
    # 1. åŠ è½½ Oracle
    env, oracle = load_maskable_ppo_oracle(
        args.oracle_path, args.env_name, args.opponent_type
    )

    # 2. åˆå§‹åŒ–
    dataset = []
    policy = None
    policies = []
    rewards = []

    n_steps_per_iter = args.total_timesteps // args.n_iter

    # 3. VIPER è¿­ä»£
    for iteration in range(args.n_iter):
        # 3.1 ç¡®å®š beta
        beta = 1.0 if iteration == 0 else 0.0

        # 3.2 é‡‡æ ·è½¨è¿¹
        trajectory = sample_trajectory_maskable_ppo(
            oracle, env, policy, beta, n_steps_per_iter
        )
        dataset += trajectory

        # 3.3 å‡†å¤‡è®­ç»ƒæ•°æ®
        X = np.array([traj[0] for traj in dataset])      # çŠ¶æ€
        y = np.array([traj[1] for traj in dataset])      # oracle åŠ¨ä½œ
        weights = np.array([traj[2] for traj in dataset]) # criticality

        # 3.4 è®­ç»ƒå†³ç­–æ ‘
        clf = DecisionTreeClassifier(
            ccp_alpha=0.0001,      # å‰ªæå‚æ•°
            criterion="entropy",    # ä¿¡æ¯å¢ç›Š
            max_depth=args.max_depth,
            max_leaf_nodes=args.max_leaves,
            random_state=42
        )
        clf.fit(X, y, sample_weight=weights)  # â­ åŠ æƒè®­ç»ƒ

        # 3.5 åŒ…è£…å’Œè¯„ä¼°
        wrapped_policy = ProbabilityMaskedTreeWrapper(clf)
        policies.append(clf)
        policy = wrapped_policy

        eval_env = gym.make(args.env_name, opponent_type=args.opponent_type)
        mean_reward, std_reward = evaluate_policy(wrapped_policy, eval_env, n_eval_episodes=100)
        rewards.append(mean_reward)

    # 4. é€‰æ‹©æœ€ä½³æ ‘
    best_idx = np.argmax(rewards)
    best_policy = policies[best_idx]

    return ProbabilityMaskedTreeWrapper(best_policy)
```

**å…³é”®ç‚¹**:
1. **æ•°æ®ç´¯ç§¯**: `dataset += trajectory` (ä¸æ˜¯ `dataset = trajectory`)
2. **åŠ æƒè®­ç»ƒ**: `sample_weight=weights`
3. **è¿­ä»£é€‰æ‹©**: è¿”å› N æ£µæ ‘ä¸­æ€§èƒ½æœ€å¥½çš„

---

## 5. å…³é”®æŠ€æœ¯ç»†èŠ‚

### 5.1 ä¸ºä»€ä¹ˆ Beta = 1 (iter 0) then 0 (iter 1+)?

è¿™æ˜¯ **DAgger (Dataset Aggregation)** ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ã€‚

#### é—®é¢˜èƒŒæ™¯

**æ™®é€šæ¨¡ä»¿å­¦ä¹ **çš„é—®é¢˜:
```python
# åªç”¨ oracle é‡‡æ ·
for iteration in range(N):
    trajectory = sample(oracle)  # å§‹ç»ˆç”¨ oracle
    tree.fit(trajectory)
```

**é—®é¢˜**: Tree çš„é”™è¯¯ä¼šç´¯ç§¯ï¼š
```
Step 1: Tree é¢„æµ‹é”™ â†’ è¿›å…¥ oracle æ²¡è§è¿‡çš„çŠ¶æ€
Step 2: Tree ä¸çŸ¥é“æ€ä¹ˆåŠ â†’ ç»§ç»­é¢„æµ‹é”™
Step 3: çŠ¶æ€è¶Šæ¥è¶Šåç¦» oracle çš„åˆ†å¸ƒ â†’ æ€§èƒ½å´©æºƒ
```

è¿™å«åš **Covariate Shiftï¼ˆåå˜é‡åç§»ï¼‰**ã€‚

#### DAgger çš„è§£å†³æ–¹æ¡ˆ

```python
# Iteration 0: ç”¨ oracle é‡‡æ ·
beta = 1.0  # 100% oracle
trajectory = sample(oracle)  # é«˜è´¨é‡åˆå§‹æ•°æ®

# Iteration 1+: ç”¨ tree é‡‡æ ·
beta = 0.0  # 100% tree
trajectory = sample(tree)  # Tree è‡ªå·±çš„çŠ¶æ€åˆ†å¸ƒ
# ä½†æ ‡ç­¾ä»ç„¶æ˜¯ oracle çš„åŠ¨ä½œï¼

# è¿™æ · tree åœ¨è‡ªå·±ä¼šé‡åˆ°çš„çŠ¶æ€ä¸Šå­¦ä¹  oracle çš„è¡Œä¸º
```

**æ•ˆæœ**:
- âœ… Tree å­¦ä¹ å¦‚ä½•ä»è‡ªå·±çš„é”™è¯¯ä¸­æ¢å¤
- âœ… ä¿®æ­£çŠ¶æ€åˆ†å¸ƒåç§»
- âœ… æ›´é²æ£’çš„ç­–ç•¥

### 5.2 Criticality Loss çš„æ•°å­¦åŸç†

#### Max-Entropy RL çš„ Q å‡½æ•°è¿‘ä¼¼

åœ¨ Max-Entropy RL æ¡†æ¶ä¸‹ï¼ˆPPO ä½¿ç”¨æ­¤æ¡†æ¶ï¼‰ï¼š

```
Q(s, a) â‰ˆ log Ï€(a|s) + const
```

**åŸå› **: æœ€ä¼˜ç­–ç•¥æ»¡è¶³
```
Ï€*(a|s) âˆ exp(Q(s,a) / Î±)
â‡’ log Ï€*(a|s) = Q(s,a) / Î± - log Z(s)
â‡’ Q(s,a) â‰ˆ Î± Â· log Ï€(a|s) + const
```

å…¶ä¸­ Î± æ˜¯æ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸ä¸º1ï¼‰ï¼ŒZ(s) æ˜¯å½’ä¸€åŒ–å¸¸æ•°ã€‚

#### Criticality å®šä¹‰

```
Criticality(s) = max_a Q(s,a) - min_a Q(s,a)
               â‰ˆ max_a log Ï€(a|s) - min_a log Ï€(a|s)
```

**ç›´è§‚ç†è§£**:
- **é«˜ criticality**: é€‰å¯¹åŠ¨ä½œå¾ˆé‡è¦ï¼ˆå·®è·å¤§ï¼‰
- **ä½ criticality**: é€‰ä»€ä¹ˆéƒ½å·®ä¸å¤šï¼ˆå·®è·å°ï¼‰

#### ä¸ºä»€ä¹ˆåªè€ƒè™‘åˆæ³•åŠ¨ä½œ?

```python
# é”™è¯¯æ–¹å¼ï¼ˆåŒ…å«éæ³•åŠ¨ä½œï¼‰
all_log_probs = [-0.5, -1.2, -5.0, -5.0, -5.0, ...]  # åé¢æ˜¯éæ³•åŠ¨ä½œ
criticality = -0.5 - (-5.0) = 4.5  # è¢«éæ³•åŠ¨ä½œæ±¡æŸ“ï¼

# æ­£ç¡®æ–¹å¼ï¼ˆä»…åˆæ³•åŠ¨ä½œï¼‰
legal_log_probs = [-0.5, -1.2]  # åªæœ‰åˆæ³•åŠ¨ä½œ
criticality = -0.5 - (-1.2) = 0.7  # çœŸå®å·®è·
```

### 5.3 å†³ç­–æ ‘è®­ç»ƒçš„æƒé‡ä½œç”¨

```python
clf.fit(X, y, sample_weight=weights)
```

**Weighted Loss**:
```
Loss = Î£ weight[i] Ã— CrossEntropy(y_pred[i], y_true[i])
     = Î£ criticality[i] Ã— CE(...)
```

**æ•ˆæœ**:
- é‡è¦çŠ¶æ€çš„é”™è¯¯è¢«æƒ©ç½šå¾—æ›´é‡
- ä¸é‡è¦çŠ¶æ€çš„é”™è¯¯å½±å“è¾ƒå°
- Tree ä¼˜å…ˆå­¦å¥½å…³é”®å†³ç­–ç‚¹

**ç¤ºä¾‹**:
```
# æ•°æ®é›†
State 1 (å¼€å±€):   weight = 0.2  (ä¸é‡è¦)
State 2 (ä¸­å±€):   weight = 1.5  (ä¸€èˆ¬)
State 3 (å†³èƒœ):   weight = 8.0  (éå¸¸é‡è¦ï¼)

# å¦‚æœ State 3 é¢„æµ‹é”™ï¼ŒLoss å¢åŠ  8.0 Ã— CE
# å¦‚æœ State 1 é¢„æµ‹é”™ï¼ŒLoss åªå¢åŠ  0.2 Ã— CE
# â†’ Tree ä¼šä¼˜å…ˆå­¦å¥½ State 3
```

### 5.4 ä¸ºä»€ä¹ˆé€‰æ‹©å•æ£µåˆ†ç±»æ ‘è€Œä¸æ˜¯å›å½’æ ‘?

é¡¹ç›®ä¸­æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š
1. **å•æ£µåˆ†ç±»æ ‘** (æœ¬å®ç°) â† æ¨è
2. **å¤šæ£µå›å½’æ ‘** (archive/regression_tree_approach/)

#### å¯¹æ¯”

| ç»´åº¦ | åˆ†ç±»æ ‘ | å›å½’æ ‘ (9æ£µ) |
|------|--------|-------------|
| **è¾“å‡º** | åŠ¨ä½œç±»åˆ« (0-8) | æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼ |
| **å¯è§£é‡Šæ€§** | âœ… å®Œæ•´ IF-THEN è§„åˆ™ | âœ— 9æ£µæ ‘ï¼Œéš¾ä»¥ç†è§£ |
| **æ¨¡å‹å¤§å°** | âœ… 1æ£µæ ‘ | âœ— 9æ£µæ ‘ |
| **éæ³•åŠ¨ä½œ** | âœ… Masking å¤„ç† | â–³ éœ€è¦é¢å¤–é€»è¾‘ |
| **ç²¾åº¦** | â–³ å¯èƒ½ç•¥ä½ | âœ… å¯èƒ½ç•¥é«˜ |

**ä¸ºä»€ä¹ˆåˆ†ç±»æ ‘æ›´å¥½**:
1. VIPER çš„æ ¸å¿ƒç›®æ ‡æ˜¯**å¯è§£é‡Šæ€§**ï¼Œä¸æ˜¯æ€§èƒ½
2. å•æ£µæ ‘å¯ä»¥å®Œæ•´æå–å†³ç­–é€»è¾‘
3. é…åˆæ¦‚ç‡æ©ç ï¼Œä¸ç‰ºç‰²æ­£ç¡®æ€§

---

## 6. ä¸é¡¹ç›®ä¸­ DQN-VIPER çš„å¯¹æ¯”

### 6.1 ä»£ç å¯¹æ¯”

| ç»„ä»¶ | DQN-VIPER (`train/viper.py`) | MaskablePPO-VIPER (`train/viper_maskable_ppo.py`) |
|------|------------------------------|--------------------------------------------------|
| **Oracle** | `stable_baselines3.DQN` | `sb3_contrib.MaskablePPO` |
| **ç¯å¢ƒ** | ç›´æ¥ `gym.make()` | `gym.make()` + `ActionMasker` |
| **Criticality** | `Q_max - Q_min` (æ‰€æœ‰åŠ¨ä½œ) | `log_prob_max - log_prob_min` (ä»…åˆæ³•) |
| **Tree è¾“å‡º** | `tree.predict()` | `tree.predict_proba()` + masking |
| **Action Masking** | âŒ ä¸æ”¯æŒ | âœ… å®Œå…¨æ”¯æŒ |

### 6.2 DQN-VIPER çš„ Criticality è®¡ç®—

```python
# train/viper.py ç¬¬ 196-203 è¡Œ
if isinstance(model, DQN):
    obs_tensor = torch.from_numpy(obs).to(model.device)
    q_values = model.q_net(obs_tensor).detach().cpu().numpy()
    # q_values: (n_env, n_actions)
    return q_values.max(axis=1) - q_values.min(axis=1)  # æ‰€æœ‰åŠ¨ä½œ
```

**é—®é¢˜**: å¯¹äº TicTacToeï¼ŒåŒ…å«éæ³•åŠ¨ä½œçš„ Q å€¼ã€‚

### 6.3 MaskablePPO-VIPER çš„æ”¹è¿›

```python
# train/viper_maskable_ppo.py
log_probs = oracle.policy.get_distribution(obs).logits
for i in range(obs.shape[0]):
    legal_actions = np.where(obs[i] == 0)[0]
    legal_log_probs = log_probs[i][legal_actions]
    criticality = legal_log_probs.max() - legal_log_probs.min()  # ä»…åˆæ³•åŠ¨ä½œ
```

**æ”¹è¿›**: åªè€ƒè™‘åˆæ³•åŠ¨ä½œï¼Œæ›´å‡†ç¡®ã€‚

### 6.4 ä½•æ—¶ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬?

| åœºæ™¯ | æ¨èç‰ˆæœ¬ |
|------|----------|
| CartPole, Pong (æ—  masking) | `viper.py` (DQN-VIPER) |
| TicTacToe (éœ€è¦ masking) | `viper_maskable_ppo.py` â­ |
| å…¶ä»–æ£‹ç±»æ¸¸æˆ (éœ€è¦ masking) | `viper_maskable_ppo.py` |

---

## 7. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: `AttributeError: 'OrderEnforcing' object has no attribute 'board'`

**åŸå› **: Gymnasium çš„åŒ…è£…å™¨å±‚çº§å¤ªæ·±ï¼Œæ— æ³•ç›´æ¥è®¿é—® `env.board`ã€‚

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨é€’å½’è§£åŒ…ï¼ˆå·²ä¿®å¤ï¼‰

```python
def mask_fn(env):
    current_env = env
    while hasattr(current_env, 'env') and not hasattr(current_env, 'board'):
        current_env = current_env.env
    return (current_env.board == 0).astype(np.int8)
```

### Q2: VIPER è®­ç»ƒæ—¶åªç”¨ä¸€ä¸ªå¯¹æ‰‹ï¼Œæ˜¯å¦éœ€è¦æ”¹æˆå¤šå¯¹æ‰‹?

**ç­”æ¡ˆ**: **ä¸éœ€è¦**ã€‚

**åŸå› **:
1. VIPER æ˜¯æ¨¡ä»¿å­¦ä¹ ï¼Œä¸æ˜¯ä»é›¶å­¦ä¹ 
2. Oracle å·²åœ¨å¤šå¯¹æ‰‹ä¸Šè®­ç»ƒ
3. å•å¯¹æ‰‹è¶³ä»¥æå– Oracle çš„çŸ¥è¯†
4. ç¬¦åˆ VIPER åŸè®ºæ–‡è®¾è®¡

**å¦‚æœæƒ³è¦å¤šå¯¹æ‰‹**:
```python
# ä¿®æ”¹ sample_trajectory_maskable_ppo
opponents = ['random', 'minmax']
for iteration in range(n_iter):
    opponent = random.choice(opponents)
    env = gym.make(env_name, opponent_type=opponent)
    # ...
```

### Q3: å†³ç­–æ ‘æ€§èƒ½ä¸å¦‚ç¥ç»ç½‘ç»œæ€ä¹ˆåŠ?

**ç­”æ¡ˆ**: è¿™æ˜¯**æ­£å¸¸çš„**ã€‚

**åŸå› **:
- å†³ç­–æ ‘çš„è¡¨è¾¾èƒ½åŠ› < ç¥ç»ç½‘ç»œ
- VIPER çš„ç›®æ ‡æ˜¯**å¯è§£é‡Šæ€§**ï¼Œä¸æ˜¯æœ€é«˜æ€§èƒ½
- é€šå¸¸å¯ä»¥è¾¾åˆ° Oracle çš„ 70-90% æ€§èƒ½

**æ”¹è¿›æ–¹æ³•**:
1. å¢åŠ æ ‘çš„å¤æ‚åº¦ (`--max-depth 12`, `--max-leaves 80`)
2. å¢åŠ é‡‡æ ·æ•°æ® (`--total-timesteps 100000`)
3. ä½¿ç”¨æ›´å¼ºçš„ Oracle

### Q4: å¦‚ä½•éªŒè¯å®ç°æ˜¯å¦æ­£ç¡®?

**æ£€æŸ¥æ¸…å•**:

1. âœ… **æ— éæ³•åŠ¨ä½œ**: è¯„ä¼°æ—¶ `illegal_moves == 0`
2. âœ… **Beta è°ƒåº¦**: ç¬¬ä¸€è½® beta=1ï¼Œåç»­ beta=0
3. âœ… **æ•°æ®ç´¯ç§¯**: Dataset æŒç»­å¢é•¿
4. âœ… **æ€§èƒ½æå‡**: åæœŸ iteration æ€§èƒ½ > åˆæœŸ
5. âœ… **ä¸ Oracle å¯¹æ¯”**: Tree æ€§èƒ½åº”è¾¾åˆ° Oracle çš„ 70%+

**æµ‹è¯•å‘½ä»¤**:
```bash
# è®­ç»ƒ
python train/viper_maskable_ppo.py \
    --oracle-path log/oracle_TicTacToe_ppo_aggressive.zip \
    --output log/viper_test.joblib \
    --total-timesteps 20000 \
    --n-iter 5 \
    --test

# è¯„ä¼°
python evaluation/evaluate_viper_tree.py \
    --model-path log/viper_test.joblib \
    --opponent both \
    --n-episodes 100
```

---

## 8. æ€»ç»“

### 8.1 æ ¸å¿ƒç»“è®º

1. **ç®—æ³•ä¸€è‡´æ€§**: æœ¬å®ç°ä¸ VIPER åŸè®ºæ–‡çš„æ ¸å¿ƒç®—æ³•**å®Œå…¨ä¸€è‡´**
2. **å¿…è¦é€‚é…**: é’ˆå¯¹ MaskablePPO å’Œ TicTacToe åšäº†**å¿…è¦ä¸”åˆç†**çš„é€‚é…
3. **å•ä¸€å¯¹æ‰‹**: ä½¿ç”¨å•ä¸€å¯¹æ‰‹æ˜¯**ç¬¦åˆ VIPER è®¾è®¡å“²å­¦**çš„ï¼Œä¸æ˜¯ç¼ºé™·
4. **å¯è§£é‡Šæ€§ä¼˜å…ˆ**: é€‰æ‹©å•æ£µåˆ†ç±»æ ‘ä¼˜å…ˆä¿è¯**å¯è§£é‡Šæ€§**

### 8.2 ä¸åŸè®ºæ–‡çš„å¯¹æ¯”æ€»ç»“

| ç»´åº¦ | ä¸€è‡´æ€§ | è¯´æ˜ |
|------|--------|------|
| æ ¸å¿ƒç®—æ³•æµç¨‹ | âœ… 100% | DAgger + Weighted Imitation |
| Criticality Loss | âœ… 100% | log_prob_max - log_prob_min (ä»…åˆæ³•åŠ¨ä½œ) |
| Beta è°ƒåº¦ | âœ… 100% | Î²=1 then 0 |
| æ ‘æ¨¡å‹ç±»å‹ | âœ… 100% | DecisionTreeClassifier |
| æ ·æœ¬åŠ æƒ | âœ… 100% | sample_weight=criticality |
| è¿­ä»£é€‰æ‹© | âœ… 100% | é€‰æ‹©æœ€ä½³æ ‘ |
| Oracle ç±»å‹ | ğŸ”§ æ‰©å±• | æ”¯æŒ MaskablePPO (åŸè®ºæ–‡: DQN/PPO) |
| éæ³•åŠ¨ä½œå¤„ç† | ğŸ”§ å¢å¼º | Probability masking (åŸè®ºæ–‡: æœªæ˜ç¡®) |

### 8.3 æ¨èä½¿ç”¨æ–¹å¼

```bash
# æ ‡å‡†è®­ç»ƒï¼ˆæ¨èï¼‰
python train/viper_maskable_ppo.py \
    --oracle-path log/oracle_TicTacToe_ppo_aggressive.zip \
    --output log/viper_TicTacToe_from_ppo.joblib \
    --total-timesteps 50000 \
    --n-iter 10 \
    --max-depth 10 \
    --max-leaves 50 \
    --opponent-type minmax \
    --test
```

---

## å‚è€ƒæ–‡çŒ®

1. Bastani, O., et al. (2018). "Verifiable Reinforcement Learning via Policy Extraction." NeurIPS.
2. Ross, S., et al. (2011). "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning." AISTATS. (DAgger)
3. Haarnoja, T., et al. (2018). "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor." ICML. (Max-Entropy RL)
