# VIPER + MaskablePPO å­¦ä¹ æŒ‡å—

## ä»é›¶å¼€å§‹å®ç° VIPER å†³ç­–æ ‘è®­ç»ƒ

æœ¬æŒ‡å—å°†æ•™ä½ å¦‚ä½•**ä»é›¶å¼€å§‹**å®ç° VIPER ç®—æ³•ï¼Œå¸®åŠ©ä½ æ·±å…¥ç†è§£æ¯ä¸€æ­¥çš„åŸç†å’Œå®ç°ç»†èŠ‚ã€‚

---

## ğŸ“š éœ€è¦çš„åº“

```python
# æ ¸å¿ƒåº“
import numpy as np                          # æ•°å€¼è®¡ç®—
import torch                                # PyTorchï¼ˆOracle ä½¿ç”¨ï¼‰
import gymnasium as gym                     # ç¯å¢ƒ
from sklearn.tree import DecisionTreeClassifier  # å†³ç­–æ ‘
from sb3_contrib import MaskablePPO         # Oracle æ¨¡å‹
import joblib                               # æ¨¡å‹ä¿å­˜/åŠ è½½
```

---

## ğŸ¯ å­¦ä¹ è·¯çº¿å›¾

æŒ‰ç…§ä»¥ä¸‹é¡ºåºå®ç°ï¼Œæ¯ä¸€æ­¥éƒ½ç‹¬ç«‹æµ‹è¯•é€šè¿‡åå†è¿›è¡Œä¸‹ä¸€æ­¥ã€‚

---

## æ­¥éª¤ 1: ç¯å¢ƒäº¤äº’åŸºç¡€ ğŸŒ

### ç›®çš„
ç†è§£å¦‚ä½•ä¸ TicTacToe ç¯å¢ƒäº¤äº’ï¼ŒæŒæ¡åŸºæœ¬çš„çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±æ¦‚å¿µã€‚

### éœ€è¦å®ç°

```python
def test_environment():
    """æµ‹è¯•ç¯å¢ƒåŸºæœ¬åŠŸèƒ½"""
    # 1. åˆ›å»ºç¯å¢ƒ
    # 2. Reset è·å–åˆå§‹çŠ¶æ€
    # 3. æ‰“å°çŠ¶æ€ï¼ˆ9ç»´å‘é‡ï¼‰
    # 4. å°è¯•å‡ ä¸ªåŠ¨ä½œ
    # 5. è§‚å¯Ÿ reward å’Œ done
```

### å‚è€ƒæ–‡ä»¶
- **`gym_env/tictactoe.py`** - ç¯å¢ƒå®Œæ•´å®ç°
  - é‡ç‚¹ï¼š`reset()` æ–¹æ³•ï¼ˆç¬¬ 75-83 è¡Œï¼‰
  - é‡ç‚¹ï¼š`step()` æ–¹æ³•ï¼ˆç¬¬ 85-137 è¡Œï¼‰

### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **observation** | 9ç»´å‘é‡è¡¨ç¤ºæ£‹ç›˜ | `[0, 0, 1, -1, 0, 0, 0, 0, 0]` |
| | `-1` = å¯¹æ‰‹çš„æ£‹å­ (O) | |
| | `0` = ç©ºä½ | |
| | `1` = è‡ªå·±çš„æ£‹å­ (X) | |
| **action** | 0-8 çš„æ•´æ•°ï¼Œè¡¨ç¤ºä½ç½® | `4` = ä¸­å¿ƒä½ç½® |
| **reward** | +1 = èƒœåˆ© | |
| | -1 = å¤±è´¥ | |
| | 0 = å¹³å±€ | |
| | -10 = éæ³•ç§»åŠ¨ | |
| **terminated** | æ¸¸æˆæ˜¯å¦ç»“æŸ | `True` / `False` |

### æµ‹è¯•ä»£ç 

```python
import gymnasium as gym
import gym_env  # æ³¨å†Œ TicTacToe ç¯å¢ƒ

# åˆ›å»ºç¯å¢ƒ
env = gym.make('TicTacToe-v0', opponent_type='random')

# Reset è·å–åˆå§‹çŠ¶æ€
obs, info = env.reset()
print("åˆå§‹çŠ¶æ€:", obs)  # åº”è¯¥æ˜¯ [0, 0, 0, 0, 0, 0, 0, 0, 0]

# ä¸‹ä¸€æ­¥æ£‹ï¼ˆé€‰æ‹©ä¸­å¿ƒä½ç½®ï¼‰
action = 4
obs, reward, terminated, truncated, info = env.step(action)
print("æ–°çŠ¶æ€:", obs)     # ä½ç½®4åº”è¯¥å˜æˆ1
print("å¥–åŠ±:", reward)
print("æ¸¸æˆç»“æŸ:", terminated)

# ç»§ç»­ç©å‡ æ­¥
done = terminated or truncated
while not done:
    # éšæœºé€‰æ‹©åˆæ³•åŠ¨ä½œ
    legal_actions = [i for i in range(9) if obs[i] == 0]
    if not legal_actions:
        break
    action = np.random.choice(legal_actions)
    obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    print(f"åŠ¨ä½œ: {action}, å¥–åŠ±: {reward}, å®Œæˆ: {done}")

env.close()
```

### éªŒè¯æ¸…å•
- [ ] èƒ½æˆåŠŸåˆ›å»ºç¯å¢ƒ
- [ ] Reset è¿”å›å…¨ 0 çš„çŠ¶æ€
- [ ] Step èƒ½æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å› 5 ä¸ªå€¼
- [ ] éæ³•åŠ¨ä½œä¼šè¿”å› -10 å¥–åŠ±
- [ ] æ¸¸æˆèƒ½æ­£å¸¸ç»“æŸ

---

## æ­¥éª¤ 2: åŠ è½½ Oracle ğŸ§™

### ç›®çš„
åŠ è½½ä½ è®­ç»ƒå¥½çš„ MaskablePPO æ¨¡å‹ï¼Œç†è§£å¦‚ä½•ä½¿ç”¨ç¥ç»ç½‘ç»œç­–ç•¥ã€‚

### éœ€è¦å®ç°

```python
def load_oracle(oracle_path):
    """åŠ è½½ MaskablePPO Oracle"""
    # 1. åˆ›å»ºç¯å¢ƒ
    # 2. ä½¿ç”¨ MaskablePPO.load() åŠ è½½æ¨¡å‹
    # 3. è¿”å› oracle å’Œ env
```

### å‚è€ƒæ–‡ä»¶
- **`train/train_delta_selfplay_ppo.py`**
  - é‡ç‚¹ï¼šæ¨¡å‹ä¿å­˜ï¼ˆç¬¬ 314 è¡Œï¼‰
  - åŠ è½½æ˜¯ä¿å­˜çš„åè¿‡ç¨‹

### å…³é”® API

```python
from sb3_contrib import MaskablePPO
import gymnasium as gym

# åˆ›å»ºç¯å¢ƒ
env = gym.make('TicTacToe-v0', opponent_type='minmax')

# åŠ è½½æ¨¡å‹
oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

print("âœ“ Oracle åŠ è½½æˆåŠŸ")
```

### Action Maskingï¼ˆé‡è¦ï¼ï¼‰

```python
# è·å–å½“å‰çŠ¶æ€çš„åˆæ³•åŠ¨ä½œæ©ç 
obs, _ = env.reset()
mask = (obs == 0).astype(bool)  # True=åˆæ³•, False=éæ³•

# è®© Oracle é€‰æ‹©åŠ¨ä½œï¼ˆå¸¦ maskingï¼‰
action, _ = oracle.predict(obs, action_masks=mask, deterministic=True)
print("Oracle é€‰æ‹©åŠ¨ä½œ:", action)

# éªŒè¯åŠ¨ä½œåˆæ³•
assert obs[action] == 0, "Oracle é€‰æ‹©äº†éæ³•åŠ¨ä½œï¼"
```

### æµ‹è¯•ä»£ç 

```python
def test_oracle():
    """æµ‹è¯• Oracle åŠ è½½å’Œä½¿ç”¨"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # ç©ä¸€å±€
    obs, _ = env.reset()
    done = False
    episode_reward = 0

    while not done:
        # è·å–åˆæ³•åŠ¨ä½œæ©ç 
        mask = (obs == 0).astype(bool)

        # Oracle é€‰æ‹©åŠ¨ä½œ
        action, _ = oracle.predict(obs, action_masks=mask, deterministic=True)

        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        episode_reward += reward

        print(f"åŠ¨ä½œ: {action}, å¥–åŠ±: {reward}")

    print(f"æ€»å¥–åŠ±: {episode_reward}")
    env.close()
```

### éªŒè¯æ¸…å•
- [ ] Oracle èƒ½æˆåŠŸåŠ è½½
- [ ] predict() èƒ½è¿”å›åŠ¨ä½œ
- [ ] å¸¦ masking æ—¶ä¸ä¼šé€‰æ‹©éæ³•åŠ¨ä½œ
- [ ] Oracle èƒ½å®Œæ•´ç©ä¸€å±€

---

## æ­¥éª¤ 3: è®¡ç®— Criticalityï¼ˆé‡è¦æ€§ï¼‰âš–ï¸

### ç›®çš„
åˆ¤æ–­ä¸€ä¸ªçŠ¶æ€æœ‰å¤šé‡è¦ï¼Œä¸ºè®­ç»ƒå†³ç­–æ ‘æä¾›æ ·æœ¬æƒé‡ã€‚

### æ•°å­¦åŸç†

åœ¨ Max-Entropy RL æ¡†æ¶ä¸‹ï¼š

```
Q(s,a) â‰ˆ log Ï€(a|s)

criticality(s) = max_aâˆˆLegal Q(s,a) - min_aâˆˆLegal Q(s,a)
               = max_aâˆˆLegal log Ï€(a|s) - min_aâˆˆLegal log Ï€(a|s)
```

**ç›´è§‚ç†è§£**:
- **é«˜ criticality**: é€‰å¯¹åŠ¨ä½œå¾ˆå…³é”®ï¼ˆä¾‹å¦‚ï¼šå³å°†è·èƒœçš„çŠ¶æ€ï¼‰
- **ä½ criticality**: éšä¾¿é€‰éƒ½å·®ä¸å¤šï¼ˆä¾‹å¦‚ï¼šå¼€å±€ç¬¬ä¸€æ­¥ï¼‰

### éœ€è¦å®ç°

```python
def compute_criticality(oracle, observation):
    """è®¡ç®—çŠ¶æ€çš„ criticality"""
    # 1. å°† observation è½¬ä¸º tensor
    # 2. ä½¿ç”¨ oracle.policy.get_distribution() è·å–åˆ†å¸ƒ
    # 3. æå– log probabilities (logits)
    # 4. æ‰¾åˆ°åˆæ³•åŠ¨ä½œï¼ˆobservation == 0ï¼‰
    # 5. è®¡ç®—åˆæ³•åŠ¨ä½œçš„ max - min
    # 6. è¿”å› criticalityï¼ˆfloatï¼‰
```

### å‚è€ƒæ–‡ä»¶
- **`train/viper.py`** ç¬¬ 187-222 è¡Œ `get_loss()` å‡½æ•°
  - é‡ç‚¹ï¼šPPO éƒ¨åˆ†ï¼ˆç¬¬ 204-220 è¡Œï¼‰

### å…³é”® API

```python
import torch

def compute_criticality(oracle, observation):
    """è®¡ç®— criticality"""
    with torch.no_grad():
        # 1. è½¬ä¸º tensor
        obs_tensor = torch.as_tensor(observation, dtype=torch.float32)
        obs_tensor = obs_tensor.unsqueeze(0)  # (1, 9)
        obs_tensor = obs_tensor.to(oracle.device)

        # 2. è·å–ç­–ç•¥åˆ†å¸ƒ
        distribution = oracle.policy.get_distribution(obs_tensor)
        log_probs = distribution.distribution.logits.cpu().numpy()[0]  # (9,)

        # 3. æ‰¾åˆæ³•åŠ¨ä½œ
        legal_actions = np.where(observation == 0)[0]

        if len(legal_actions) == 0:
            return 0.0  # æ— åˆæ³•åŠ¨ä½œï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰

        # 4. è®¡ç®—åˆæ³•åŠ¨ä½œçš„ Q å€¼èŒƒå›´
        legal_log_probs = log_probs[legal_actions]
        criticality = legal_log_probs.max() - legal_log_probs.min()

        return float(criticality)
```

### ç¤ºä¾‹

```python
# çŠ¶æ€1: å³å°†è·èƒœï¼ˆå¯¹æ‰‹æœ‰ä¸¤ä¸ªè¿æˆä¸€çº¿ï¼Œæˆ‘å¿…é¡»å µï¼‰
obs1 = np.array([0, -1, -1,  # O O .
                 0,  1,  0,  # . X .
                 0,  0,  0]) # . . .
legal_actions = [0, 3, 5, 6, 7, 8]
log_probs = {0: -0.1, 3: -3.0, 5: -2.5, ...}  # ä½ç½®0å‡ ä¹å¿…é€‰
criticality1 = -0.1 - (-3.0) = 2.9  # å¾ˆé‡è¦ï¼

# çŠ¶æ€2: å¼€å±€ç¬¬ä¸€æ­¥
obs2 = np.array([0, 0, 0,
                 0, 0, 0,
                 0, 0, 0])
legal_actions = [0,1,2,3,4,5,6,7,8]
log_probs = [-1.2, -1.3, -1.1, -1.2, -1.0, ...]  # å·®ä¸å¤š
criticality2 = -1.0 - (-1.3) = 0.3  # ä¸é‡è¦
```

### æµ‹è¯•ä»£ç 

```python
def test_criticality():
    """æµ‹è¯• criticality è®¡ç®—"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # æµ‹è¯•å‡ ä¸ªçŠ¶æ€
    test_states = [
        # å¼€å±€
        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32),
        # ä¸­å±€
        np.array([1, -1, 0, 0, 1, 0, 0, 0, -1], dtype=np.float32),
        # å…³é”®æ—¶åˆ»ï¼ˆä¸¤ä¸ªXè¿ä¸€çº¿ï¼‰
        np.array([1, 1, 0, -1, 0, 0, 0, -1, 0], dtype=np.float32),
    ]

    for i, obs in enumerate(test_states):
        crit = compute_criticality(oracle, obs)
        print(f"çŠ¶æ€{i+1} criticality: {crit:.3f}")
```

### éªŒè¯æ¸…å•
- [ ] å‡½æ•°èƒ½è¿”å›æ•°å€¼ï¼ˆä¸æŠ¥é”™ï¼‰
- [ ] Criticality åœ¨åˆç†èŒƒå›´ï¼ˆé€šå¸¸ 0-5ï¼‰
- [ ] å…³é”®çŠ¶æ€çš„ criticality > æ™®é€šçŠ¶æ€
- [ ] æ²¡æœ‰åˆæ³•åŠ¨ä½œæ—¶è¿”å› 0

---

## æ­¥éª¤ 4: é‡‡æ ·è½¨è¿¹ ğŸ²

### ç›®çš„
åœ¨ç¯å¢ƒä¸­é‡‡æ ·ï¼Œæ”¶é›†è®­ç»ƒæ•°æ® `(state, action, weight)`ã€‚

### VIPER æ•°æ®æ”¶é›†ç­–ç•¥

**ç¬¬ä¸€è½®ï¼ˆiteration 0ï¼‰**:
- ä½¿ç”¨ **Oracle** é‡‡æ ·
- ç›®çš„ï¼šæ”¶é›†é«˜è´¨é‡åˆå§‹æ•°æ®

**åç»­è½®ï¼ˆiteration 1+ï¼‰**:
- ä½¿ç”¨ **Tree** é‡‡æ ·ï¼ˆDAgger ç­–ç•¥ï¼‰
- ç›®çš„ï¼šè®© Tree åœ¨è‡ªå·±ä¼šé‡åˆ°çš„çŠ¶æ€ä¸Šå­¦ä¹ 
- ä¿®æ­£åå˜é‡åç§»ï¼ˆCovariate Shiftï¼‰

### éœ€è¦å®ç°

```python
def sample_trajectories(oracle, env, n_steps, use_oracle=True):
    """é‡‡æ · n_steps ä¸ªæ ·æœ¬

    Returns:
        dataset: List of (observation, action, weight)
    """
    dataset = []
    obs, _ = env.reset()

    while len(dataset) < n_steps:
        # 1. é€‰æ‹©åŠ¨ä½œï¼ˆOracle æˆ–å…¶ä»–ç­–ç•¥ï¼‰
        if use_oracle:
            # ä½¿ç”¨ Oracle
            mask = (obs == 0).astype(bool)
            action, _ = oracle.predict(obs, action_masks=mask, deterministic=True)
        else:
            # ä½¿ç”¨ Treeï¼ˆæˆ–éšæœºï¼‰
            legal_actions = np.where(obs == 0)[0]
            action = np.random.choice(legal_actions)

        # 2. è·å– Oracle çš„åŠ¨ä½œï¼ˆä½œä¸ºæ ‡ç­¾ï¼‰
        mask = (obs == 0).astype(bool)
        oracle_action, _ = oracle.predict(obs, action_masks=mask, deterministic=True)

        # 3. è®¡ç®— criticalityï¼ˆæƒé‡ï¼‰
        criticality = compute_criticality(oracle, obs)

        # 4. ä¿å­˜æ ·æœ¬
        dataset.append((obs.copy(), oracle_action, criticality))

        # 5. æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        if done:
            obs, _ = env.reset()
        else:
            obs = next_obs

    return dataset
```

### å‚è€ƒæ–‡ä»¶
- **`train/viper.py`** ç¬¬ 137-184 è¡Œ `sample_trajectory()` å‡½æ•°

### æ•°æ®æ ¼å¼

```python
# dataset æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸‰å…ƒç»„
dataset = [
    (obs1, action1, weight1),  # æ ·æœ¬1
    (obs2, action2, weight2),  # æ ·æœ¬2
    ...
]

# ç¤ºä¾‹
obs = np.array([0, 0, 1, -1, 0, 0, 0, 0, 0])  # çŠ¶æ€
action = 4                                      # Oracle é€‰æ‹©çš„åŠ¨ä½œ
weight = 1.5                                    # Criticality
```

### æµ‹è¯•ä»£ç 

```python
def test_sampling():
    """æµ‹è¯•é‡‡æ ·"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # é‡‡æ · 100 ä¸ªæ ·æœ¬
    dataset = sample_trajectories(oracle, env, n_steps=100, use_oracle=True)

    print(f"âœ“ é‡‡æ ·å®Œæˆï¼Œæ”¶é›† {len(dataset)} ä¸ªæ ·æœ¬")

    # æ£€æŸ¥æ•°æ®æ ¼å¼
    obs, action, weight = dataset[0]
    print(f"  çŠ¶æ€ shape: {obs.shape}")      # (9,)
    print(f"  åŠ¨ä½œ: {action}")                # 0-8
    print(f"  æƒé‡: {weight:.3f}")            # float

    # æ£€æŸ¥åˆæ³•æ€§
    for obs, action, weight in dataset[:10]:
        assert obs[action] == 0, f"éæ³•åŠ¨ä½œï¼obs[{action}] = {obs[action]}"
    print("âœ“ æ‰€æœ‰åŠ¨ä½œåˆæ³•")
```

### éªŒè¯æ¸…å•
- [ ] èƒ½é‡‡æ ·æŒ‡å®šæ•°é‡çš„æ ·æœ¬
- [ ] æ¯ä¸ªæ ·æœ¬æ˜¯ (obs, action, weight) ä¸‰å…ƒç»„
- [ ] æ‰€æœ‰åŠ¨ä½œéƒ½æ˜¯åˆæ³•çš„ï¼ˆobs[action] == 0ï¼‰
- [ ] Weight åœ¨åˆç†èŒƒå›´å†…

---

## æ­¥éª¤ 5: è®­ç»ƒå†³ç­–æ ‘ ğŸŒ³

### ç›®çš„
ä½¿ç”¨ sklearn è®­ç»ƒå†³ç­–æ ‘ï¼Œæ¨¡ä»¿ Oracle çš„è¡Œä¸ºã€‚

### éœ€è¦å®ç°

```python
def train_decision_tree(dataset, max_depth=10, max_leaves=50):
    """è®­ç»ƒå†³ç­–æ ‘

    Args:
        dataset: [(obs, action, weight), ...]
        max_depth: æ ‘çš„æœ€å¤§æ·±åº¦
        max_leaves: æœ€å¤§å¶å­èŠ‚ç‚¹æ•°

    Returns:
        tree: è®­ç»ƒå¥½çš„ DecisionTreeClassifier
    """
    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    X = np.array([sample[0] for sample in dataset])      # (N, 9)
    y = np.array([sample[1] for sample in dataset])      # (N,)
    weights = np.array([sample[2] for sample in dataset])# (N,)

    # 2. åˆ›å»ºå†³ç­–æ ‘
    tree = DecisionTreeClassifier(
        criterion='entropy',        # ä½¿ç”¨ä¿¡æ¯å¢ç›Š
        max_depth=max_depth,        # é™åˆ¶æ·±åº¦ï¼ˆæ§åˆ¶å¯è§£é‡Šæ€§ï¼‰
        max_leaf_nodes=max_leaves,  # é™åˆ¶å¶å­æ•°
        random_state=42,
        ccp_alpha=0.0001           # å‰ªæå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    )

    # 3. è®­ç»ƒï¼ˆå¸¦æƒé‡ï¼‰
    tree.fit(X, y, sample_weight=weights)

    print(f"âœ“ è®­ç»ƒå®Œæˆ")
    print(f"  æ ‘æ·±åº¦: {tree.tree_.max_depth}")
    print(f"  å¶å­èŠ‚ç‚¹æ•°: {tree.tree_.n_leaves}")

    return tree
```

### å‚è€ƒæ–‡ä»¶
- **`train/viper.py`** ç¬¬ 75-84 è¡Œ
- sklearn æ–‡æ¡£: https://scikit-learn.org/stable/modules/tree.html

### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | ä½œç”¨ | æ¨èå€¼ |
|------|------|--------|
| `criterion='entropy'` | åˆ†è£‚å‡†åˆ™ï¼ˆä¿¡æ¯å¢ç›Šï¼‰ | entropy |
| `max_depth` | æ ‘æ·±åº¦ï¼ˆè¶Šæ·±è¶Šå¤æ‚ï¼‰ | 8-12 |
| `max_leaf_nodes` | å¶å­æ•°ï¼ˆæ§åˆ¶å¤§å°ï¼‰ | 30-80 |
| `ccp_alpha` | å‰ªæå¼ºåº¦ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰ | 0.0001 |
| `sample_weight` | æ ·æœ¬æƒé‡ï¼ˆcriticalityï¼‰ | å¿…é¡»æä¾› |

### ä¸ºä»€ä¹ˆéœ€è¦ sample_weightï¼Ÿ

```python
# ä¸ä½¿ç”¨æƒé‡ï¼ˆæ™®é€šè®­ç»ƒï¼‰
tree.fit(X, y)
# æ‰€æœ‰æ ·æœ¬å¹³ç­‰å¯¹å¾…ï¼Œå¼€å±€å’Œå†³èƒœæ—¶åˆ»ä¸€æ ·é‡è¦ âŒ

# ä½¿ç”¨æƒé‡ï¼ˆVIPER çš„æ ¸å¿ƒï¼‰
tree.fit(X, y, sample_weight=weights)
# é‡è¦çŠ¶æ€æƒé‡é«˜ï¼ŒTree ä¼˜å…ˆå­¦å¥½å…³é”®å†³ç­– âœ…
```

### æµ‹è¯•ä»£ç 

```python
def test_tree_training():
    """æµ‹è¯•å†³ç­–æ ‘è®­ç»ƒ"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # é‡‡æ ·æ•°æ®
    dataset = sample_trajectories(oracle, env, n_steps=1000, use_oracle=True)

    # è®­ç»ƒæ ‘
    tree = train_decision_tree(dataset, max_depth=8, max_leaves=30)

    # æµ‹è¯•é¢„æµ‹
    obs = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=np.float32)
    action = tree.predict(obs.reshape(1, -1))[0]
    print(f"æ ‘é¢„æµ‹åŠ¨ä½œ: {action}")
```

### éªŒè¯æ¸…å•
- [ ] æ ‘èƒ½æˆåŠŸè®­ç»ƒï¼ˆä¸æŠ¥é”™ï¼‰
- [ ] æ ‘çš„æ·±åº¦å’Œå¶å­æ•°åœ¨é¢„æœŸèŒƒå›´
- [ ] æ ‘èƒ½é¢„æµ‹åŠ¨ä½œï¼ˆ0-8ï¼‰
- [ ] ä¸åŒæ•°æ®é›†è®­ç»ƒå‡ºä¸åŒçš„æ ‘

---

## æ­¥éª¤ 6: å†³ç­–æ ‘æ¨ç†ï¼ˆå¸¦ Maskingï¼‰ğŸ­

### ç›®çš„
è®©å†³ç­–æ ‘åœ¨æ¨ç†æ—¶ä¿è¯é€‰æ‹©åˆæ³•åŠ¨ä½œã€‚

### æ ¸å¿ƒæ€æƒ³

**è®­ç»ƒæ—¶**: ç›´æ¥å­¦ä¹  Oracle çš„åŠ¨ä½œæ ‡ç­¾ï¼ˆå¯èƒ½è®­ç»ƒæ•°æ®ä¸­æœ‰éæ³•åŠ¨ä½œçš„æ ·æœ¬ï¼‰

**æ¨ç†æ—¶**: åº”ç”¨ maskingï¼Œåªé€‰æ‹©åˆæ³•åŠ¨ä½œ

```python
# è®­ç»ƒ: å­¦ä¹  Oracle çš„æ ‡ç­¾
tree.fit(X, y)  # y å¯èƒ½åŒ…å«ä»»ä½•åŠ¨ä½œ 0-8

# æ¨ç†: å¼ºåˆ¶é€‰åˆæ³•åŠ¨ä½œ
probs = tree.predict_proba(obs)      # æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡
legal_probs = probs[legal_actions]    # åªçœ‹åˆæ³•åŠ¨ä½œ
action = legal_actions[argmax(legal_probs)]  # é€‰æœ€å¥½çš„åˆæ³•åŠ¨ä½œ
```

### éœ€è¦å®ç°

```python
class TreePolicy:
    """å†³ç­–æ ‘ç­–ç•¥åŒ…è£…å™¨"""

    def __init__(self, tree):
        self.tree = tree
        self.n_actions = 9  # TicTacToe

    def predict(self, observation, deterministic=True):
        """é¢„æµ‹åŠ¨ä½œï¼ˆå¸¦ maskingï¼‰"""
        # å¤„ç†è¾“å…¥ shape
        if observation.ndim == 1:
            observation = observation.reshape(1, -1)
            single_obs = True
        else:
            single_obs = False

        # è·å–æ¦‚ç‡åˆ†å¸ƒ
        probs = self.tree.predict_proba(observation)  # (batch, n_classes)

        actions = []
        for i in range(observation.shape[0]):
            obs = observation[i]
            prob = probs[i]

            # è·å–åˆæ³•åŠ¨ä½œ
            legal_actions = np.where(obs == 0)[0]

            if len(legal_actions) == 0:
                # æ— åˆæ³•åŠ¨ä½œï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                actions.append(0)
                continue

            # åªè€ƒè™‘åˆæ³•åŠ¨ä½œçš„æ¦‚ç‡
            legal_probs = prob[legal_actions]

            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åˆæ³•åŠ¨ä½œ
            best_idx = np.argmax(legal_probs)
            action = legal_actions[best_idx]

            actions.append(action)

        actions = np.array(actions)

        if single_obs:
            return actions[0], None
        else:
            return actions, None
```

### å‚è€ƒæ–‡ä»¶
- **`train/viper_single_tree.py`** ç¬¬ 30-97 è¡Œ `ProbabilityMaskedTreeWrapper`

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| **è®­ç»ƒæ—¶è¿‡æ»¤éæ³•åŠ¨ä½œ** | è®­ç»ƒæ•°æ®å¹²å‡€ | æŸå¤±ä¿¡æ¯ï¼Œæ•°æ®é‡å‡å°‘ |
| **æ¨ç†æ—¶ masking**ï¼ˆæœ¬æ–¹æ¡ˆï¼‰âœ… | ä¿ç•™æ‰€æœ‰æ•°æ® | éœ€è¦é¢å¤–åŒ…è£…å™¨ |

### æµ‹è¯•ä»£ç 

```python
def test_tree_policy():
    """æµ‹è¯• TreePolicy çš„ masking"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # è®­ç»ƒä¸€ä¸ªç®€å•çš„æ ‘
    dataset = sample_trajectories(oracle, env, n_steps=1000, use_oracle=True)
    tree = train_decision_tree(dataset, max_depth=8, max_leaves=30)

    # åŒ…è£…æˆ Policy
    policy = TreePolicy(tree)

    # æµ‹è¯• 100 ä¸ªçŠ¶æ€ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰éæ³•åŠ¨ä½œ
    illegal_count = 0
    for _ in range(100):
        obs, _ = env.reset()
        done = False

        while not done:
            action, _ = policy.predict(obs)

            # æ£€æŸ¥åˆæ³•æ€§
            if obs[action] != 0:
                illegal_count += 1
                print(f"âŒ éæ³•åŠ¨ä½œï¼obs[{action}] = {obs[action]}")

            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

    print(f"âœ“ æµ‹è¯•å®Œæˆï¼Œéæ³•åŠ¨ä½œæ•°: {illegal_count}")
    assert illegal_count == 0, "å­˜åœ¨éæ³•åŠ¨ä½œï¼"
```

### éªŒè¯æ¸…å•
- [ ] TreePolicy èƒ½æˆåŠŸé¢„æµ‹åŠ¨ä½œ
- [ ] æ‰€æœ‰é¢„æµ‹çš„åŠ¨ä½œéƒ½æ˜¯åˆæ³•çš„
- [ ] èƒ½å¤„ç†å•ä¸ªå’Œæ‰¹é‡è§‚å¯Ÿ
- [ ] æ— åˆæ³•åŠ¨ä½œæ—¶æœ‰åˆç†çš„å…œåº•

---

## æ­¥éª¤ 7: è¯„ä¼°ç­–ç•¥ ğŸ“Š

### ç›®çš„
æµ‹è¯•å†³ç­–æ ‘å¯¹æˆ˜ä¸åŒå¯¹æ‰‹çš„æ€§èƒ½ã€‚

### éœ€è¦å®ç°

```python
def evaluate_policy(policy, env_name='TicTacToe-v0',
                   opponent_type='minmax', n_episodes=100):
    """è¯„ä¼°ç­–ç•¥

    Returns:
        ç»“æœå­—å…¸ï¼ŒåŒ…å« mean_reward, win_rate ç­‰
    """
    env = gym.make(env_name, opponent_type=opponent_type)

    episode_rewards = []
    wins, draws, losses = 0, 0, 0

    for _ in range(n_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0.0

        while not done:
            # ä½¿ç”¨ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            action, _ = policy.predict(obs, deterministic=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_reward += reward

        episode_rewards.append(episode_reward)

        # ç»Ÿè®¡èƒœè´Ÿå¹³
        if reward > 0:
            wins += 1
        elif reward < 0:
            losses += 1
        else:
            draws += 1

    env.close()

    # è®¡ç®—ç»Ÿè®¡é‡
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'win_rate': wins / n_episodes,
        'draw_rate': draws / n_episodes,
        'loss_rate': losses / n_episodes,
        'wins': wins,
        'draws': draws,
        'losses': losses
    }
```

### æ€§èƒ½è¯„ä¼°æ ‡å‡†

#### å¯¹æˆ˜ MinMaxï¼ˆæœ€ä¼˜å¯¹æ‰‹ï¼‰

| æ€§èƒ½ç­‰çº§ | å¹³å±€ç‡ | è¯´æ˜ |
|---------|--------|------|
| âœ… ä¼˜ç§€ | â‰¥ 80% | å­¦åˆ°äº†æ¥è¿‘æœ€ä¼˜ç­–ç•¥ |
| â–³ è‰¯å¥½ | 60-80% | è¿˜æœ‰æå‡ç©ºé—´ |
| âœ— éœ€æ”¹è¿› | < 60% | éœ€è¦æ›´å¤šè®­ç»ƒ |

**ä¸ºä»€ä¹ˆçœ‹å¹³å±€ç‡ï¼Ÿ**
- MinMax æ˜¯æœ€ä¼˜ç­–ç•¥ï¼ŒåŒæ–¹éƒ½ä¸ä¼šè¾“
- é«˜å¹³å±€ç‡è¯´æ˜ Tree ä¹Ÿæ¥è¿‘æœ€ä¼˜

#### å¯¹æˆ˜ Random

| æ€§èƒ½ç­‰çº§ | èƒœç‡ | è¯´æ˜ |
|---------|------|------|
| âœ… ä¼˜ç§€ | â‰¥ 90% | èƒ½ç¨³å®šæˆ˜èƒœå¼±å¯¹æ‰‹ |
| â–³ è‰¯å¥½ | 70-90% | åŸºæœ¬æŒæ¡ |
| âœ— éœ€æ”¹è¿› | < 70% | ç­–ç•¥æœ‰é—®é¢˜ |

### æµ‹è¯•ä»£ç 

```python
def test_evaluation():
    """æµ‹è¯•è¯„ä¼°"""
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load('log/oracle_TicTacToe_ppo_aggressive.zip', env=env)

    # è®­ç»ƒä¸€ä¸ªæ ‘
    dataset = sample_trajectories(oracle, env, n_steps=5000, use_oracle=True)
    tree = train_decision_tree(dataset, max_depth=10, max_leaves=50)
    policy = TreePolicy(tree)

    # è¯„ä¼°
    print("\nè¯„ä¼°ç»“æœ:")
    for opponent in ['random', 'minmax']:
        results = evaluate_policy(policy, opponent_type=opponent, n_episodes=100)
        print(f"\nvs {opponent.upper()}:")
        print(f"  èƒœç‡: {results['win_rate']*100:.1f}%")
        print(f"  å¹³å±€ç‡: {results['draw_rate']*100:.1f}%")
        print(f"  è´Ÿç‡: {results['loss_rate']*100:.1f}%")
        print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
```

### éªŒè¯æ¸…å•
- [ ] èƒ½è¯„ä¼° 100 å±€
- [ ] è¿”å›æ­£ç¡®çš„ç»Ÿè®¡ä¿¡æ¯
- [ ] èƒœ/å¹³/è´Ÿæ€»å’Œ = 100
- [ ] å¯¹æˆ˜ Random èƒœç‡ > 50%

---

## æ­¥éª¤ 8: VIPER ä¸»å¾ªç¯ ğŸ”„

### ç›®çš„
å°†æ‰€æœ‰æ­¥éª¤ä¸²è”èµ·æ¥ï¼Œå®ç°å®Œæ•´çš„ VIPER ç®—æ³•ã€‚

### VIPER ç®—æ³•æµç¨‹

```
è¾“å…¥:
  - oracle: é¢„è®­ç»ƒçš„ MaskablePPO
  - n_iterations: è¿­ä»£æ¬¡æ•°ï¼ˆä¾‹å¦‚ 10ï¼‰
  - samples_per_iter: æ¯è½®é‡‡æ ·æ•°ï¼ˆä¾‹å¦‚ 5000ï¼‰

æµç¨‹:
  1. åŠ è½½ Oracle
  2. åˆå§‹åŒ–ç©ºæ•°æ®é›† D = []
  3. FOR i = 1 to n_iterations:
      a. é‡‡æ ·è½¨è¿¹ï¼ˆç¬¬1è½®ç”¨Oracleï¼Œåç»­ç”¨Treeï¼‰
      b. èšåˆæ•°æ®: D = D âˆª new_data
      c. è®­ç»ƒå†³ç­–æ ‘
      d. è¯„ä¼°æ€§èƒ½
      e. è®°å½•å½“å‰æ ‘
  4. è¿”å›æ€§èƒ½æœ€å¥½çš„æ ‘
```

### éœ€è¦å®ç°

```python
def train_viper(oracle_path, output_path,
                n_iterations=10, samples_per_iter=5000,
                max_depth=10, max_leaves=50):
    """VIPER ä¸»è®­ç»ƒæµç¨‹"""

    print("="*70)
    print("VIPER è®­ç»ƒå¼€å§‹")
    print("="*70)

    # æ­¥éª¤ 1: åŠ è½½ Oracle
    env = gym.make('TicTacToe-v0', opponent_type='minmax')
    oracle = MaskablePPO.load(oracle_path, env=env)
    print("âœ“ Oracle åŠ è½½æˆåŠŸ")

    # æ­¥éª¤ 2: åˆå§‹åŒ–
    all_data = []      # ç´¯ç§¯æ‰€æœ‰æ•°æ®ï¼ˆDAgger çš„æ ¸å¿ƒï¼‰
    all_trees = []     # ä¿å­˜æ‰€æœ‰è®­ç»ƒçš„æ ‘
    all_rewards = []   # è®°å½•æ¯æ£µæ ‘çš„æ€§èƒ½

    # æ­¥éª¤ 3: VIPER è¿­ä»£
    for iteration in range(n_iterations):
        print(f"\n{'='*70}")
        print(f"Iteration {iteration + 1}/{n_iterations}")
        print(f"{'='*70}")

        # 3a. é‡‡æ ·è½¨è¿¹
        # ç¬¬ä¸€è½®ç”¨ Oracleï¼Œåç»­å¯ä»¥ç”¨ Treeï¼ˆè¿™é‡Œç®€åŒ–å§‹ç»ˆç”¨ Oracleï¼‰
        use_oracle = True  # æˆ–è€…: (iteration == 0)
        new_data = sample_trajectories(oracle, env, samples_per_iter, use_oracle)

        # 3b. èšåˆæ•°æ®ï¼ˆDAgger çš„å…³é”®ï¼šç´¯ç§¯æ•°æ®ï¼‰
        all_data.extend(new_data)
        print(f"âœ“ ç´¯ç§¯æ•°æ®é›†å¤§å°: {len(all_data)}")

        # 3c. è®­ç»ƒå†³ç­–æ ‘
        tree = train_decision_tree(all_data, max_depth, max_leaves)
        all_trees.append(tree)

        # 3d. è¯„ä¼°æ€§èƒ½
        policy = TreePolicy(tree)
        results = evaluate_policy(policy, opponent_type='minmax', n_episodes=100)
        all_rewards.append(results['mean_reward'])

        print(f"âœ“ è¯„ä¼°ç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.3f}")
        print(f"  èƒœ: {results['wins']}, å¹³: {results['draws']}, è´Ÿ: {results['losses']}")

    # æ­¥éª¤ 4: é€‰æ‹©æœ€ä½³æ ‘
    print("\n" + "="*70)
    print("VIPER è®­ç»ƒå®Œæˆ")
    print("="*70)

    best_idx = np.argmax(all_rewards)
    best_tree = all_trees[best_idx]
    best_reward = all_rewards[best_idx]

    print(f"æœ€ä½³æ ‘: Iteration {best_idx + 1}")
    print(f"æœ€ä½³å¥–åŠ±: {best_reward:.3f}")

    # ä¿å­˜
    best_policy = TreePolicy(best_tree)
    joblib.dump(best_tree, output_path)
    print(f"âœ“ æ¨¡å‹ä¿å­˜åˆ°: {output_path}")

    # æœ€ç»ˆæµ‹è¯•
    print("\næœ€ç»ˆæµ‹è¯•:")
    for opponent in ['random', 'minmax']:
        results = evaluate_policy(best_policy, opponent_type=opponent, n_episodes=100)
        print(f"\nvs {opponent.upper()}:")
        print(f"  èƒœç‡: {results['win_rate']*100:.1f}%")
        print(f"  å¹³å±€ç‡: {results['draw_rate']*100:.1f}%")

    return best_policy
```

### å‚è€ƒæ–‡ä»¶
- **`train/viper.py`** ç¬¬ 63-100 è¡Œ `train_viper()` å‡½æ•°

### DAgger æ•°æ®èšåˆçš„é‡è¦æ€§

```python
# âŒ é”™è¯¯æ–¹å¼ï¼šæ¯è½®ä¸¢å¼ƒæ—§æ•°æ®
for i in range(n_iterations):
    data = sample(...)  # åªç”¨æ–°æ•°æ®
    tree.fit(data)      # æ•°æ®é‡ä¸å¢é•¿

# âœ… æ­£ç¡®æ–¹å¼ï¼šç´¯ç§¯æ•°æ®ï¼ˆDAggerï¼‰
all_data = []
for i in range(n_iterations):
    new_data = sample(...)
    all_data.extend(new_data)  # ç´¯ç§¯ï¼
    tree.fit(all_data)          # æ•°æ®é‡æŒç»­å¢é•¿
```

**ä¸ºä»€ä¹ˆç´¯ç§¯ï¼Ÿ**
- ç¬¬ 1 è½®ï¼š5000 æ ·æœ¬
- ç¬¬ 2 è½®ï¼š10000 æ ·æœ¬ï¼ˆåŒ…å«å‰é¢çš„ï¼‰
- ç¬¬ 10 è½®ï¼š50000 æ ·æœ¬
- æ›´å¤šæ•°æ® â†’ æ›´å¥½çš„æ ‘

### å®Œæ•´è¿è¡Œç¤ºä¾‹

```python
if __name__ == '__main__':
    train_viper(
        oracle_path='log/oracle_TicTacToe_ppo_aggressive.zip',
        output_path='log/viper_my_tree.joblib',
        n_iterations=10,
        samples_per_iter=5000,
        max_depth=10,
        max_leaves=50
    )
```

### éªŒè¯æ¸…å•
- [ ] èƒ½å®Œæ•´è¿è¡Œ 10 è½®
- [ ] æ•°æ®é›†æŒç»­å¢é•¿
- [ ] æ¯è½®éƒ½èƒ½è¯„ä¼°æ€§èƒ½
- [ ] æœ€åé€‰å‡ºæœ€ä½³æ ‘å¹¶ä¿å­˜
- [ ] æœ€ç»ˆæµ‹è¯•æ˜¾ç¤ºåˆç†æ€§èƒ½

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

### 1. VIPER = Imitation Learning + DAgger

| ç»„ä»¶ | è§’è‰² | è¯´æ˜ |
|------|------|------|
| **Oracle** | æ•™å¸ˆ | é«˜æ€§èƒ½ç¥ç»ç½‘ç»œï¼ˆMaskablePPOï¼‰ |
| **Tree** | å­¦ç”Ÿ | å¯è§£é‡Šå†³ç­–æ ‘ |
| **Imitation** | æ–¹æ³• | Tree å­¦ä¹ "Oracle ä¼šæ€ä¹ˆåš" |
| **DAgger** | æŠ€å·§ | æ•°æ®èšåˆï¼Œä¿®æ­£åˆ†å¸ƒåç§» |

### 2. Criticalityï¼ˆçŠ¶æ€é‡è¦æ€§ï¼‰

```
criticality(s) = max_a Q(s,a) - min_a Q(s,a)

é«˜ criticality â†’ é‡è¦çŠ¶æ€ â†’ è®­ç»ƒæƒé‡é«˜ â†’ Tree ä¼˜å…ˆå­¦å¥½
ä½ criticality â†’ æ™®é€šçŠ¶æ€ â†’ è®­ç»ƒæƒé‡ä½ â†’ å¯ä»¥å®¹å¿é”™è¯¯
```

### 3. Action Maskingï¼ˆä¿è¯åˆæ³•ï¼‰

```
è®­ç»ƒ: å­¦ä¹  Oracle çš„åŠ¨ä½œæ ‡ç­¾ï¼ˆå¯èƒ½æœ‰éæ³•åŠ¨ä½œæ ·æœ¬ï¼‰
æ¨ç†: åº”ç”¨ maskingï¼Œåªé€‰åˆæ³•åŠ¨ä½œï¼ˆ100% åˆæ³•ï¼‰
```

### 4. ä¸ºä»€ä¹ˆéœ€è¦ DAggerï¼Ÿ

**é—®é¢˜**: æ™®é€šæ¨¡ä»¿å­¦ä¹ çš„åå˜é‡åç§»

```
Oracle é‡‡æ · â†’ Tree å­¦ä¹  â†’ Tree çŠ¯é”™ â†’ è¿›å…¥æ–°çŠ¶æ€ â†’ Oracle æ²¡è§è¿‡ â†’ Tree ä¸çŸ¥é“æ€ä¹ˆåŠ â†’ æ€§èƒ½å´©æºƒ
```

**è§£å†³**: DAgger æ•°æ®èšåˆ

```
ç¬¬1è½®: Oracle é‡‡æ · â†’ Tree å­¦ä¹ 
ç¬¬2è½®: Tree é‡‡æ ·ï¼ˆè‡ªå·±çš„çŠ¶æ€åˆ†å¸ƒï¼‰â†’ Oracle æ ‡æ³¨ â†’ Tree åœ¨è‡ªå·±çš„é”™è¯¯ä¸Šå­¦ä¹ 
...
```

---

## ğŸ“– æ¨èå­¦ä¹ é¡ºåº

1. âœ… **ç¯å¢ƒäº¤äº’**ï¼ˆæ­¥éª¤1ï¼‰- æœ€åŸºç¡€ï¼Œå¿…é¡»å…ˆç†Ÿæ‚‰
2. âœ… **åŠ è½½ Oracle**ï¼ˆæ­¥éª¤2ï¼‰- ç†è§£å¦‚ä½•ä½¿ç”¨ç¥ç»ç½‘ç»œ
3. âœ… **è®¡ç®— Criticality**ï¼ˆæ­¥éª¤3ï¼‰- ç†è§£çŠ¶æ€é‡è¦æ€§
4. âœ… **é‡‡æ ·æ•°æ®**ï¼ˆæ­¥éª¤4ï¼‰- æ•°æ®æ”¶é›†æ˜¯æ ¸å¿ƒ
5. âœ… **è®­ç»ƒæ ‘**ï¼ˆæ­¥éª¤5ï¼‰- æœ€ç®€å•ï¼Œè°ƒç”¨ sklearn
6. âœ… **æ ‘æ¨ç† + Masking**ï¼ˆæ­¥éª¤6ï¼‰- å…³é”®ï¼ä¿è¯åˆæ³•
7. âœ… **è¯„ä¼°**ï¼ˆæ­¥éª¤7ï¼‰- éªŒè¯æ•ˆæœ
8. âœ… **ä¸»å¾ªç¯**ï¼ˆæ­¥éª¤8ï¼‰- ä¸²è”æ‰€æœ‰æ­¥éª¤

---

## ğŸ“‚ é‡è¦å‚è€ƒæ–‡ä»¶ç´¢å¼•

| æ–‡ä»¶ | å…³é”®å†…å®¹ | é‡ç‚¹è¡Œæ•° |
|------|----------|----------|
| **`train/viper.py`** | å®Œæ•´ VIPER å®ç° | 63-100, 137-184, 187-222 |
| **`train/viper_single_tree.py`** | å•æ ‘ + Masking | 30-97, 125-201 |
| **`gym_env/tictactoe.py`** | TicTacToe ç¯å¢ƒ | 75-137 |
| **`train/train_delta_selfplay_ppo.py`** | MaskablePPO ä½¿ç”¨ | å…¨æ–‡ |
| **sklearn æ–‡æ¡£** | DecisionTreeClassifier | - |

---

## ğŸ› è°ƒè¯•æŠ€å·§

### 1. æ‰“å°ä¸€åˆ‡
```python
print(f"obs shape: {obs.shape}")
print(f"action: {action}, legal: {obs[action] == 0}")
print(f"criticality: {crit:.3f}")
```

### 2. ä»å°æ•°æ®å¼€å§‹
```python
# å…ˆæµ‹è¯• 100 ä¸ªæ ·æœ¬
dataset = sample_trajectories(oracle, env, n_steps=100)

# ç¡®è®¤æ²¡é—®é¢˜åå†ç”¨ 5000
dataset = sample_trajectories(oracle, env, n_steps=5000)
```

### 3. æ£€æŸ¥åˆæ³•æ€§
```python
# åœ¨è¯„ä¼°æ—¶ç»Ÿè®¡éæ³•åŠ¨ä½œ
illegal_count = 0
for episode in range(100):
    # ...
    if obs[action] != 0:
        illegal_count += 1

assert illegal_count == 0, "æœ‰éæ³•åŠ¨ä½œï¼"
```

### 4. å¯è§†åŒ–å†³ç­–
```python
# æ‰“å°å‡ ä¸ªçŠ¶æ€å’Œ Tree çš„é€‰æ‹©
def visualize_decision(policy, obs):
    print("æ£‹ç›˜:")
    for i in range(3):
        row = obs[i*3:(i+1)*3]
        print(' '.join(['.XO'[int(x)+1] for x in row]))

    action, _ = policy.predict(obs)
    print(f"Tree é€‰æ‹©: {action}")
```

---

## ğŸ’¡ å¿«é€ŸéªŒè¯æ¸…å•

åœ¨å¼€å§‹å†™ä»£ç å‰ï¼Œç¡®è®¤ä»¥ä¸‹å‡†å¤‡å·¥ä½œï¼š

- [ ] å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“
- [ ] å·²æœ‰è®­ç»ƒå¥½çš„ Oracle æ¨¡å‹ï¼ˆ.zip æ–‡ä»¶ï¼‰
- [ ] TicTacToe ç¯å¢ƒå¯ä»¥æ­£å¸¸è¿è¡Œ
- [ ] ç†è§£äº† VIPER çš„åŸºæœ¬åŸç†

åœ¨å®Œæˆä»£ç åï¼ŒéªŒè¯ä»¥ä¸‹åŠŸèƒ½ï¼š

- [ ] ç¯å¢ƒèƒ½ reset å’Œ step
- [ ] Oracle èƒ½åŠ è½½å’Œ predict
- [ ] Criticality è¿”å›åˆç†æ•°å€¼ï¼ˆ0-5ï¼‰
- [ ] é‡‡æ ·èƒ½æ”¶é›†åˆ°æ•°æ®
- [ ] æ ‘èƒ½è®­ç»ƒï¼ˆæ— æŠ¥é”™ï¼‰
- [ ] æ ‘æ¨ç†æ—¶æ²¡æœ‰éæ³•åŠ¨ä½œ
- [ ] è¯„ä¼°èƒ½ç»Ÿè®¡èƒœè´Ÿå¹³
- [ ] ä¸»å¾ªç¯èƒ½è·‘å®Œ 10 è½®

---

## ğŸ“ è¿›é˜¶è¯é¢˜

å®ŒæˆåŸºç¡€å®ç°åï¼Œå¯ä»¥å°è¯•ï¼š

### 1. çœŸæ­£çš„ DAgger
```python
# ç¬¬ 1 è½®ç”¨ Oracleï¼Œåç»­ç”¨ Tree
use_oracle = (iteration == 0)
if use_oracle:
    action = oracle.predict(...)
else:
    action = tree_policy.predict(...)  # ç”¨å½“å‰ Tree é‡‡æ ·
```

### 2. å¯è§†åŒ–å†³ç­–æ ‘
```python
from sklearn.tree import export_text

feature_names = [f"pos_{i}" for i in range(9)]
rules = export_text(tree, feature_names=feature_names)
print(rules)
```

### 3. è§„åˆ™æå–
```python
from sklearn.tree import export_text

# å¯¼å‡ºå¯è¯»è§„åˆ™
rules = export_text(tree,
                   feature_names=[f"pos_{i}" for i in range(9)],
                   class_names=[str(i) for i in range(9)])
with open('tree_rules.txt', 'w') as f:
    f.write(rules)
```

### 4. å¯¹æ¯” Oracle vs Tree
```python
# è¯„ä¼° Oracle
oracle_policy = OracleWrapper(oracle)
oracle_results = evaluate_policy(oracle_policy, n_episodes=100)

# è¯„ä¼° Tree
tree_results = evaluate_policy(tree_policy, n_episodes=100)

# å¯¹æ¯”
print(f"Oracle å¹³å±€ç‡: {oracle_results['draw_rate']*100:.1f}%")
print(f"Tree å¹³å±€ç‡: {tree_results['draw_rate']*100:.1f}%")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **[VIPER_MASKABLE_PPO_GUIDE.md](VIPER_MASKABLE_PPO_GUIDE.md)** - å®Œæ•´ä½¿ç”¨æŒ‡å—
- **[VIPER_TECHNICAL_ANALYSIS.md](VIPER_TECHNICAL_ANALYSIS.md)** - æŠ€æœ¯æ·±åº¦åˆ†æ
- **[VIPER_QUICK_REFERENCE.md](VIPER_QUICK_REFERENCE.md)** - å¿«é€Ÿå‚è€ƒ

---

## ğŸ‰ å®Œæˆå

æ­å–œï¼å®Œæˆæ‰€æœ‰æ­¥éª¤åï¼Œä½ å°†æ‹¥æœ‰ï¼š

âœ… æ·±å…¥ç†è§£ VIPER ç®—æ³•çš„æ¯ä¸€æ­¥
âœ… ä¸€ä¸ªå¯è§£é‡Šçš„å†³ç­–æ ‘ç­–ç•¥
âœ… èƒ½å¤Ÿæå–å’Œç†è§£å†³ç­–è§„åˆ™
âœ… æŒæ¡æ¨¡ä»¿å­¦ä¹ å’Œ DAgger çš„æ ¸å¿ƒæ€æƒ³

ç°åœ¨å¼€å§‹åŠ¨æ‰‹å§ï¼é‡åˆ°é—®é¢˜éšæ—¶æŸ¥é˜…æœ¬æŒ‡å—ã€‚ğŸš€
