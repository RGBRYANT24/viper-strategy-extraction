"""
åˆ†æè®­ç»ƒç›²ç‚¹ï¼šä¸ºä»€ä¹ˆæŸäº›æˆ˜æœ¯æ²¡å­¦åˆ°
"""

import numpy as np
from stable_baselines3 import DQN
import torch
import gym_env
import gymnasium as gym


def analyze_why_missing_tactics(model_path='log/oracle_TicTacToe_delta_selfplay.zip'):
    print("=" * 70)
    print("åˆ†æï¼šä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œé”™è¿‡äº†æŸäº›æˆ˜æœ¯ï¼Ÿ")
    print("=" * 70)

    model = DQN.load(model_path)

    # å¤±è´¥çš„æ¡ˆä¾‹åˆ†æ
    failed_cases = [
        {
            'board': np.array([1, 0, 0, 1, 0, 0, 0, 0, 0], dtype=np.float32),
            'correct': 7,
            'description': 'ç«‹å³è·èƒœ-ç«–å‘',
            'pattern': 'ç«–å‘ä¸‰è¿',
        },
        {
            'board': np.array([1, 0, 0, 0, 1, 0, 0, 0, 0], dtype=np.float32),
            'correct': 8,
            'description': 'ç«‹å³è·èƒœ-å¯¹è§’çº¿',
            'pattern': 'å¯¹è§’çº¿ä¸‰è¿',
        },
        {
            'board': np.array([0, 0, 1, 0, 1, 0, 1, 0, 0], dtype=np.float32),
            'correct': 3,
            'description': 'ç«‹å³è·èƒœ-åå¯¹è§’çº¿',
            'pattern': 'åå¯¹è§’çº¿ä¸‰è¿',
        },
    ]

    print("\nã€åˆ†æ1ã€‘Qå€¼åˆ†å¸ƒæ£€æŸ¥")
    print("æ£€æŸ¥ç½‘ç»œæ˜¯å¦çœŸæ­£ç†è§£äº†è¿™äº›å±€é¢\n")

    for case in failed_cases:
        obs = case['board']
        correct_action = case['correct']
        desc = case['description']

        # è·å–Qå€¼
        obs_tensor = model.policy.obs_to_tensor(obs)[0]
        with torch.no_grad():
            q_values = model.policy.q_net(obs_tensor).cpu().numpy()[0]

        predicted_action = np.argmax(q_values)
        legal_actions = np.where(obs == 0)[0]

        print(f"[{desc}]")
        print(f"  æ£‹ç›˜:\n{obs.reshape(3, 3)}")
        print(f"  æ­£ç¡®åŠ¨ä½œ: {correct_action} (Q={q_values[correct_action]:.3f})")
        print(f"  é¢„æµ‹åŠ¨ä½œ: {predicted_action} (Q={q_values[predicted_action]:.3f})")
        print(f"  Qå€¼å·®å¼‚: {q_values[predicted_action] - q_values[correct_action]:.3f}")

        # æ£€æŸ¥æ­£ç¡®åŠ¨ä½œçš„Qå€¼æ’å
        q_legal = {i: q_values[i] for i in legal_actions}
        sorted_actions = sorted(q_legal.items(), key=lambda x: x[1], reverse=True)
        rank = [i for i, (a, _) in enumerate(sorted_actions) if a == correct_action][0] + 1

        print(f"  æ­£ç¡®åŠ¨ä½œæ’å: {rank}/{len(legal_actions)}")

        if rank == 1:
            print(f"  âœ“ ç½‘ç»œç†è§£æ­£ç¡®ï¼Œä½†ç­–ç•¥é€‰æ‹©é”™è¯¯")
        elif rank <= 3:
            print(f"  â–³ ç½‘ç»œéƒ¨åˆ†ç†è§£ï¼ŒQå€¼æ¥è¿‘")
        else:
            print(f"  âœ— ç½‘ç»œå®Œå…¨ä¸ç†è§£è¿™ä¸ªå±€é¢")
        print()

    print("\nã€åˆ†æ2ã€‘è®­ç»ƒæ•°æ®è¦†ç›–åº¦æ¨æµ‹")
    print("æ¨æµ‹ï¼šè¿™äº›å±€é¢åœ¨è®­ç»ƒä¸­å‡ºç°çš„é¢‘ç‡\n")

    # æ¨¡æ‹Ÿæ£€æŸ¥ï¼šæ¨ªå‘vsç«–å‘vså¯¹è§’çº¿çš„å‡ºç°æ¦‚ç‡
    print("TicTacToeå¯¹ç§°æ€§åˆ†æ:")
    print("  â€¢ æ¨ªå‘ä¸‰è¿: 3ç§ (ç¬¬1,2,3è¡Œ)")
    print("  â€¢ ç«–å‘ä¸‰è¿: 3ç§ (ç¬¬1,2,3åˆ—)")
    print("  â€¢ å¯¹è§’çº¿:   2ç§ (ä¸»å¯¹è§’+åå¯¹è§’)")
    print()
    print("å¦‚æœè®­ç»ƒæ•°æ®æ˜¯éšæœºçš„ï¼Œåº”è¯¥:")
    print("  â€¢ æ¨ªå‘å‡ºç°: 3/8 = 37.5%")
    print("  â€¢ ç«–å‘å‡ºç°: 3/8 = 37.5%")
    print("  â€¢ å¯¹è§’çº¿:   2/8 = 25.0%")
    print()
    print("âš  ä½†ä½ çš„ç½‘ç»œåªå­¦ä¼šäº†æ¨ªå‘ï¼")
    print()
    print("å¯èƒ½åŸå› :")
    print("  1. MinMaxå¯¹æ‰‹æ€»æ˜¯ä¼˜å…ˆé˜²å®ˆæ¨ªå‘")
    print("  2. è®­ç»ƒæ—©æœŸéšæœºå¯¹æ‰‹è¢«åˆ©ç”¨æ¨ªå‘æˆ˜æœ¯å‡»è´¥")
    print("  3. ç½‘ç»œè¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ï¼ˆåªä¼šæ¨ªå‘æ”»å‡»ï¼‰")

    print("\nã€åˆ†æ3ã€‘å¯¹æ¯”ï¼šæ¨ªå‘æ˜¯å¦å­¦åˆ°äº†ï¼Ÿ")
    print()

    horizontal_case = np.array([1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32)
    obs_tensor = model.policy.obs_to_tensor(horizontal_case)[0]
    with torch.no_grad():
        q_values = model.policy.q_net(obs_tensor).cpu().numpy()[0]

    print(f"[æ¨ªå‘è·èƒœæœºä¼š]")
    print(f"  æ£‹ç›˜:\n{horizontal_case.reshape(3, 3)}")
    print(f"  æ­£ç¡®åŠ¨ä½œ: 2")
    print(f"  é¢„æµ‹åŠ¨ä½œ: {np.argmax(q_values)}")
    print(f"  Q(æ­£ç¡®): {q_values[2]:.3f}")

    if np.argmax(q_values) == 2:
        print(f"  âœ“ æ¨ªå‘å®Œç¾å­¦ä¹ ")
        print()
        print("ç»“è®º: ç½‘ç»œæœ‰èƒ½åŠ›å­¦ä¹ æˆ˜æœ¯ï¼Œä½†**è®­ç»ƒæ•°æ®ä¸å¹³è¡¡**")
    else:
        print(f"  âœ— è¿æ¨ªå‘éƒ½æ²¡å­¦å¥½")
        print()
        print("ç»“è®º: ç½‘ç»œæ ¹æœ¬æ²¡å­¦åˆ°æˆ˜æœ¯ï¼Œéœ€è¦**é‡æ–°è®­ç»ƒ**")

    print("\n" + "=" * 70)
    print("æ ¹æœ¬åŸå› è¯Šæ–­")
    print("=" * 70)

    print("""
ğŸ¯ æ ¸å¿ƒé—®é¢˜: è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸å‡

ä½ çš„æ¨¡å‹èƒ½è¾¾åˆ°100%å¹³å±€vs MinMaxï¼Œè¯´æ˜ï¼š
  âœ“ ç½‘ç»œå®¹é‡è¶³å¤Ÿ
  âœ“ å­¦ä¹ ç®—æ³•æ­£å¸¸
  âœ— ä½†æŸäº›æˆ˜æœ¯å±€é¢ä»æœªå……åˆ†è®­ç»ƒ

è¿™æ˜¯å› ä¸º:
  1. Self-playä¼šé™·å…¥"å…±åŒç›²ç‚¹"
     - å¦‚æœåŒæ–¹éƒ½ä¸æ“…é•¿ç«–å‘æ”»å‡»
     - è®­ç»ƒä¸­ç«–å‘æœºä¼šå°±å¾ˆå°‘å‡ºç°
     - æ°¸è¿œå­¦ä¸åˆ°ç«–å‘æˆ˜æœ¯

  2. MinMaxåŸºå‡†ä¸å¤Ÿå¼º
     - ä½ ç”¨äº† --use-minmaxï¼Œä½†åªæœ‰2ä¸ªåŸºå‡†ï¼ˆRandom + MinMaxï¼‰
     - MinMaxåœ¨æ± ä¸­å æ¯”å¤ªå°ï¼ˆ2/22 = 9%ï¼‰
     - å¤§éƒ¨åˆ†å¯¹æˆ˜æ˜¯å¼±å¯¹æ‰‹

è§£å†³æ–¹æ¡ˆ:
""")

    print("\næ–¹æ¡ˆA: å¢åŠ æ•°æ®å¢å¼ºï¼ˆæ¨èï¼‰")
    print("-" * 70)
    print("""
åœ¨ç¯å¢ƒä¸­æ·»åŠ æ£‹ç›˜æ—‹è½¬/é•œåƒå˜æ¢ï¼Œå¼ºåˆ¶å­¦ä¹ æ‰€æœ‰æ–¹å‘:

ä¿®æ”¹ tictactoe_delta_selfplay.py:

    def _get_observation(self):
        obs = ...  # åŸå§‹è§‚å¯Ÿ

        # éšæœºåº”ç”¨æ—‹è½¬ (25%æ¦‚ç‡)
        if np.random.random() < 0.25:
            obs_2d = obs.reshape(3, 3)
            k = np.random.randint(1, 4)  # æ—‹è½¬90/180/270åº¦
            obs_2d = np.rot90(obs_2d, k=k)
            obs = obs_2d.flatten()

        return obs
""")

    print("\næ–¹æ¡ˆB: æ”¹è¿›è®­ç»ƒå‚æ•°")
    print("-" * 70)
    print("""
python train/train_delta_selfplay.py \\
    --total-timesteps 500000 \\        # å¢åŠ è®­ç»ƒæ­¥æ•°
    --max-pool-size 10 \\              # å‡å°æ± ï¼Œå¢åŠ MinMaxæ¯”ä¾‹
    --update-interval 20000 \\         # å‡å°‘æ›´æ–°é¢‘ç‡
    --use-minmax \\
    --n-env 8

è¿™æ · MinMax å æ¯”æé«˜åˆ°: 2/12 = 16.7%
""")

    print("\næ–¹æ¡ˆC: è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰")
    print("-" * 70)
    print("""
å…ˆç”¨å›ºå®šçš„æˆ˜æœ¯åœºæ™¯è®­ç»ƒï¼Œå†è¿›è¡Œself-play:

1. é˜¶æ®µ1: å¯¹æˆ˜ MinMax (50kæ­¥)
2. é˜¶æ®µ2: Delta-Uniform Self-Play (200kæ­¥)
3. é˜¶æ®µ3: ç²¾è°ƒå¯¹æˆ˜ MinMax (50kæ­¥)
""")


def suggest_immediate_fix():
    print("\n" + "=" * 70)
    print("ç«‹å³å¯æ‰§è¡Œçš„ä¿®å¤æ–¹æ¡ˆ")
    print("=" * 70)

    print("""
ğŸš€ æ–¹æ¡ˆ1: ç»§ç»­è®­ç»ƒï¼ˆæœ€ç®€å•ï¼‰
-----------------------------------------
ä½ çš„æ¨¡å‹å·²ç»å¾ˆæ¥è¿‘æœ€ä¼˜ï¼Œç»§ç»­è®­ç»ƒå¯èƒ½ä¼šè‡ªåŠ¨ä¿®å¤:

python train/train_delta_selfplay.py \\
    --total-timesteps 200000 \\      # é¢å¤–20ä¸‡æ­¥
    --max-pool-size 10 \\            # ç¼©å°æ± ï¼Œå¢åŠ å¼ºå¯¹æ‰‹æ¯”ä¾‹
    --use-minmax \\
    --output log/oracle_TicTacToe_delta_selfplay_v2.zip

é¢„æœŸ: éšç€è®­ç»ƒç»§ç»­ï¼Œç«–å‘/å¯¹è§’çº¿åœºæ™¯ä¼šé€æ¸å‡ºç°


ğŸš€ æ–¹æ¡ˆ2: çº¯MinMaxè®­ç»ƒï¼ˆæœ€æœ‰æ•ˆï¼‰
-----------------------------------------
ä¸ç”¨self-playï¼Œç›´æ¥å¯¹æˆ˜MinMax:

python train/train_delta_selfplay.py \\
    --total-timesteps 300000 \\
    --max-pool-size 1 \\             # æ± å¤§å°=1ï¼Œå¼ºåˆ¶åªå¯¹æˆ˜MinMax
    --use-minmax \\
    --output log/oracle_TicTacToe_minmax_only.zip

é¢„æœŸ: ç›´æ¥å­¦åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œæ— ç›²ç‚¹


ğŸš€ æ–¹æ¡ˆ3: æ··åˆè®­ç»ƒï¼ˆæœ€å¹³è¡¡ï¼‰
-----------------------------------------
# ç¬¬ä¸€é˜¶æ®µ: å¯¹æˆ˜MinMaxæ‰“åŸºç¡€
python train/train_delta_selfplay.py \\
    --total-timesteps 150000 \\
    --max-pool-size 1 \\
    --use-minmax \\
    --output log/oracle_TicTacToe_phase1.zip

# ç¬¬äºŒé˜¶æ®µ: Self-Playç²¾è°ƒ
python train/train_delta_selfplay.py \\
    --total-timesteps 150000 \\
    --max-pool-size 20 \\
    --use-minmax \\
    --output log/oracle_TicTacToe_phase2.zip
    # (éœ€è¦ä¿®æ”¹ä»£ç æ”¯æŒåŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ)


ğŸ“Š å¦‚ä½•é€‰æ‹©?
-----------------------------------------
â€¢ å¦‚æœä½ æƒ³å¿«é€Ÿå¾—åˆ°æœ€ä¼˜ç­–ç•¥ â†’ æ–¹æ¡ˆ2 (çº¯MinMax)
â€¢ å¦‚æœä½ æƒ³ç ”ç©¶self-playæœºåˆ¶   â†’ æ–¹æ¡ˆ1 (ç»§ç»­è®­ç»ƒ)
â€¢ å¦‚æœä½ æ—¶é—´å……è£•            â†’ æ–¹æ¡ˆ3 (æ··åˆè®­ç»ƒ)

æˆ‘æ¨è: æ–¹æ¡ˆ2ï¼Œå› ä¸ºç®€å•æœ‰æ•ˆï¼Œ3-4å°æ—¶å°±èƒ½å®Œæˆ
""")


if __name__ == "__main__":
    import sys
    model_path = sys.argv[1] if len(sys.argv) > 1 else 'log/oracle_TicTacToe_delta_selfplay.zip'

    analyze_why_missing_tactics(model_path)
    suggest_immediate_fix()
